[{"title":"近端策略优化—PPO","path":"/2025/02/23/MudSynth/RL/近端策略优化—PPO/","content":"强化学习基础 Action Space Reward Trajectory Return(回报) E(x)x∼p(x)=∑xx⋅p(x)≈1n∑i=1nxx∼p(x)E(x)_{x \\sim p(x)} = \\sum_{x} x \\cdot p(x) \\approx \\frac{1}{n} \\sum_{i=1}^{n} x \\quad x \\sim p(x) E(x)x∼p(x)​=x∑​x⋅p(x)≈n1​i=1∑n​xx∼p(x) 目的：训练一个Policy网络π\\piπ，在所有状态S下，给出相应的Action，得到Return的期望最大 目的：训练一个Policy网络π\\piπ，在所有的Trajectory中，给出相应的Action，得到Return的期望最大。 也就是最大化：E(R(τ))τ∼Pθ(τ)=∑τR(τ)Pθ(τ)E(R(\\tau))_{\\tau \\sim P_\\theta(\\tau)} = \\sum_{\\tau} R(\\tau) P_\\theta(\\tau)E(R(τ))τ∼Pθ​(τ)​=∑τ​R(τ)Pθ​(τ), 可以使用梯度上升法。 在强化学习中，我们认为下一个状态的概率仅由当前状态和要做的动作决定，所以Pθ(τn)P_\\theta(\\tau^n)Pθ​(τn)可以被表示为： Pθ(τn)=∏t=1Tnp(sn∣sn−1,an−1)Pθ(ant∣snt)P_\\theta(\\tau^n) = \\prod_{t=1}^{T_n}p(s_n|s_{n-1},a_{n-1})P_\\theta(a_n^t|s_n^t) Pθ​(τn)=t=1∏Tn​​p(sn​∣sn−1​,an−1​)Pθ​(ant​∣snt​) 由于ppp一般为常数，可以被忽略，继续化简上式可得 该导数的直观意义是，如果 R(τn)R(\\tau^n)R(τn) 是大于0的，则增大所有执行当前trajectory动作的概率（Pθ(ant∣snt)P_\\theta(a_n^t|s_n^t)Pθ​(ant​∣snt​)），反之减少。该算法被称为 Policy gradient 算法。 Policy Gradient是一个on policy的算法，每次让agent完n轮游戏，然后用n轮的trajectory更新策略。 缺点：收集的数据只被用于更新一回，大量数据收集，导致时间效率低。 可以优化的地方 给R(τn)R(\\tau^n)R(τn)添加一个折扣因子。（一个动作只能影响后面的reward，且后面的reward我们应该尽快拿到更好，所以添加了一个折扣因子） 在好的局势下，所有动作的概率都会被提升，坏的局势下所有动作的概率都会被减少。添加一个baseline（相减）可以让部分动作概率提升，部分动作概率减少，使学习收敛更迅速。 具体优化见下图： 接下来回顾三个常见的强化学习新概念： 然后如上图所示，可以使用Advantage Function替换掉原式子中的 Rtn−B(Snt)R_t^n-B(S_n^t)Rtn​−B(Snt​),但是我们此时在实现的过程中还是需要训练两个神经网络分别拟合Action-Value Function以及State-Value Function。有没有办法只训练一个网络呢？ 观察红框不难发现，此时Advantage Function中只含有State-Value Function我们只需要训练一个网络即可。 为了使右侧式子表示的更为简洁，定义 δ\\deltaδ 如下图所示： 从上图不难理解，随着优势函数阶数的增加，估计的偏差越小，方差越大。为了更好的估计真实的优势函数（平衡方差和偏差），我们下面引入GAE算法（如下图所示）。 最后总结下上面推到出来最为重要的几个公式（见下图）： δ\\deltaδ GAE估计的优势函数 优化后的策略目标 其中，状态价值函数使用神经网络进行拟合，和策略函数共用参数。（红色label代表的是拟合估计的V和真实的折扣回报值） PPO 之前On Policy所存在的问题已经简单介绍过了，主要是采集的数据只能够用于训练一次神经网络，有点浪费，时间效率低。 PPO对这个On Policy进行了优化，让每次采样的数据可以被用于训练多次来优化网络参数。 重要性采样：如何让AI「吃老本」还不翻车？ 重要性采样在PPO中的作用，可以理解为“用旧经验学新招，但避免翻车”。比如你学骑自行车时，用之前摔跤的经验调整动作，但又不让新动作和旧动作差别太大。PPO通过给旧策略的数据加上“调整系数”（重要性权重），让新策略能复用这些数据更新自己，同时限制调整幅度（比如用clip函数“剪掉”过大的改动），防止步子迈太大导致策略崩坏。这样既能高效利用数据，又能稳定训练。 1. 从On-Policy的痛点说起 想象你正在学骑自行车（训练策略θ），传统On-Policy算法就像一位严格的教练： 规则：每次调整动作（更新θ）后，必须重新上路骑行（采样新轨迹τ∼pθ），记录摔倒的姿势。 问题：哪怕只是微调了车把角度，之前所有摔倒的数据全部作废。效率极低，且95%的时间浪费在重复采样上！ 此时，一个灵魂拷问诞生了：能否用历史摔倒数据（旧策略）指导新动作（新策略）？ 答案就是重要性采样（Importance Sampling）。 2. 重要性采样的直观意义 2.1 二手书定价的统计哲学 假设你想计算某绝版书在书店的均价（分布p(x)），但只能从二手市场（分布q(x)）收集数据。直接计算二手市场的均价会偏高，因为绝版书被炒高价。 解决方法：为每本书的价格乘上一个修正系数（= 书店定价/二手市场定价）。例如： 某书在书店原价50元，二手市场卖100元，则修正系数=0.5 最终统计时，这本书的贡献是：100元 × 0.5 = 50元（还原真实价值） 这里的修正系数就是重要性权重 p(x)q(x)\\frac{p(x)}{q(x)}q(x)p(x)​，它抵消了分布差异带来的偏差。 2.2 回到自行车：如何用旧动作学新招？ 假设旧策略θ_old的摔倒数据中有一个动作：“腿抬高10cm时摔倒”。现在新策略θ想尝试“腿抬8cm”。直接复用旧数据会有两个问题： 偏差：旧数据中“抬10cm”的频率可能和新策略“抬8cm”的实际概率不同。 风险：如果盲目参考旧动作，可能学得比原来更差（梯度爆炸）。 重要性采样的作用： 给每个旧动作加上一个可信度标签（重要性权重=新策略产生该动作的概率 / 旧策略的概率）。 如果新策略θ认为“抬10cm”的概率是旧策略的80%，则这个动作的梯度更新权重就是0.8。 效果：相当于教练说：“你过去抬10cm摔了，但现在你只打算抬8cm，所以这个旧动作的参考价值打8折。” 3. 数学视角：期望修正与概率比 3.1 原始On-Policy梯度 ∇Rˉθ=Eτ∼pθ(τ)[R(τ)∇log⁡pθ(τ)] abla \\bar{R}_\\theta = \\mathbb{E}_{\\tau \\sim p_\\theta(\\tau)} \\left[ R(\\tau) abla \\log p_\\theta(\\tau) \\right] ∇Rˉθ​=Eτ∼pθ​(τ)​[R(τ)∇logpθ​(τ)] 痛点：每次更新θ后，必须重新采样τ∼pθ(τ)，成本高昂。 3.2 重要性采样改造后的梯度 ∇Rˉθ=Eτ∼q(τ)[pθ(τ)q(τ)R(τ)∇log⁡pθ(τ)] abla \\bar{R}_\\theta = \\mathbb{E}_{\\tau \\sim q(\\tau)} \\left[ \\frac{p_\\theta(\\tau)}{q(\\tau)} R(\\tau) abla \\log p_\\theta(\\tau) \\right] ∇Rˉθ​=Eτ∼q(τ)​[q(τ)pθ​(τ)​R(τ)∇logpθ​(τ)] 魔法：采样分布从新策略pθ切换到旧策略q，但通过乘上概率比 pθ(τ)q(τ)\\frac{p_\\theta(\\tau)}{q(\\tau)}q(τ)pθ​(τ)​ 修正偏差。 物理意义： 若新策略pθ比旧策略q更倾向于某个τ（即pθq&gt;1\\frac{p_\\theta}{q} &gt;1qpθ​​&gt;1），则放大该τ的梯度贡献。 反之则缩小，甚至忽略（如PPO的Clip操作）。 4. 重要性采样的隐患：方差爆炸 4.1 为什么不能无脑复用旧数据？ 回到二手书例子：如果某书在书店原价50元，但二手市场标价10,000元（概率比=0.005），则修正后的价格为10,000×0.005=50元。看似合理，但问题在于： 若这本书在二手市场被频繁交易（采样多），则它会主导整体均价的计算，导致统计结果波动极大。 对应到RL中：若新旧策略在某个轨迹τ上的概率差异极大（例如pθq=100\\frac{p_\\theta}{q} =100qpθ​​=100），则少量τ的样本会主导梯度更新，使得训练不稳定。 4.2 PPO的智慧：给「吃老本」加上安全绳 PPO的解决方案简单粗暴——限制重要性权重的幅度： 计算原始权重：r(θ)=pθ(τ)q(τ)r(\\theta) = \\frac{p_\\theta(\\tau)}{q(\\tau)}r(θ)=q(τ)pθ​(τ)​ 用Clip函数裁剪：rclip=clip(r(θ),1−ϵ,1+ϵ)r_{\\text{clip}} = \\text{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon)rclip​=clip(r(θ),1−ϵ,1+ϵ) 直观解释： 教练说：“旧动作和新动作差异超过±10%的部分我不认，避免你乱改动作摔得更惨。” 数学上，这限制了梯度更新的最大步长，避免单一样本主导学习过程。 5. 总结：重要性采样的双重价值 高效性：打破On-Policy的采样枷锁，让AI能复用历史数据，大幅提升训练效率。 安全性：通过概率比和Clip机制，在「大胆创新」和「谨慎迭代」之间找到平衡，避免策略崩溃。 如同学骑自行车： 看自己过去的摔倒录像（重要性采样）比每次都真摔（On-Policy）更高效。 但模仿时需注意：“上次抬腿10cm摔了，这次试试8cm，但绝不突然抬腿20cm（Clip限制）”。 目标函数更新 理解了重要性采样，我们便可用其来更新我们的目标函数，将原来On-Policy的学习转为Off-Policy。 其中第一步是原始的On-Policy损失函数，第二步变为了Off-Policy损失函数（θ′\\theta&#x27;θ′为参考策略，参考策略用于采样，并可以用于多次训练优化θ\\thetaθ）。最后的Loss就是PPO算法的损失函数。 但是对于参考策略和现有的策略还是有一些限制，就是参考策略和现有的策略不能相差太大。不然训练效果会非常不好。 可以通过下面的方式进行约束： 增加KL散度正则化项进行约束 使用CLIP函数，防止训练的策略和参考的策略偏差过大","tags":["LLM","RL","PPO"],"categories":["LLM","RL"]},{"title":"从零开始了解LLaMA：开源大语言模型的架构解析","path":"/2025/02/02/MudSynth/LLM/从零开始了解LLaMA：开源大语言模型的架构解析/","content":"整理自B站教程：图解llama架构 解读源码实现_哔哩哔哩_bilibili 0. 引言 近年来，大型语言模型（LLM）在自然语言处理领域取得了突破性进展，而LLaMA（Large Language Model Meta AI）作为Meta AI开源的一系列LLM，以其优异的性能和开放的姿态，迅速成为研究者和开发者关注的焦点。 你是否好奇LLaMA是如何工作的？它与其他LLM相比有何优势？在这篇博客中，我们将结合代码深入浅出地解析LLaMA的整体架构，带你从零开始了解这一强大的语言模型。我们将探讨LLaMA的模型结构，帮助你全面理解LLaMA的运作机制，并为你开启探索LLM世界的大门。 为了能够更好的配合原代码进行阅读，首先需要安装transformers库： 1pip install transformers 使用pip show transformers定位到安装包的位置后，使用代码编辑器打开库所在文件夹，我们需要阅读的代码在models/llama文件夹中。 1. 分词器部分 在LLaMA的整体架构中，分词器（Tokenizer）部分负责将原始文本转换为模型能够理解的输入格式。这个过程是深度学习模型处理文本数据的关键步骤，主要通过将文本转换为一系列数字化的ID（即token IDs）来实现。 以一个简单的例子为说明：假设我们有一个文本输入 “I love machine learning.”。分词器首先会将文本分割成以下几个单元： “I” “love” “machine” “learning” ”.” 接着，分词器将这些词汇映射为对应的数字ID（token IDs）。例如，假设词汇表中的映射为： “I” -&gt; 101 “love” -&gt; 2057 “machine” -&gt; 12345 “learning” -&gt; 67890 “.” -&gt; 999 最终，分词器输出的token IDs序列为：[101, 2057, 12345, 67890, 999]。这些ID作为LLaMA模型的输入，供模型进行进一步的计算和处理。 通过这种方式，LLaMA能够高效地理解和处理文本数据，为后续的模型计算奠定基础。分词器在这一过程中的作用不仅仅是将文字转为数字，它还要确保分割的单位能够反映出文本的语法和语义结构，从而提高模型的理解能力和生成效果。 2. LLaMA主干部分 LLaMA的主干部分主要负责对输入文本进行深入的特征提取和理解。这一部分从分词器输出的token IDs开始，经过嵌入层（Embedding），并通过多个隐藏状态（hidden states）层层传递，最终输出模型的高级特征表示。 2.1 嵌入层（Embedding） 在LLaMA模型的主干部分，首先是嵌入层（Embedding）。嵌入层将每个token ID转化为高维向量，使得模型可以在更高层次上理解和处理输入数据。嵌入向量包含了每个token的语义信息，使得模型能够基于这些信息进行后续的计算。 2.2 隐藏状态（hidden states） 从嵌入层输出的向量会进入一系列的隐藏状态（hidden states）。这些隐藏状态是神经网络中每一层的输出，包含了输入数据在网络中的逐步转换过程。LLaMA使用了多个隐藏状态层，这些层共同作用，捕捉了文本中更复杂的语法和语义特征。每一层的输出都会提供关于输入文本的不同方面的理解，帮助模型构建更精确的上下文关系。 2.3 多层处理（Layers） LLaMA模型的核心部分由多个层（Layers） 组成，每一层都在不断改进模型对输入文本的理解。这些层通过多个神经网络模块，如自注意力机制（Self-attention），帮助模型捕捉长距离依赖关系和复杂的语义信息。每一层都从前一层的输出（即隐藏状态）中提取更高层次的特征表示，逐步增强对文本的理解。在后面我们会结合代码进行具体讲解。 最终，LLaMA通过这些多层的处理机制，能够获得更为丰富的语义表示，进而完成各种语言理解和生成任务。 通过这种结构，LLaMA能够有效地进行文本的深度处理，最大化其对输入的理解能力。每个层级的输出（隐藏状态）都会传递至下一层，以确保模型在每一阶段都能构建出更具语境感知的特征表示。 2.4 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaModel类 123456789101112131415161718192021222324class LlamaModel(LlamaPreTrainedModel): &quot;&quot;&quot; Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`] Args: config: LlamaConfig &quot;&quot;&quot; def __init__(self, config: LlamaConfig): super().__init__(config) self.padding_idx = config.pad_token_id self.vocab_size = config.vocab_size self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx) self.layers = nn.ModuleList( [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)] ) self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) self.rotary_emb = LlamaRotaryEmbedding(config=config) self.gradient_checkpointing = False # Initialize weights and apply final processing self.post_init() 然后核心的执行顺序如下（forward函数精简版）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061if inputs_embeds is None:\tinputs_embeds = self.embed_tokens(input_ids)causal_mask = self._update_causal_mask( attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions)hidden_states = inputs_embeds# create position embeddings to be shared across the decoder layersposition_embeddings = self.rotary_emb(hidden_states, position_ids)# hidden states forward(decoder layers)for decoder_layer in self.layers[: self.config.num_hidden_layers]:\tif output_hidden_states: all_hidden_states += (hidden_states,)\tif self.gradient_checkpointing and self.training: layer_outputs = self._gradient_checkpointing_func( decoder_layer.__call__, hidden_states, causal_mask, position_ids, past_key_values, output_attentions, use_cache, cache_position, position_embeddings, )\telse: layer_outputs = decoder_layer( hidden_states, attention_mask=causal_mask, position_ids=position_ids, past_key_value=past_key_values, output_attentions=output_attentions, use_cache=use_cache, cache_position=cache_position, position_embeddings=position_embeddings, **flash_attn_kwargs, )\thidden_states = layer_outputs[0]\tif output_attentions: all_self_attns += (layer_outputs[1],)hidden_states = self.norm(hidden_states)# add hidden states from the last decoder layerif output_hidden_states:\tall_hidden_states += (hidden_states,)output = BaseModelOutputWithPast(\tlast_hidden_state=hidden_states,\tpast_key_values=past_key_values if use_cache else None,\thidden_states=all_hidden_states,\tattentions=all_self_attns,)return output if return_dict else output.to_tuple() 3. LLaMA的CLM任务 在LLaMA模型中，CLM任务（Causal Language Modeling）是用于训练模型理解文本生成的关键任务之一。其主要目标是通过预测下一个token的方式，训练模型掌握语言的生成能力。 3.1 自回归Loss（Causal Loss） LLaMA在CLM任务中使用了自回归损失（Causal Loss）。在该任务中，模型基于输入的文本（或token序列）逐步预测每个token，尝试生成下一个token。这一过程通过计算预测值与实际值之间的差距来优化模型，从而使模型能够更准确地预测未来的token。损失函数则根据这个差距来反向传播误差，更新模型参数。 3.2 CLM输出 CLM任务的最终输出是CLM output，即通过训练得到的token预测结果。这个输出包含了每个时间步的预测token，以及其对应的生成概率分布，最终构成了模型的语言生成能力。 3.3 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaForCausalLM类 123456789101112class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin): _tied_weights_keys = [&quot;lm_head.weight&quot;] _tp_plan = &#123;&quot;lm_head&quot;: &quot;colwise_rep&quot;&#125; def __init__(self, config): super().__init__(config) self.model = LlamaModel(config) self.vocab_size = config.vocab_size self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False) # Initialize weights and apply final processing self.post_init() 可以看到，他上来就定义了一个LlamaModel，利用输出的hidden_states做自回归任务（next token prediction）。 然后我们看forward部分（精简版），输入input_ids，先过上面我们说的model。然后过linear层，最后计算一个loss，loss function的定义见后续代码。 forward部分精简版如下： 12345678910111213141516171819202122232425262728293031323334# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)outputs = self.model(\tinput_ids=input_ids,\tattention_mask=attention_mask,\tposition_ids=position_ids,\tpast_key_values=past_key_values,\tinputs_embeds=inputs_embeds,\tuse_cache=use_cache,\toutput_attentions=output_attentions,\toutput_hidden_states=output_hidden_states,\treturn_dict=return_dict,\tcache_position=cache_position,\t**kwargs,)hidden_states = outputs[0]# Only compute necessary logits, and do not upcast them to float if we are not computing the losslogits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])loss = Noneif labels is not None:\tloss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)if not return_dict:\toutput = (logits,) + outputs[1:]\treturn (loss,) + output if loss is not None else outputreturn CausalLMOutputWithPast(\tloss=loss,\tlogits=logits,\tpast_key_values=outputs.past_key_values,\thidden_states=outputs.hidden_states,\tattentions=outputs.attentions,) loss部分代码如下(默认是ForCausalLM)： 1234567891011@propertydef loss_function(self):\tloss_type = getattr(self, &quot;loss_type&quot;, None)\tif loss_type is None or loss_type not in LOSS_MAPPING: logger.warning_once( f&quot;`loss_type=&#123;loss_type&#125;` was set in the config but it is unrecognised.&quot; f&quot;Using the default loss: `ForCausalLMLoss`.&quot; ) loss_type = &quot;ForCausalLM&quot;\treturn LOSS_MAPPING[loss_type] ForCausalLMLoss 函数：因果语言模型损失计算 这一部分使用的就是： 123456def fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs): reduction = &quot;sum&quot; if num_items_in_batch is not None else &quot;mean&quot; loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction) if reduction == &quot;sum&quot;: loss = loss / num_items_in_batch return loss 1234567891011121314151617def ForCausalLMLoss( logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs): # Upcast to float if we need to compute the loss to avoid potential precision issues logits = logits.float() labels = labels.to(logits.device) # Shift so that tokens &lt; n predict n shift_logits = logits[..., :-1, :].contiguous() shift_labels = labels[..., 1:].contiguous() # Flatten the tokens shift_logits = shift_logits.view(-1, vocab_size) shift_labels = shift_labels.view(-1) # Enable model parallelism shift_labels = shift_labels.to(shift_logits.device) loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs) return loss 1. 输入参数 logits: 模型输出，形状 (batch_size, sequence_length, vocab_size)。 labels: 真实标签，形状 (batch_size, sequence_length)。 vocab_size: 词汇表大小。 ignore_index: 忽略的标签索引（如填充符）。 2. 核心步骤 2.1 移位操作 12shift_logits = logits[..., :-1, :] # 预测：去掉最后一个词shift_labels = labels[..., 1:] # 标签：去掉第一个词 目的: 使模型只能基于前 ( n-1 ) 个词预测第 ( n ) 个词。 2.2 展平 12shift_logits = shift_logits.view(-1, vocab_size) # 展平为 (N, vocab_size)shift_labels = shift_labels.view(-1) # 展平为 (N,) 目的: 将所有预测和标签放在统一维度，方便计算损失。 2.3 计算交叉熵损失 1loss = fixed_cross_entropy(shift_logits, shift_labels, ...) 公式:loss=−1N∑i=1N∑j=1Vyijlog⁡(pij)\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{V} y_{ij} \\log(p_{ij}) loss=−N1​i=1∑N​j=1∑V​yij​log(pij​) 其中： pij=softmax(shift_logitsij)p_{ij} = \\text{softmax}(\\text{shift\\_logits}_{ij})pij​=softmax(shift_logitsij​)。 yijy_{ij}yij​ 是真实标签的 one-hot 编码。 3. 总结 功能: 计算因果语言模型的损失。 核心: 通过移位操作实现因果性，使用交叉熵衡量预测与真实标签的差异。 输出: 损失值（标量）。 示例 输入： logits: (2, 3, 5)（批次 2，序列 3，词汇表 5）。 labels: (2, 3)。 输出： shift_logits: (2, 2, 5)。 shift_labels: (2, 2)。 最终损失：标量值。 4. LLaMA的文本分类任务 除了生成任务，LLaMA还支持文本分类任务，这是许多自然语言处理任务的核心部分，特别是在情感分析、主题分类等任务中具有广泛应用。 4.1 分类层（Classifier Layer） 在文本分类任务中，LLaMA通过一个nn.Linear层将隐藏状态（hidden states）转换为适合分类任务的输出。该层根据输入文本的特征，计算出每个类别的概率。 4.2 分类损失（Classification Loss） LLaMA使用分类损失（Classification Loss） 来优化文本分类任务。与CLM任务不同，分类任务的目标是将文本分配到特定的类别中，损失函数计算预测类别与实际类别之间的差距，并通过反向传播调整模型参数。 4.3 分类输出（Classifier output） 分类任务的输出是Classifier output，即模型对输入文本所预测的分类结果。这个输出表示模型对每个类别的预测概率，并通过选择概率最大的类别作为最终分类结果。 总结来说，LLaMA不仅在语言生成任务（CLM任务）中表现出色，还在文本分类任务中具备强大的能力。通过使用自回归损失和分类损失，LLaMA能够在这两类任务中进行高效的学习与优化，从而实现广泛的自然语言处理应用。 4.4 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaForSequenceClassification类 123456789class LlamaForSequenceClassification(LlamaPreTrainedModel): def __init__(self, config): super().__init__(config) self.num_labels = config.num_labels self.model = LlamaModel(config) self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False) # Initialize weights and apply final processing self.post_init() 然后我们看forward部分（精简版），输入input_ids，先过上面我们说的model。然后过linear层，最后计算一个loss，loss function的定义见后续代码。 forward部分精简版如下： 12345678910111213141516171819202122232425262728293031transformer_outputs = self.model(\tinput_ids,\tattention_mask=attention_mask,\tposition_ids=position_ids,\tpast_key_values=past_key_values,\tinputs_embeds=inputs_embeds,\tuse_cache=use_cache,\toutput_attentions=output_attentions,\toutput_hidden_states=output_hidden_states,\treturn_dict=return_dict,)hidden_states = transformer_outputs[0]logits = self.score(hidden_states)pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]loss = Noneif labels is not None:\tloss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)if not return_dict:\toutput = (pooled_logits,) + transformer_outputs[1:]\treturn ((loss,) + output) if loss is not None else outputreturn SequenceClassifierOutputWithPast(\tloss=loss,\tlogits=pooled_logits,\tpast_key_values=transformer_outputs.past_key_values,\thidden_states=transformer_outputs.hidden_states,\tattentions=transformer_outputs.attentions,) loss还是使用的是交叉熵损失函数，这里不再赘述 然后其实还有两个函数: 4.5 LlamaForQuestionAnswering 任务：问答任务 损失函数：交叉熵损失（Cross-Entropy Loss） 在问答任务中，特别是抽取式问答任务中，LLaMA通常会通过交叉熵损失来计算预测的答案位置与实际答案之间的差距。模型会输出每个token作为答案的概率分布，交叉熵损失用于比较预测的答案跨度与真实答案的跨度之间的差距。 这里的损失函数通常是torch.nn.CrossEntropyLoss，用来对答案span进行评估。 4.6 LlamaForTokenClassification 任务：标记分类任务 损失函数：交叉熵损失（Cross-Entropy Loss） 在标记分类任务中（如命名实体识别、词性标注等），模型会为输入的每个token分配一个标签。每个token的logits将与真实标签进行比较，通常使用交叉熵损失来计算损失。 同样，交叉熵损失通过 torch.nn.CrossEntropyLoss 来实现。 5. LLaMA的Layer层 LLaMA的Layer层是模型的核心结构之一，每一层都由多个重要组成部分构成，这些组件共同作用，帮助模型处理输入数据并提取更深层次的特征表示。每一层的处理步骤通常包括自注意力机制（Attention）、归一化（Norm）和多层感知机（MLP）等模块。 5.1 归一化（Norm） 每一层的输入（即隐藏状态hidden states）首先会经过归一化（Norm） 操作。归一化的目的是调整输入的分布，使得模型训练更加稳定，并加速收敛过程。通过这种方式，LLaMA能够确保每一层的输入都处于一个合理的数值范围内。 5.2 自注意力机制（Attention） 接下来，自注意力机制（Attention） 会被应用于每一层的隐藏状态。自注意力机制是LLaMA处理序列数据的关键，它帮助模型根据输入数据的各个部分之间的关系来调整每个token的表示。通过这种方式，模型能够捕捉到序列中远程依赖的关系，从而提升对文本语义的理解能力。注意力机制中的细节会在后面进行详细解释。 在此步骤中，模型会计算每个token与其他token的相关性，并基于这些信息来更新隐藏状态，使得每个token的表示更加贴合上下文。 5.3 残差连接（Residual） 在自注意力模块之后，残差连接（Residual） 被用于将输入隐藏状态与经过自注意力机制更新后的隐藏状态相加，从而形成更新后的隐藏状态。这种残差连接有助于防止梯度消失问题，使得深层模型能够更好地训练。 5.4 多层感知机（MLP） 每一层的后半部分是多层感知机（MLP） 模块，它通常由若干个全连接层（fully connected layers）构成。MLP层的作用是进一步处理更新后的隐藏状态，通过非线性变换来增加模型的表达能力。MLP层帮助模型在处理信息时更好地捕捉复杂的模式。 5.5 残差连接（Residual） 与自注意力机制类似，MLP模块后的输出也会通过一个残差连接与输入进行相加。这样可以确保每一层的特征表示不仅仅依赖于当前层的输出，还结合了输入的信息，使得梯度能够顺利传播，从而提高训练效果。 5.6 最终输出（hidden states） 经过上述步骤，每一层的最终输出是更新后的隐藏状态（hidden states），它包含了模型在该层提取到的所有语义信息。这些隐藏状态将被传递到下一层，继续进行进一步的处理，直到整个网络完成训练。 总结来说，LLaMA的每一层通过自注意力、归一化、MLP和残差连接等模块的协作，能够从输入数据中提取越来越丰富的特征。这些层级的组合构建了一个深度神经网络，使得LLaMA能够在多个自然语言处理任务中取得良好的效果。 5.7 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaDecoderLayer类 12345678910class LlamaDecoderLayer(nn.Module): def __init__(self, config: LlamaConfig, layer_idx: int): super().__init__() self.hidden_size = config.hidden_size self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx) self.mlp = LlamaMLP(config) self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps) 可以看到确实如我们之前所介绍那样，包含了3个部分： Attention部分 MLP部分 标准化部分 forward函数实现如下： 1234567891011121314151617181920212223242526272829residual = hidden_stateshidden_states = self.input_layernorm(hidden_states)# Self Attentionhidden_states, self_attn_weights = self.self_attn(\thidden_states=hidden_states,\tattention_mask=attention_mask,\tposition_ids=position_ids,\tpast_key_value=past_key_value,\toutput_attentions=output_attentions,\tuse_cache=use_cache,\tcache_position=cache_position,\tposition_embeddings=position_embeddings,\t**kwargs,)hidden_states = residual + hidden_states# Fully Connectedresidual = hidden_stateshidden_states = self.post_attention_layernorm(hidden_states)hidden_states = self.mlp(hidden_states)hidden_states = residual + hidden_statesoutputs = (hidden_states,)if output_attentions:\toutputs += (self_attn_weights,)return outputs 可以看到就是如上图所示的一个流程。 6. Attention部分 在LLaMA模型中，Attention机制是一个关键的模块，帮助模型在处理文本时捕捉不同位置之间的依赖关系。通过这种机制，模型能够动态地关注输入序列中与当前token相关的部分，从而改善对上下文的理解。 6.1 输入处理 首先，模型会将隐藏状态（hidden states）通过线性层（nn.Linear）转换为查询（query） 、键（key）和值（value）。这些表示将作为自注意力计算的核心输入。 6.2 旋转位置编码（apply_rotary_pos_emb） 接着，通过旋转位置编码（apply_rotary_pos_emb） 对查询和键进行位置编码。这一步确保模型能够捕捉到输入中各token的位置信息，从而正确理解其在上下文中的角色。 6.3 计算注意力权重 然后，查询和键将进行点积计算，并通过softmax函数得到注意力权重（attn_weights）。这些权重表示每个token在当前上下文中对其他token的“关注程度”。 6.4 计算输出 最后，注意力权重会与值（value）进行矩阵乘法（MatMul），从而得到最终的Attention输出（attn_output）。这个输出表示了模型基于输入文本中各部分关系所做的加权汇总。 6.5 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaAttention类 12345678910111213141516171819202122232425class LlamaAttention(nn.Module): &quot;&quot;&quot;Multi-headed attention from &#x27;Attention Is All You Need&#x27; paper&quot;&quot;&quot; def __init__(self, config: LlamaConfig, layer_idx: int): super().__init__() self.config = config self.layer_idx = layer_idx self.head_dim = getattr(config, &quot;head_dim&quot;, config.hidden_size // config.num_attention_heads) self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads self.scaling = self.head_dim**-0.5 self.attention_dropout = config.attention_dropout self.is_causal = True self.q_proj = nn.Linear( config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias ) self.k_proj = nn.Linear( config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias ) self.v_proj = nn.Linear( config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias ) self.o_proj = nn.Linear( config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias ) forward函数实现如下（精简版）： 12345678910111213141516171819202122232425262728input_shape = hidden_states.shape[:-1]hidden_shape = (*input_shape, -1, self.head_dim)query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)cos, sin = position_embeddingsquery_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)if past_key_value is not None:\t# sin and cos are specific to RoPE models; cache_position needed for the static cache\tcache_kwargs = &#123;&quot;sin&quot;: sin, &quot;cos&quot;: cos, &quot;cache_position&quot;: cache_position&#125;\tkey_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)attn_output, attn_weights = attention_interface(\tself,\tquery_states,\tkey_states,\tvalue_states,\tattention_mask,\tdropout=0.0 if not self.training else self.attention_dropout,\tscaling=self.scaling,\t**kwargs,)attn_output = attn_output.reshape(*input_shape, -1).contiguous()attn_output = self.o_proj(attn_output)return attn_output, attn_weights 对于Attention中的一些重要技术如RoPE以及Paged Attention等后续会单开栏目进行介绍。 7. MLP部分 7.1 简介 MLP这一部分的设计则更为简单，详细设计后续会结合代码进行进一步的讲解，简单介绍下为什么这样设计： 门控与投影的结合：通过将gate_proj_output和up_proj_output相乘，模型能够在每一层中灵活地调整信息流动。这种设计通过加权和门控的方式，确保模型能够有选择性地保留或抑制不同的输入特征，从而提高处理能力。（up是升采样的流程，down是降采样的过程） 非线性激活：激活函数的引入确保了模型能够捕捉复杂的非线性模式，使得LLaMA可以更好地拟合数据中的复杂关系，提升模型的准确性和泛化能力。 7.2 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaMLP类 代码如下： 1234567891011121314class LlamaMLP(nn.Module): def __init__(self, config): super().__init__() self.config = config self.hidden_size = config.hidden_size self.intermediate_size = config.intermediate_size self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias) self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias) self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias) self.act_fn = ACT2FN[config.hidden_act] def forward(self, x): down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)) return down_proj 其中self.act_fn指的是激活函数，符合上图所示的运算步骤 8. 归一化部分（RMSNorm） 8.1 简介 在LLaMA模型中，RMSNorm（均方根归一化）是一种用于归一化的技术，类似于传统的Layer Normalization，但采用了不同的归一化方式。与LayerNorm基于均值和方差的标准化不同，RMSNorm只使用输入的方差（而非均值），通过计算均方根（RMS）来调整每个token的特征。这种方法的优势在于数值计算的简化与稳定性，尤其是在处理大规模预训练模型时。RMSNorm能够避免训练过程中出现的梯度消失或爆炸问题，并在许多任务中展现了出色的性能。 与标准化方法（如LayerNorm）不同，RMSNorm不需要中心化输入数据，而是直接对每个token的特征进行归一化，保留了输入数据的原始分布。这使得它在大规模神经网络训练中具有更高的效率与稳定性，尤其适用于像LLaMA这样的大型模型。 接下来，我们将深入探讨 LlamaRMSNorm 层的实现和工作原理。 8.2 代码实现 这一部分的代码对应的就是models/llama/modeling_llama.py中的LlamaMLP类 123456789101112131415161718class LlamaRMSNorm(nn.Module): def __init__(self, hidden_size, eps=1e-6): &quot;&quot;&quot; LlamaRMSNorm is equivalent to T5LayerNorm &quot;&quot;&quot; super().__init__() self.weight = nn.Parameter(torch.ones(hidden_size)) self.variance_epsilon = eps def forward(self, hidden_states): input_dtype = hidden_states.dtype hidden_states = hidden_states.to(torch.float32) variance = hidden_states.pow(2).mean(-1, keepdim=True) hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon) return self.weight * hidden_states.to(input_dtype) def extra_repr(self): return f&quot;&#123;tuple(self.weight.shape)&#125;, eps=&#123;self.variance_epsilon&#125;&quot; LlamaRMSNorm 是一种归一化方法，类似于 T5 模型中的 T5LayerNorm，但它使用 均方根（RMS）归一化，而不是常规的标准化方法（如 LayerNorm）。这种方法通过对输入的方差进行归一化，使得模型在训练时更加稳定。 数学公式 均方根归一化的核心公式如下： x^i=xi1N∑i=1Nxi2+ϵ\\hat{x}_i = \\frac{x_i}{\\sqrt{\\frac{1}{N} \\sum_{i=1}^N x_i^2 + \\epsilon}} x^i​=N1​∑i=1N​xi2​+ϵ​xi​​ 其中： xix_ixi​ 是输入的第 iii 个元素（例如某个token的特征值）。 NNN 是特征维度的大小。 ϵ\\epsilonϵ 是防止除零错误的小常数（例如 1e−61e-61e−6）。 x^i\\hat{x}_ix^i​ 是归一化后的输出。 1. 初始化 12self.weight = nn.Parameter(torch.ones(hidden_size))self.variance_epsilon = eps self.weight：一个可学习的参数，用于对归一化后的隐藏状态进行缩放，初始值为 1。 self.variance_epsilon：小常数 eps，用于防止除零错误。 2. 前向传播 1234hidden_states = hidden_states.to(torch.float32)variance = hidden_states.pow(2).mean(-1, keepdim=True)hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)return self.weight * hidden_states.to(input_dtype) 计算方差：首先将输入 hidden_states 转换为 float32，然后沿特征维度计算方差。 均方根归一化：使用 torch.rsqrt() 对方差加上小常数 eps 后取平方根的倒数，进行归一化处理。 缩放和恢复数据类型：归一化后的结果乘以可学习的 weight 参数，并恢复到输入的原始数据类型。 3. 额外显示信息 12def extra_repr(self): return f&quot;&#123;tuple(self.weight.shape)&#125;, eps=&#123;self.variance_epsilon&#125;&quot; extra_repr() 用于返回该层的额外信息，便于调试。 总结 LlamaRMSNorm 是一种优化的归一化方法，常用于 LLaMA 模型。它通过计算输入的方差并对其进行均方根归一化，使用可学习的参数进行缩放，帮助模型在训练过程中保持稳定性。与传统的标准化方法相比，RMSNorm 在一些任务中表现出更好的效果，尤其是在处理大规模模型时。 9. 代码调试深入理解 为了能够更深入的理解Llama，可以对下面的代码进行调试，一步一步调试进去就可以对Llama3模型的架构掌握的更加清晰： 123456789101112131415161718192021from transformers.models.llama import LlamaModel, LlamaConfigimport torchdef run_llama(): # 1. 模型初始化 llamaconfig = LlamaConfig(vocab_size=32000, hidden_size=4096//2, intermediate_size=11000//2, num_hidden_layers=32//2, num_attention_heads=32//2, max_position_embeddings=2048//2) llamamodel = LlamaModel(config=llamaconfig) # 2. 定义输入 batchsize=4 inputs_ids = torch.randint( low=0, high=llamaconfig.vocab_size, size=(4, 30)) # 3. 进行推理forward res = llamamodel(inputs_ids) print(res) if __name__ == &#x27;__main__&#x27;: run_llama() 在这段代码中，我们使用 Hugging Face 的 transformers 库来初始化和运行一个简化版的 LLaMA 模型。LLaMA 是一种基于 Transformer 架构的大语言模型，广泛应用于自然语言处理任务。代码首先通过 LlamaConfig 定义了模型的配置参数，如词汇表大小、隐藏层维度、注意力头数等，并将原始模型的参数减半以降低计算成本。接着，我们生成了一个随机的输入张量，形状为 (4, 30)，表示 4 个样本，每个样本长度为 30。最后，将输入传递给模型进行推理，并输出结果。这段代码展示了如何快速搭建和运行一个简化版的 LLaMA 模型，适合初学者了解模型的基本使用流程。 根据代码调试，不难知道，针对上面这个代码，hidden_states的大小为[4, 30, 2048]因为中间是transformer结构，所以hidden_states的大小不会发生变化（多头注意力的时候是先proj再分多头） 更完整的一个推理过程调试可以采用下面这个代码： 1234567891011121314151617181920212223242526272829from transformers import AutoTokenizer, LlamaForCausalLMimport torch# 指定模型路径model_path = &quot;/home/vegetabot/Filesys/CodeField_win/LLaMA-Factory/Meta-Llama-3-8B-Instruct&quot;# ✅ 使用 AutoTokenizer，让它自动匹配 Llama 3 的 tokenizertokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False)# ✅ 直接使用 LlamaForCausalLMmodel = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(&quot;cuda&quot;)# 预编译模型，加速推理model = torch.compile(model)print(&quot;✅ Model Compilation Complete!&quot;)# 输入文本# input_text = &quot;你好，请问 Llama 3 有哪些新特性？请使用中文回答&quot;input_text = &quot;你好，请问Llama 3 有哪些新特性？请使用中文回答&quot;# 编码输入文本inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)# 生成文本（使用 KV Cache 加速）outputs = model.generate(**inputs, max_new_tokens=1000, use_cache=True)# 解码输出generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)print(&quot;🤖 Llama 3 生成的回答： &quot;, generated_text) 在这段优化后的代码中，我们使用 Hugging Face 的 transformers 库快速调用 Meta-Llama-3-8B-Instruct 模型进行中文对话生成。首先通过 AutoTokenizer 和 AutoModelForCausalLM 加载本地预训练的分词器和模型（需提前下载模型权重），并以 bfloat16 精度量化模型以降低显存占用。接着利用 torch.compile 对模型进行编译优化，加速推理效率。输入问题 “你好，请问 Llama 3 有哪些新特性？” 被编码为 GPU 张量后，模型通过 generate 方法生成最多 1000 个新 token 的回答，最终解码输出自然流畅的中文文本。整个过程展示了如何高效部署大语言模型并进行交互式推理。 经过调试，不难知道，输入的query首先经过tokenizer被编码成了[1,20]大小的向量。然后再进模型进行推理，其中hidden_states的大小为[1,20,4096],使用的attention是sdp attention，然后我们的RoPE作用在Q K上，注意力机制的头数为32。 对于MLP层，隐藏层是从4096先变化到14336然后再被映射回来，采用的激活函数是SiLU。一共是有32层Decoder Layer 过对LLaMA架构的深入解析，我们可以看到，它的设计巧妙地平衡了性能与效率，为自然语言处理领域提供了强大的工具。无论是研究者还是开发者，LLaMA的开源都为我们探索语言模型的潜力打开了新的大门。希望这篇博客能帮助你更好地理解LLaMA，也希望它能激发你对大语言模型的更多兴趣与思考。未来已来，让我们一起期待更多创新与突破！","tags":["LLM","llama"],"categories":["LLM"]},{"title":"【论文笔记】AWQ","path":"/2025/01/06/MudSynth/Quant/【论文笔记】AWQ/","content":"文章基本信息 文章名称：AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration 发表会议/年份：MLSys 2024 作者：Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang 单位：MIT, SJTU, NVIDIA, Tsinghua MIT-IBM, UMass 摘要 大型语言模型（LLMs）显著提升了许多AI应用的性能，而将其部署到设备端可以降低云计算成本并保护用户隐私。然而，模型规模过大和硬件资源有限使得部署面临挑战。本文提出了Activation-aware Weight Quantization (AWQ)，一种面向硬件的低比特量化方法，用于压缩LLM的权重。AWQ发现仅需保护约1%的重要权重即可显著降低量化误差，并通过参考激活分布（activation distribution）而非权重分布来确定重要权重。为避免混合精度量化带来的硬件效率低下，AWQ通过数学推导保护重要权重通道并利用激活统计数据进行缩放。该方法无需反向传播或重建，因此能够很好地泛化到不同领域和模态，而不会过拟合校准数据集。AWQ在多种任务和模型上优于现有方法，首次实现了指令调优模型和多模态LLM的优异量化性能。同时，本文实现了高效灵活的推理框架TinyChat，将4-bit LLM部署到多种边缘平台，在桌面和移动GPU上实现了超过3倍的加速。 之前工作存在的问题 低比特量化在模型推理中可以显著减少内存占用，但当前的量化感知训练（QAT） 成本高昂，而训练后量化（PTQ） 在低比特场景下准确性大幅下降。 GPTQ方法虽然使用二阶信息进行误差补偿，但可能在重建过程中过拟合校准集，从而失去对分布外领域的泛化能力。 混合精度量化（如FP16和低比特结合）虽然可以提升性能，但在硬件实现中效率较低。 主要贡献/创新 提出了基于激活感知（activation-aware） 的低比特权重量化方法AWQ，能够显著降低量化误差，同时避免过拟合。 开发了一个硬件友好的权重缩放方法，保护重要权重通道，避免硬件效率低下的混合精度实现。 实现了支持4-bit推理的高效框架TinyChat，通过核融合(kernel fusion)和平台感知的权重打包(weight packing)，在多种边缘设备上实现了显著的推理加速。 AWQ首次在多模态模型（如OpenFlamingo和LLaVA）上验证了其量化效果，并将Llama-2-70B部署到移动GPU上，具有广泛适应性和低资源需求。 相关工作 模型量化方法 模型量化主要有两种方法，量化感知训练（QAT）和训练后量化（PTQ）。QAT需要反向传播更新权重，而PTQ通常无需训练。由于QAT难以扩展到大型模型，因此对大型语言模型（LLMs）的量化通常采用PTQ方法。量化能够减少模型尺寸并加速推理。 LLMs的量化 针对LLMs的量化研究有两种设置： W8A8量化，将激活值和权重均量化为INT8格式（Dettmers et al., 2022; Xiao et al., 2022; Yao et al., 2022; Wei et al., 2022a; 2023）。 低位权重量化（Low-bit weight-only quantization），例如W4A16，仅对权重进行低位整数的量化（Frantar et al., 2022; Dettmers &amp; Zettlemoyer, 2022; Sheng et al., 2023; Park et al., 2022）。 本文的研究聚焦于第二种设置，因为它不仅降低了硬件门槛（需要更小的内存容量），还加快了token生成速度（缓解了内存受限的工作负载）。除了基础的“舍入到最近值”方法（Round-to-Nearest，RTN），GPTQ（Frantar et al., 2022）是与本研究最接近的方法。然而，GPTQ的重建过程会导致校准集过拟合的问题，可能无法保持LLMs在其他模态和领域中的泛化能力。此外，它对某些模型（例如LLaMA-7B（Touvron et al., 2023a）和OPT-66B（Zhang et al., 2022））需要重新排序的技巧才能正常工作。 除了针对通用硬件的量化方法外，SpAtten（Wang et al., 2020）设计了一种渐进式的方法，通过逐步增加softmax计算中使用的位数来优化性能。 低位量化LLMs的系统支持 低位量化的大型语言模型（LLMs）因其能够降低推理成本而成为一种热门设置。目前已有一些系统支持以实现实际的加速效果。例如： GPTQ（Frantar et al., 2022）为OPT模型提供了INT3内核，并通过Triton（Tillet et al., 2019）的支持，扩展了对LLaMA模型INT4重排序量化的内核支持。 FlexGen（Sheng et al., 2023）、llama.cpp 和 exllama 执行组间INT4量化，以减少I/O成本和卸载数据的开销。 FasterTransformer 实现了基于FP16×INT4的GEMM（通用矩阵乘法）用于权重量化（每个张量的量化），但不支持组间量化。 LUT-GEMM（Park et al., 2022）借助查找表，在GPU的CUDA核心上执行基于比特的计算。 本文的并行工作MLC-LLM（MLC-Team, 2023），通过强大的TVM后端（Chen et al., 2018; Feng et al., 2023），在多种边缘CPU和GPU平台上取得了优异的结果。 AWQ: ACTIVATION-AWARE WEIGHT QUANTIZATION 图2:我们观察到，我们可以根据激活分布（中）找到 LLM 中 1% 的显着权重。将显着权重保留在 FP16 中可以显着提高量化性能（PPL 从 43.2（左）到 13.0（中）），但混合精度格式并不具有硬件效率。我们遵循激活感知原则并提出 AWQ（右）。 AWQ 执行每通道缩放以保护显着权重并减少量化误差。我们测量了 INT3-g128 量化下 OPT-6.7B 的困惑度。 量化将浮点数映射为低位整数，是一种有效的方法，可以减小LLMs（大型语言模型）的模型尺寸并降低推理成本（Dettmers et al., 2022; Frantar et al., 2022; Yao et al., 2022; Xiao et al., 2022）。在本节中，我们首先提出了一种仅对权重进行量化的方法，通过保护更多“重要”的权重，在无需训练或回归（without training/regression） 的情况下提高准确性。随后，我们开发了一种数据驱动方法，用于搜索减少量化误差的最优缩放比例（详见图2）。 1. 通过保留 1% 的显著权重改进 LLM 量化 我们观察到，大型语言模型（LLMs）的权重并不是同等重要的：其中有一小部分显著权重（salient weights）对 LLM 的性能更为关键。跳过这些显著权重的量化过程可以在不进行任何训练或回归（如图 2(b) 所示）的情况下，减少量化损失带来的性能下降。 为验证这一观点，我们在表 1 中测试了跳过部分权重通道的量化 LLM 的性能。我们测量了 INT3 量化模型的性能，同时将部分权重通道保留为 FP16（16位浮点数）。一种广泛使用的方法是通过权重的范数（如 L2L_2L2​-范数）来确定权重的重要性（Han et al., 2015; Frankle &amp; Carbin, 2018）。然而，我们发现基于权重 L2L_2L2​-范数（即以权重 WWW 为基础的 FP16%）跳过权重通道并不能显著提升量化模型的性能，其效果与随机选择的提升幅度类似。 有趣的是，基于激活幅值（activation magnitude） 选择权重可以显著提升性能，即便只保留 0.1%-1% 的通道为 FP16。我们推测，输入特征的幅值较大时通常更重要，保留这些对应权重为 FP16 可以保留这些特征，从而有助于模型性能的提升。 限制 尽管仅保留 0.1% 的权重为 FP16 可以在量化性能上实现改进且对模型大小（以总比特数衡量）没有显著影响，但这种混合精度数据类型会增加系统实现的难度。我们需要找到一种方法，在不实际将权重保留为 FP16 的情况下保护这些重要权重。 2. 通过激活感知缩放保护显著权重 我们提出了一种替代方法，通过逐通道缩放（per-channel scaling） 来减少显著权重的量化误差。该方法避免了硬件低效问题。 分析量化误差 我们首先从权重量化误差开始分析。设一组或一个区块的权重为 www，线性操作可表示为 y=wxy = wxy=wx，而量化后的对应操作为 y=Q(w)xy = Q(w)xy=Q(w)x。具体来说，量化函数定义如下： Q(w)=Δ⋅Round(wΔ),Δ=max⁡(∣w∣)2N−1Q(w) = \\Delta \\cdot \\text{Round}\\left(\\frac{w}{\\Delta}\\right), \\quad \\Delta = \\frac{\\max(|w|)}{2^{N-1}} Q(w)=Δ⋅Round(Δw​),Δ=2N−1max(∣w∣)​ 其中，NNN 表示量化的比特数，Δ\\DeltaΔ 是根据绝对最大值确定的量化缩放因子。 现在考虑 www 中的一个权重元素 www，如果将其乘以 s&gt;1s &gt; 1s&gt;1，并将输入 xxx 按比例缩小为 xs\\frac{x}{s}sx​，则我们有： Q(ws)⋅xs=Δ′⋅Round(wsΔ′)⋅xsQ(ws) \\cdot \\frac{x}{s} = \\Delta&#x27; \\cdot \\text{Round}\\left(\\frac{ws}{\\Delta&#x27;}\\right) \\cdot \\frac{x}{s} Q(ws)⋅sx​=Δ′⋅Round(Δ′ws​)⋅sx​ 其中，Δ′\\Delta&#x27;Δ′ 是应用 sss 后的新量化缩放因子。通过实验，我们发现以下结论： 量化舍入误差（RoundErr）的期望值不变：因为舍入函数将浮点数映射到整数，其误差在 [0,0.5][0, 0.5][0,0.5] 之间均匀分布，平均误差为 0.250.250.25（即 RoundErr∼0.25\\text{RoundErr} \\sim 0.25RoundErr∼0.25）。 单独放大权重 www 通常不会改变整个组的最大值，因此 Δ′≈Δ\\Delta&#x27; \\approx \\DeltaΔ′≈Δ。 Δ\\DeltaΔ 和 Δ′\\Delta&#x27;Δ′ 在 FP16 中表示，不会引入额外量化误差。 因此，量化误差可以表达为： Err(Q(w)x)=Δ⋅RoundErr(wΔ)⋅x\\text{Err}(Q(w)x) = \\Delta \\cdot \\text{RoundErr}\\left(\\frac{w}{\\Delta}\\right) \\cdot x Err(Q(w)x)=Δ⋅RoundErr(Δw​)⋅x Err(Q(ws)xs)=Δ′⋅RoundErr(wsΔ′)⋅xs\\text{Err}\\left(Q(ws)\\frac{x}{s}\\right) = \\Delta&#x27; \\cdot \\text{RoundErr}\\left(\\frac{ws}{\\Delta&#x27;}\\right) \\cdot \\frac{x}{s} Err(Q(ws)sx​)=Δ′⋅RoundErr(Δ′ws​)⋅sx​ 新误差与原始误差的比值为： Δ′Δ⋅1s\\frac{\\Delta&#x27;}{\\Delta} \\cdot \\frac{1}{s} ΔΔ′​⋅s1​ 由于 Δ′≈Δ\\Delta&#x27; \\approx \\DeltaΔ′≈Δ 且 s&gt;1s &gt; 1s&gt;1，显著权重 www 的相对误差会减小。 实验验证 为了验证这一思想，我们对 OPT-6.7B 模型的 1% 显著通道进行放大（乘以 s&gt;1s &gt; 1s&gt;1），并测量每组的 Δ\\DeltaΔ 变化（见表 2）。结果表明，放大显著通道效果显著：当 s=1s = 1s=1（即简单的 RTN 方法）时，困惑度（perplexity, PPL）为 23.54，而 s=2s = 2s=2 时，PPL 降至 11.92。 随着 sss 增大，变化的 Δ\\DeltaΔ 比例一般会变大，但对于 s&lt;2s &lt; 2s&lt;2，该比例仍然较小（小于 5%）。同时，显著通道的相对误差随着 sss 的增加而继续减小。然而，当 sss 非常大时，Δ\\DeltaΔ 的增加会放大非显著通道的相对误差（比例为 Δ′Δ\\frac{\\Delta&#x27;}{\\Delta}ΔΔ′​），并且当 s=4s = 4s=4 时，这种比例放大影响了 21.2% 的非显著通道，可能损害模型的整体准确性。 搜索最优比例 为了同时考虑显著权重和非显著权重，我们选择自动搜索每个输入通道的最优缩放因子，以最小化量化后在某一层的输出差异。正式地，我们需要优化以下目标函数： s∗=arg⁡min⁡sL(s)s^* = \\arg\\min_s \\mathcal{L}(s) s∗=argsmin​L(s) L(s)=∥Q(W⋅diag(s))(diag(s)−1⋅X)−WX∥\\mathcal{L}(s) = \\| Q(W \\cdot \\text{diag}(s))(\\text{diag}(s)^{-1} \\cdot X) - WX \\| L(s)=∥Q(W⋅diag(s))(diag(s)−1⋅X)−WX∥ 其中，QQQ 表示权重量化函数（例如 INT3/INT4 量化，组大小为 128），WWW 是原始的 FP16 权重，XXX 是从小型校准集（calibration set）中缓存的输入特征。校准集从预训练数据集中随机抽取，避免针对特定任务过拟合。sss 是每个输入通道的缩放因子；对于 s−1s^{-1}s−1 和 XXX，它通常可以与前一个算子融合（参考 Wei et al., 2022b; Xiao et al., 2022）。 由于量化函数不可微分，无法通过常规反向传播优化。尽管一些技术依赖于近似梯度（Bengio et al., 2013; Esser et al., 2019），但我们发现这些方法在收敛性上仍然不够稳定。 为使优化过程更加稳定，我们通过分析影响缩放因子选择的因素，定义了一个搜索空间。如上一节所述，权重通道的显著性实际上由激活幅值（activation scale）决定（因此称为“激活感知”）。因此，我们使用一个非常简单的搜索空间： s=sXα,α∗=arg⁡min⁡αL(sXα)s = s_X^\\alpha, \\quad \\alpha^* = \\arg\\min_\\alpha \\mathcal{L}(s_X^\\alpha) s=sXα​,α∗=argαmin​L(sXα​) 其中，sXs_XsX​ 是每通道激活幅值的平均值，我们通过单个超参数 α\\alphaα 来平衡显著通道与非显著通道的保护。通过快速网格搜索，可以在区间 [0,1][0, 1][0,1] 中找到最佳 α\\alphaα（0 表示不进行缩放，1 表示搜索空间中的最强缩放）。我们还通过应用权重裁剪（weight clipping）来最小化量化的均方误差（MSE）。 在表 5 中，我们对 OPT 模型在 INT3-g128 量化下进行了消融研究。AWQ（激活感知量化）始终优于最近舍入量化（RTN），并且在硬件友好的情况下实现了与混合精度（1% FP16）相当的性能。 优势 我们的方法不依赖任何回归（Frantar et al., 2022）或反向传播，这是许多量化感知训练方法所需的。此外，该方法对校准集的依赖极少，因为我们仅测量每个通道的平均幅值，从而避免了过拟合（如图 8 所示）。因此，我们的方法在量化过程中需要更少的数据，并且能够保留 LLM 的知识，使其不受校准集分布的限制。有关更多细节，请参见第 5.3 节。 TINYCHAT: MAPPING AWQ ONTO EDGE PLATFORMS AWQ 可以显著减小 LLM 的模型大小。然而，将 W4A16（4-bit 权重，16-bit 激活）的量化所带来的理论内存节省转化为实际的加速效果并非易事。另一种替代方法，例如 SmoothQuant（Xiao et al., 2022），在存储和计算中保持相同的数据精度（W8A8 量化方法）。这允许反量化过程无缝集成到计算内核的结尾阶段。 另一方面，W4A16 量化为内存访问和计算引入了不同的数据类型。因此，其反量化过程必须被集成到主计算循环中以实现最佳性能，这增加了实现的难度。 为解决这一问题，我们提出了 TinyChat：一个灵活的系统，用于 AWQ 模型的推理。它采用 PyTorch 前端，并使用特定设备指令集（例如 CUDA/PTX、Neon、AVX）的后端来增强性能。 1. 为什么 AWQ 能加速本地设备上的 LLM 图3：Llama-2-7B 模型在 NVIDIA RTX 4090 上的瓶颈分析。 左图： 在本地设备上的 LLM 应用中，生成阶段比上下文阶段慢得多。 中图： 生成阶段是内存受限的，且具有较低的算术强度（arithmetic intensity）。W4A16 量化可以将算术强度提高 4 倍。 右图： 权重访问量比激活访问量高出若干个数量级。因此，权重量化对于本地设备上的 LLM 更为有效。 为了理解量化 LLM 在边缘设备上的加速潜力，我们首先对 LLaMA-7B（Touvron et al., 2023a）模型在 RTX 4090 GPU 上的延迟组成进行了分析。我们采用推理批量大小为 1，以适应边缘设备的使用场景，并在 NVIDIA FasterTransformer 中使用 FP16 实现该模型。 上下文阶段 vs. 生成阶段的延迟 如图 3(a) 所示，生成 20 个 token 需要 310 毫秒，而对包含 200 个 token 的提示（prompt）进行摘要仅需 10 毫秒。因此，生成阶段的延迟显著高于上下文阶段，尤其是在针对交互式本地设备应用时。 生成阶段是内存受限的 为了加速生成阶段，我们在图 3(b) 中进行了 Roofline 分析。RTX 4090 GPU 的峰值计算吞吐量为 165 TFLOPS，内存带宽为 1TB/s。因此，任何算术强度（即计算与内存访问的比率）小于 165 的工作负载都受到内存限制。 值得注意的是，当以 FP16 执行时，本地设备 LLM 的生成阶段算术强度约为 1。这表明工作负载的内存受限特性。由于给定模型的 FLOPs 是固定的，唯一提高峰值性能的方法是减少内存访问量。AWQ 通过将权重内存减少至原来的四分之一，显著降低了内存流量。 权重访问占主导的内存流量 我们进一步对权重和激活的内存访问进行了分解分析，如图 3© 所示。显然，权重访问占据了本地设备 LLM 的大部分内存流量。将模型权重量化为 4-bit 整数，可以将算术强度从 1 FLOPs/Byte 提升到约 4 FLOPs/Byte，从而在图 3(b) 中达到 4TFLOPS 的峰值性能。 由于权重量化减少了权重的位宽（因此提高了理论性能上限），对于本地设备上的 LLM 应用，遵循这种量化设置是合理的。AWQ 专注于权重量化，非常适合此类应用场景。 2. 使用 TinyChat 部署 AWQ 我们证明了 4-bit 权重量化可以带来 4 倍的理论峰值性能提升。为了实现这种加速效果，我们进一步设计了 TinyChat 系统。在 GPU 上，我们专注于实现关键组件，包括注意力机制、层归一化和线性投影内核。灵活的前端允许轻松定制并快速支持新模型。TinyChat 使用 4-bit AWQ，与 Huggingface 的 FP16 实现相比，在不同家族的 LLM 上获得了超过 3 倍的加速。在 CPU 上，我们将整个计算图转为 C++ 实现，以最小化开销。 即时权重反量化（On-the-fly Weight Dequantization） 对于量化层，由于硬件通常不提供 INT4 和 FP16 之间的乘法指令，我们需要将整数即时反量化为 FP16。 SIMD 感知权重打包（SIMD-aware Weight Packing） 即时反量化减少了对中间 DRAM 的访问，但仍然比较昂贵。例如，反量化单个 4-bit 权重需要执行 1 次移位（shift）、1 次按位与操作（bitwise AND）和 1 次 FMA（乘加运算）缩放操作，而反量化的权重只进行 1 次 FMA 计算。 这一过程在具有 SIMD 架构（单指令多数据）的 CPU 上尤其昂贵，因为这些架构偏向于矢量化指令。为此，我们建议根据设备的 SIMD 单元位宽量身定制权重打包方案。 图 4 展示了我们针对 ARM CPU（具有 128-bit SIMD 寄存器）的策略，这种策略提供了高达 1.2 倍的加速。在这种方法中，每个寄存器包含 32 个 4-bit 权重，按照以下顺序存储：w0,w16,w1,w17,...,w15,w31w_0, w_{16}, w_1, w_{17}, ..., w_{15}, w_{31}w0​,w16​,w1​,w17​,...,w15​,w31​。该方法只需要 3 条 SIMD 指令即可解包所有 32 个权重，而传统打包方式中，每个权重需要 3 条标量指令来解包（如 w0,w1,...,w31w_0, w_1, ..., w_{31}w0​,w1​,...,w31​）。 一般而言，对于 2n2^n2n-bit 的 SIMD 寄存器，相邻权重的索引偏移量为 1/8×2n1/8 \\times 2^n1/8×2n，因为每个寄存器可以容纳 1/8×2n1/8 \\times 2^n1/8×2n 个 8-bit 整数。在 GPU 上，我们发现将每 8 个权重打包为 w0,2,4,6,1,3,5,7w_{0,2,4,6,1,3,5,7}w0,2,4,6,1,3,5,7​ 更为高效（参考 Kim et al., 2022）。 内核融合（Kernel Fusion） 我们还广泛应用内核融合来优化本地设备上的 LLM 推理。对于层归一化（layer normalization），我们将所有算子（如乘法、除法和平方根）融合到单个内核中。对于注意力层（attention layers），我们将 QKV 投影融合到一个内核中，并在其中即时计算位置嵌入（positional embedding）。此外，我们在注意力内核中预分配 KV 缓存并进行缓存更新。 内核融合对于前向传播实现效率较低的模型（如 Falcon 和 StarCoder）尤为有用。例如，在 4090 GPU 上，每个 FP16 内核的计算时间约为 0.01 毫秒，这与 GPU 内核的启动开销相当。因此，通过内核融合减少内核调用数量可以直接实现加速效果。 实验结果 实验设置 量化 本研究重点关注仅针对权重的分组量化（weight-only grouped quantization）。如之前的研究（Dettmers &amp; Zettlemoyer, 2022；Frantar et al., 2022）所示，分组量化对提高性能和模型尺寸的权衡（performance/model size trade-off）总是有帮助的。在本文中，除非特别说明，我们在实验中使用了组大小为128（group size = 128）。我们主要研究了 INT4/INT3 量化方法，因为这些方法可以很好地保留大语言模型（LLMs，Large Language Models）的性能（Dettmers &amp; Zettlemoyer, 2022）。对于 AWQ 方法，我们从 Pile 数据集（Gao et al., 2020）中选取了一个小型校准集，以避免过拟合到某个特定的下游领域。此外，我们使用了网格大小为20（grid size = 20）来搜索公式5中最优的 α\\alphaα。 模型 我们在 LLaMA（Touvron et al., 2023a）和 OPT（Zhang et al., 2022）系列模型上对方法进行了基准测试。此外，尽管还有其他开源的大语言模型（如 BLOOM（Scao et al., 2022）），但由于其质量通常较差，我们未将其纳入研究中。我们还进一步在指令调优模型 Vicuna（Chiang et al., 2023）以及视觉语言模型 OpenFlamingo-9B（Awadalla et al., 2023）和 LLaVA-13B（Liu et al., 2023a）上进行了测试，以验证方法的通用性（generability）。 评价 按照之前的文献（Dettmers et al., 2022；Xiao et al., 2022；Frantar et al., 2022；Dettmers &amp; Zettlemoyer, 2022；Yao et al., 2022），我们主要在语言建模任务中评估量化模型的表现（例如 WikiText-2 数据集上的困惑度评估（perplexity evaluation），Merity et al., 2016），因为困惑度能够稳定地反映大语言模型（LLMs）的性能（Dettmers &amp; Zettlemoyer, 2022）。 基线 我们的主要基线是基础的最近舍入量化（vanilla round-to-nearest quantization, RTN）。当使用小的组大小（如128）时，RTN 实际上表现非常强（Frantar et al., 2022；Dettmers &amp; Zettlemoyer, 2022）。此外，我们还与最先进的量化方法 GPTQ（Frantar et al., 2022）进行了比较。对于 GPTQ，我们还比较了一种改进版本，该版本采用了“重排序（reorder）”技巧（称为 GPTQ-Reorder 或 GPTQ-R）。其他方法，如 ZeroQuant（Yao et al., 2022）、AdaRound（Nagel et al., 2020）和 BRECQ（Li et al., 2021），依赖于反向传播来更新量化权重，这些方法在扩展到大模型规模时可能会遇到困难；同时，它们的性能也不如 GPTQ（Frantar et al., 2022），因此未纳入研究范围。 评价结果 LLaMA 模型：AWQ 在 LLaMA 和 LLaMA-2 模型上的评估显示，在量化前后及不同模型规模（7B-70B）下，其性能始终优于其他方法（如 RTN 和 GPTQ）。 Mistral/Mixtral 模型：AWQ 在 Mistral 和 Mixtral 模型中也表现优异，证明了其在不同模型架构中的通用性和有效性。 指令调优模型：AWQ 方法在指令调优模型 Vicuna 上表现优异，使用 GPT-4 得分评估，证明了其在量化配置下对模型性能的增强效果以及在指令调优任务中的通用性。 多模态语言模型：AWQ 在多模态模型（如 OpenFlamingo-9B 和 VILA）上实现了高效的低比特量化，展示了其在零样本和小样本任务中的优越性，并提供了显著的模型压缩能力，同时性能几乎无损。 视觉推理结果：在 LLaVA-13B 模型的视觉推理任务中，AWQ 在量化配置下比 RTN 产生了更合理的回答，例如能够正确理解图像内容。 编程与数学任务结果：AWQ 在编程（MBPP 数据集）和数学（GSM8K 数据集）任务中表现出色，在量化配置（INT4-g128）下实现了与 FP16 模型相当的性能，同时优于其他量化方法。 校准集的数据效率和泛化能力 数据效率：AWQ 方法对校准集需求较小，仅需 GPTQ 所需校准集规模的1/10，即可实现更优的量化性能。 泛化能力和鲁棒性：AWQ 在不同校准集分布下表现出优异的鲁棒性，其困惑度的增加幅度显著小于 GPTQ，证明了其方法对校准集分布变化的低敏感性和更强的泛化能力。 加速性能评估 加速结果：TinyChat 对主流 LLM（如 LLaMA-2 和 Falcon）在不同硬件平台上提供了显著加速效果，尤其在资源受限的设备上仍表现优异。 与其他系统比较：相比现有推理系统（如 llama.cpp 和 exllama），TinyChat 展现了更广的适配性和更高的加速性能，同时支持多种模型并显著快于 AutoGPTQ。 总结 在本研究中，我们提出了一种名为激活感知权重量化（Activation-aware Weight Quantization, AWQ）的方法。这是一种简单但有效的低比特权重压缩技术，专用于大语言模型（LLMs）。基于权重在 LLM 中的重要性并不均等的观察，AWQ 对每个通道进行缩放（per-channel scaling），以减少显著权重的量化损失。 AWQ 不会对校准集过拟合，同时保留了 LLM 在各种领域和模态上的通用能力。它在语言建模任务上优于现有方法，并适用于指令调优模型（instruction-tuned LMs）和多模态模型（multi-modal LMs）。 我们的 TinyChat 系统进一步将 AWQ 实现的理论内存节省转化为实测的 3.2-3.3 倍加速效果（相较于 Huggingface 的 FP16 实现），支持桌面和移动 GPU。这使得边缘设备上的 LLM 部署成为可能，从而推动了 LLM 的普及化。 方法亮点：AWQ 提出了一种通道级缩放的量化方法，减少显著权重的量化损失，同时避免了对校准集的过拟合。 适用性：AWQ 适用于多种任务和模型类型，包括语言建模、指令调优模型和多模态模型。 加速性能：通过 TinyChat 系统，AWQ 在桌面和移动 GPU 上实现了 3.2-3.3 倍的加速，显著提升了低资源设备上 LLM 部署的可行性。","tags":["LLM","Quant","PTQ"],"categories":["LLM","Quant"]},{"title":"【论文笔记】Re-ranking","path":"/2024/09/26/MudSynth/ReID/【论文笔记】Re-ranking/","content":"文章基本信息 文章名称：Re-ranking Person Re-identification with k-reciprocal Encoding 发表会议/年份：CVPR 2017 作者：Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li 单位：Cognitive Science Department, Xiamen University, China / University of Technology Sydney / Fujian Key Laboratory of Brain-inspired Computing Technique and Applications, Xiamen University 摘要 在将人员重新识别（Re-ID）视为检索过程时，重排名是提高其准确性的重要步骤。然而，在Re-ID领域，投入到重排名上的努力有限，尤其是那些完全自动化的、无监督的解决方案。在本文中，我们提出了一种k-互惠编码方法来重新排名Re-ID的结果。我们的假设是，如果图库图像在k-互惠最近邻中与探针图像相似，那么它更有可能是真正的匹配。具体来说，给定一张图像，通过将其k-互惠最近邻编码为单个向量来计算k-互惠特征，该向量用于基于Jaccard距离的重新排名。最终距离计算为原始距离和Jaccard距离的组合。我们的重排名方法不需要任何人工干预或标记数据，因此适用于大规模数据集。在大规模Market-1501、CUHK03、MARS和PRW数据集上的实验结果证实了我们方法的有效性。 之前工作存在的问题 过往的重排名方法依赖于初始排名列表的质量，通过利用初始排名列表中高排名图像之间的相似关系进行重排名。然而，这些方法存在以下问题： 初始排名列表中可能包含错误匹配，这些错误匹配可能会被误认为是真正的匹配，从而影响重排名的准确性； 即使真正的匹配存在，它们也可能不会出现在初始排名列表的前几名，从而导致重排名时的噪声增加，最终结果被破坏。 比如如上图1所示，在图1中，P1、P2、P3和P4是探针的四个真正匹配，但它们都不在前四名排名中。我们观察到一些错误匹配（N1-N6）获得了高排名。因此，直接使用前k名排名图像可能会在重排名系统中引入噪声，影响最终结果。 主要贡献/创新 本文介绍了一种基于k-互惠近邻的重新排名方法，用于改进图像重新识别（re-ID）系统的性能。通过将加权的k-互惠邻居集编码为向量，计算图像之间的Jaccard距离，并结合局部查询扩展方法，最终得到改进的重新排名列表。该方法无需人工交互或标注数据，且能自动且无监督地应用于人重新识别排名，显著提高了多个数据集上的重新识别性能。 本文的贡献可以概括如下： 我们提出了通过将k-互惠特征编码为一个向量的k-互惠特征。重新排名过程可以通过向量比较轻松完成。 我们的方法不需要任何人工交互或标注数据，可以自动且无监督地应用于任何人重新识别排名结果。 所提出的方法有效提高了多个数据集上的人重新识别性能，包括Market-1501、CUHK03、MARS和PRW。特别是在Market-1501数据集上，我们在rank-1和mAP上都达到了最先进的精度。 方法 问题定义 给定一个探测者p和包含N幅图像的图库集合G={gi∣i=1,2,…,N}\\mathcal{G} = \\{ g_i \\mid i = 1, 2, \\ldots, N \\}G={gi​∣i=1,2,…,N}，两个人物p和gig_igi​之间的原始距离可以通过马氏距离（Mahalanobis distance）来测量： d(p,gi)=(xp−xgi)⊤M(xp−xgi)d(p, g_i) = (x_p - x_{g_i})^\\top \\mathbf{M} (x_p - x_{g_i}) d(p,gi​)=(xp​−xgi​​)⊤M(xp​−xgi​​) 其中xpx_pxp​和xgix_{g_i}xgi​​分别是人物p和gig_igi​的特征向量，M\\mathbf{M}M是一个正定矩阵。 初始排名列表 L(p,G)={g10,g20,…,gN0}\\mathcal{L}(p, \\mathcal{G}) = \\{g_1^0, g_2^0, \\ldots, g_N^0\\}L(p,G)={g10​,g20​,…,gN0​} 可以根据探测者 ppp 和图库 gig_igi​ 之间的成对原始距离获得，其中 d(p,gi0)&lt;d(p,gi+10)d(p, g_i^0) &lt; d(p, g_{i+1}^0)d(p,gi0​)&lt;d(p,gi+10​)。我们的目标是重新排序 L(p,G)\\mathcal{L}(p, \\mathcal{G})L(p,G)，以使更多的正样本排在列表的顶部，从而提高人物重新识别（re-ID）的性能。 K-reciprocal Nearest Neighbors 按照(35)，我们将 N(p,k)N(p, k)N(p,k) 定义为探测者 ppp 的k-近邻（即排名列表的前k个样本）： N(p,k)={g10,g20,…,gk0},∣N(p,k)∣=kN(p, k) = \\{g_1^0, g_2^0, \\ldots, g_k^0\\}, \\quad |N(p, k)| = k N(p,k)={g10​,g20​,…,gk0​},∣N(p,k)∣=k 其中 ∣⋅∣|\\cdot|∣⋅∣ 表示集合中候选项的数量。k-互惠近邻 R(p,k)\\mathcal{R}(p, k)R(p,k) 可以定义为： R(p,k)={gi∣(gi∈N(p,k))∧(p∈N(gi,k))}\\mathcal{R}(p, k) = \\{ g_i \\mid (g_i \\in N(p, k)) \\land (p \\in N(g_i, k)) \\} R(p,k)={gi​∣(gi​∈N(p,k))∧(p∈N(gi​,k))} 根据之前的描述，k-互惠近邻与探测者 ppp 的关系比k-近邻更密切。然而，由于光照、姿势、视角和遮挡的变化，正样本可能被排除在k-近邻之外，进而不包含在k-互惠近邻中。为了解决这个问题，我们通过逐步将R(p,k)\\mathcal{R}(p, k)R(p,k)中每个候选项的 12k\\frac{1}{2}k21​k 互惠近邻添加到一个更鲁棒的集合 R∗(p,k)\\mathcal{R}^*(p, k)R∗(p,k) 中来改进，依据如下条件： R∗(p,k)←R(p,k)∪R(q,12k)\\mathcal{R}^*(p, k) \\leftarrow \\mathcal{R}(p, k) \\cup \\mathcal{R}(q, \\frac{1}{2}k) R∗(p,k)←R(p,k)∪R(q,21​k) 条件是： ∣R(p,k)∩R(q,12k)∣≥23∣R(q,12k)∣,∀q∈R(p,k)|\\mathcal{R}(p, k) \\cap \\mathcal{R}(q, \\frac{1}{2}k)| \\geq \\frac{2}{3} |\\mathcal{R}(q, \\frac{1}{2}k)|, \\quad \\forall q \\in \\mathcal{R}(p, k) ∣R(p,k)∩R(q,21​k)∣≥32​∣R(q,21​k)∣,∀q∈R(p,k) qqq是原本p的k近邻 通过这种操作，我们可以将更多与R(p,k)\\mathcal{R}(p, k)R(p,k) 中候选项更相似的正样本添加到R∗(p,k)\\mathcal{R}^*(p, k)R∗(p,k)中，而不是与探测者 ppp 更相似的样本。与（35）相比，这在防止包含过多负样本方面更为严格。 Jaccard距离 在本小节中，我们通过比较探测者 ppp 和图库 gig_igi​ 的k-互惠近邻集来重新计算它们之间的成对距离。如之前工作所述 (2) (46)，我们认为如果两幅图像相似，它们的k-互惠近邻集会有重叠，即在集合中存在一些重复的样本。重复样本越多，两幅图像就越相似。探测者 ppp 和图库 gig_igi​ 之间的新距离可以通过它们的k-互惠集的Jaccard度量来计算，公式如下： dJ(p.gi)=1−∣R∗(p,k)∩R∗(gi,k)∣∣R∗(p,k)∪R∗(gi,k)∣d_J(p.g_i) = 1 - \\frac{|\\mathcal{R}^*(p,k)\\cap\\mathcal{R}^*(g_i,k)|}{|\\mathcal{R}^*(p,k)\\cup\\mathcal{R}^*(g_i,k)|} dJ​(p.gi​)=1−∣R∗(p,k)∪R∗(gi​,k)∣∣R∗(p,k)∩R∗(gi​,k)∣​ 其中 ∣⋅∣|\\cdot|∣⋅∣ 表示集合中候选项的数量。我们采用Jaccard距离来命名这个新距离。尽管上述方法能够捕捉到两幅图像之间的相似关系，但仍存在三个明显的缺点： 获取两个邻居集 R∗(p,k)\\mathcal{R}^*(p, k)R∗(p,k) 和 R∗(gi,k)\\mathcal{R}^*(g_i, k)R∗(gi​,k) 的交集和并集在很多情况下是非常耗时的，并且当需要计算所有图像对的Jaccard距离时，这变得更加具有挑战性。一个替代方法是将邻居集编码为一个更简单但等效的向量，从而大大减少计算复杂度，同时保持邻居集中的原始结构。 这种距离计算方法对所有邻居赋予了相同的权重，导致邻居集简单但缺乏辨别力。实际上，更接近探测者 ppp 的邻居更有可能是真正的正样本。因此，重新基于原始距离计算权重并为更近的样本赋予较大的权重是令人信服和合理的。 仅仅考虑上下文信息在测量两个人的相似性时会带来相当大的障碍，因为不可避免的变化使得难以区分足够的上下文信息。因此，将原始距离和Jaccard距离结合起来对于一个稳健的距离是很重要的。 受(2)的启发，提出了k-互惠特征来解决前两个缺点，通过将k-互惠近邻集编码为向量 Vp=[Vp,g1,Vp,g2,…,Vp,gN]\\mathbf{V}_p = [\\mathcal{V}_{p, g_1}, \\mathcal{V}_{p, g_2}, \\ldots, \\mathcal{V}_{p, g_N}]Vp​=[Vp,g1​​,Vp,g2​​,…,Vp,gN​​]，其中 Vp,gi\\mathcal{V}_{p, g_i}Vp,gi​​ 最初由一个二进制指示函数定义如下： Vp,gi={1如果 gi∈R∗(p,k)0否则\\mathcal{V}_{p, g_i} = \\begin{cases} 1 &amp; \\text{如果 } g_i \\in \\mathcal{R}^*(p, k) \\\\ 0 &amp; \\text{否则} \\end{cases} Vp,gi​​={10​如果 gi​∈R∗(p,k)否则​ 通过这种方式，k-互惠邻居集可以表示为一个N维向量，其中向量的每个项表示对应的图像是否包含在R∗(p,k)\\mathcal{R}^*(p, k)R∗(p,k)中。然而，这个函数仍然将每个邻居视为等同的。直观上，离探测者 ppp 更近的邻居应该与探测者 ppp 更相似。因此，我们根据探测者与其邻居之间的原始距离重新分配权重，我们通过成对距离的高斯核重新定义公式如下： Vp,gi={e−d(p,gi)如果 gi∈R∗(p,k)0否则\\mathcal{V}_{p, g_i} = \\begin{cases} e^{-d(p,g_i)} &amp; \\text{如果 } g_i \\in \\mathcal{R}^*(p, k) \\\\ 0 &amp; \\text{否则} \\end{cases} Vp,gi​​={e−d(p,gi​)0​如果 gi​∈R∗(p,k)否则​ 通过这种方式，硬权重（0或1）被转换为软权重，离探测者更近的邻居被分配更大的权重，而更远的邻居则分配较小的权重。基于上述定义，交集和并集中候选项的数量可以计算为： ∣R∗(p,k)∩R∗(gi,k)∣=∥min⁡(Vp,Vgi)∥1 |\\mathcal{R}^*(p, k) \\cap \\mathcal{R}^*(g_i, k)| = \\|\\min(\\mathbf{V}_p, \\mathbf{V}_{g_i})\\|_1 ∣R∗(p,k)∩R∗(gi​,k)∣=∥min(Vp​,Vgi​​)∥1​ ∣R∗(p,k)∪R∗(gi,k)∣=∥max⁡(Vp,Vgi)∥1 |\\mathcal{R}^*(p, k) \\cup \\mathcal{R}^*(g_i, k)| = \\|\\max(\\mathbf{V}_p, \\mathbf{V}_{g_i})\\|_1 ∣R∗(p,k)∪R∗(gi​,k)∣=∥max(Vp​,Vgi​​)∥1​ 其中，min⁡\\minmin 和 max⁡\\maxmax 操作基于元素的最小化和最大化，∥⋅∥1\\|\\cdot\\|_1∥⋅∥1​ 是 L1L_1L1​ 范数。因此，我们可以将公式5中的Jaccard距离重新写为： dJ(p,gi)=1−∑j=1Nmin⁡(Vp,gj,Vgi,gj)∑j=1Nmax⁡(Vp,gj,Vgi,gj)d_J(p, g_i) = 1 - \\frac{\\sum_{j=1}^{N} \\min(\\mathcal{V}_{p, g_j}, \\mathcal{V}_{g_i, g_j})}{\\sum_{j=1}^{N} \\max(\\mathcal{V}_{p, g_j}, \\mathcal{V}_{g_i, g_j})} dJ​(p,gi​)=1−∑j=1N​max(Vp,gj​​,Vgi​,gj​​)∑j=1N​min(Vp,gj​​,Vgi​,gj​​)​ 通过将公式5转换为公式10，我们成功地将集合比较问题转换为纯向量计算，这在实际操作中要容易得多。 Local Query Expansion 模仿同一类图像可能共享相似特征的想法，我们使用探测者 (p) 的k-近邻来实现局部查询扩展。局部查询扩展定义为： Vp=1∣N(p,k)∣∑gi∈N(p,k)Vgi\\mathbf{V}_p = \\frac{1}{|N(p, k)|} \\sum_{g_i \\in N(p, k)} \\mathbf{V}_{g_i} Vp​=∣N(p,k)∣1​gi​∈N(p,k)∑​Vgi​​ 因此，k-互惠特征 Vp\\mathbf{V}_pVp​ 被探测者 ppp 的k-近邻扩展。请注意，我们在探测者 ppp 和图库 gig_igi​ 上都实现了此查询扩展。由于在k-近邻中会有噪声，我们将局部查询扩展中使用的 N(p,k)N(p, k)N(p,k) 的大小限制为较小的值。为了区分公式7和公式11中使用的 R∗(gi,k)\\mathcal{R}^*(g_i, k)R∗(gi​,k) 和 N(p,k)N(p, k)N(p,k) 的大小，我们将前者记为 k1k_1k1​，后者记为 k2k_2k2​，其中 k1&gt;k2k_1 &gt; k_2k1​&gt;k2​。 Final Distance 在本小节中，我们关注公式5的第三个缺点。虽然大多数现有的重新排名方法在重新排名时忽略了原始距离的重要性，但我们将原始距离和Jaccard距离联合起来，修正初始排名列表。最终距离 d∗d^*d∗ 定义为： d∗(p,gi)=(1−λ)dJ(p,gi)+λd(p,gi)d^*(p, g_i) = (1 - \\lambda)d_J(p, g_i) + \\lambda d(p, g_i) d∗(p,gi​)=(1−λ)dJ​(p,gi​)+λd(p,gi​) 其中 λ∈[0,1]\\lambda \\in [0, 1]λ∈[0,1] 表示惩罚因子，它惩罚离探测者 ppp 较远的图库。当 λ=0\\lambda = 0λ=0 时，只考虑k-互惠距离。相反，当 λ=1\\lambda = 1λ=1 时，只考虑原始距离。第4节讨论了 λ\\lambdaλ 的影响。最后，通过对最终距离进行升序排序，可以得到修正后的排名列表 L∗(p,G)\\mathcal{L}^*(p, \\mathcal{G})L∗(p,G)。 Complexity Analysis 在所提出的方法中，大部分计算成本集中在所有图库对的成对距离计算上。假设图库集合的大小为 NNN，距离度量和排名过程所需的计算复杂度分别是 O(N2)O(N^2)O(N2) 和 O(N2log⁡N)O(N^2 \\log N)O(N2logN)。然而，在实际应用中，我们可以离线预先计算成对距离并获取图库的排名列表。因此，给定一个新的探测者 ppp，我们只需计算 ppp 与图库之间的成对距离，计算复杂度为 O(N)O(N)O(N)，并对所有最终距离进行排序，计算复杂度为 O(Nlog⁡N)O(N \\log N)O(NlogN)。 实验结果 主要将所提方法在两个基于图片的数据集，一个基于视频的数据集以及一个端到端的数据集上进行了实验，所提方法相比基线，效果均有明显的提升。 参数分析 k1的影响 随着k1k_1k1​的上升，rank-1准确率先上升，在k1=20k_1=20k1​=20时达到最高，然后缓慢下降。这是因为开始的时候kkk较小，随着kkk越来越大，k-互惠特征中错误的邻居数量逐渐增多，导致性能下降。 k2的影响 k2k_2k2​的影响如上图所示。当k2k_2k2​等于1时，不考虑本地查询扩展。显然，随着k2k_2k2​在合理范围内增加，性能也会提高。请注意，为 k2k_2k2​ 分配太大的值会降低性能。因为它可能会导致本地查询扩展中以指数方式包含错误匹配，这无疑会损害该功能，从而损害性能。事实上，当给k2k_2k2​设置合适的值时，本地查询扩展对于进一步提升性能是非常有好处的。 λ的影响 参数 λ\\lambdaλ 的影响如上图所示。注意，当 λ\\lambdaλ 设置为 0 时，我们只考虑 Jaccard 距离作为最终距离；相反，当 λ\\lambdaλ 等于1时，忽略了杰卡德距离，其结果正是使用纯原始距离得到的基线结果。可以看出，当仅考虑杰卡德距离时，我们的方法始终优于基线。这表明所提出的杰卡德距离对于重新排序是有效的。此外，当同时考虑原始距离和Jaccard距离时，当 λ\\lambdaλ 值在0.3左右时，性能得到进一步提高，这表明原始距离对于重新排序也很重要。 总结 在这篇论文中，我们解决了行人再识别（re-ID）中的重排序问题。我们提出了一种k-互惠特征，通过将k-互惠最近邻居编码成一个单一向量，使得重排序过程可以通过向量比较轻松地完成。为了从相似样本中捕捉相似性关系，提出了局部扩展查询以获得更稳健的k-互惠特征。基于原始距离和Jaccard距离的组合计算的最终距离在多个大规模数据集上有效地提升了re-ID的性能。值得一提的是，我们的方法是完全自动化且无监督的，并且可以轻松地应用于任何排名结果。","tags":["ReID","VI-ReID","CVPR","rerank","2017"],"categories":["ReID"]},{"title":"【论文笔记】IDKL","path":"/2024/09/26/MudSynth/ReID/【论文笔记】IDKL/","content":"文章基本信息 文章名称：Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification 发表会议/年份：CVPR 2024 作者：Kaijie Ren, Lei Zhang 单位：School of Microelectronics and Communication Engineering, Chongqing University, China 摘要 由于不同相机之间的类内变化和跨模态差异，可见红外人员重新识别 (VI-ReID) 是一项具有挑战性的任务跨模态行人检索任务。现有的工作主要集中在将不同模态的图像嵌入到一个统一的空间中，挖掘模态共享特征。它们只在这些共享特征中寻找独特的信息，而忽略了隐藏在模态特定特征中的身份感知有用信息。为了解决这个问题，我们提出了一种新颖的隐式判别知识学习 (IDKL) 网络来揭示和利用模态特定中包含的隐式判别信息。首先，我们使用一种新颖的双流网络提取特定于模态和模态共享特征。然后，经过净化，在保持身份感知判别知识的同时，减少其模态风格差异。随后，这种隐性知识被提炼成模态共享特征以增强其独特性。最后，提出了一种对齐损失来最小化增强模态共享特征的模态差异。在多个公共数据集上的广泛实验证明了 IDKL 网络优于最先进的方法。代码可在 https://github.com/1KK077/IDKL 获得。 之前工作存在的问题 VI-ReID中的方法可以被分为两类： 学习模态共享特征（尽管效果较好，但是无法很好的弥合两个模态之间的模态差距） 引入额外的模态信息来弥合模态差距 尽管以上这些模型取得了很好的效果，但是这些模型不可避免地丢弃了一些依赖于特定模态特征的判别信息，这些信息以前没有被充分利用和利用。同时传统的VI-ReID方法涉及蒸馏、对齐和相互学习通常依赖于logits。但是，在测试阶段没有分类器参与，匹配仅在特征级别执行。因此，在特征级别进行判别信息蒸馏也是必不可少的。 主要贡献/创新 为了解决上述限制，在本文中，我们提出了一个隐式判别知识学习 (IDKL) 框架，该框架从特定于模态的特征中捕获隐式不变信息，并将其提炼成模态共享特征以增强其判别能力。我们首先分别使用模态鉴别器和模态混淆器提取模态特定和模态共享特征。模态鉴别器有效地区分了不同的模态特征，赋予它们特定的特征；而模态混淆器无法区分模态特征，从而为它们赋予共享特征。由于前一阶段的模态特定特征包含大量模态差异，不适合直接蒸馏到共享特征中。我们最初使用实例归一化来减少域差异。然而，重要的是要承认 IN 不可避免地会导致某些判别特征的损失。因此，我们的目标是在保留身份感知判别知识的同时减少其模态风格差异。随后，我们通过特征图结构将这种隐式知识提取到特征级别的通道共享特征中，并通过 logit 向量的语义级别以增强其独特性。最后，提出了一种对齐损失来最小化增强模态共享特征的模态差异。 主要贡献可以概括为： 我们提出了隐式判别知识学习 (IDKL) 网络来利用模态特定特征中隐含的判别知识来增强模态共享特征的判别能力的上限。 为了减少模态风格差异而不丢失模态特定信息的判别信息，我们提出了一种 IN 引导的信息净化器 (IP)，它由判别增强损失和差异减少损失监督。 提出了一种新的TGSA损失，将判别模态特定信息提炼为模态共享特征，充分缓解模态共享特征的模态间差异。大量的实验结果证明了我们方法的优越性。 方法 整体流程图 上图展示的IDKL整体流程框架，由ResNet模块构建的双一流网络首先在模态判别器和模态混淆器的约束下提取模态特定特征 (FspF_{sp}Fsp​) 和模态共享特征 (FshF_{sh}Fsh​)，同时使用常规的ReID损失来基本优化网络。然后，将模态特定特征输入信息净化器以调节模态样式差异，同时保留隐式判别信息并获得净化的模态特定特征 (F~sp\\tilde{F}_{sp}F~sp​)。随后，这些隐式知识通过Triplet Graph Structure Alignment(TGSA，在feature-level)和Class Semantic Alignment(CSA，在logit-level)蒸馏到模态共享特征中。最后，提出了 Lmdr\\mathcal{L}_{mdr}Lmdr​​ 以最小化增强模态共享特征内的模态差异。 特征提取 首先将可见光和红外图片分别表示为： V={xiV}i=1NVI={xiI}i=1NIV=\\{x_i^V\\}_{i=1}^{N_V}\\qquad I=\\{x_i^I\\}_{i=1}^{N_I} V={xiV​}i=1NV​​I={xiI​}i=1NI​​ 其中NV=NI=NN_V=N_I=NNV​=NI​=N,表示从一个mini_batch中采样的红外和可见光照片数量N=P×KN=P\\times KN=P×K（PPP代表的是人物ID数量，KKK代表的是每个ID所选择的图片数量）,因此一个mini-batch中一共包含2N2N2N张照片。 然后所有的照片X首先会送入双流network中来提取模态独特特征FspF_{sp}Fsp​（Fsp,VF_{sp,V}Fsp,V​和Fsp,IF_{sp,I}Fsp,I​）和FshF_{sh}Fsh​(Fsh,VF_{sh,V}Fsh,V​和Fsh,IF_{sh,I}Fsh,I​),使用下式表示： Fsp=Esp(x∣Θ,Ψ)Fsh=Esh(x∣Θ,Φ)F_{sp} = E_{sp}(x | \\Theta, \\Psi)\\qquad F_{sh} = E_{sh}(x | \\Theta, \\Phi)\\qquad Fsp​=Esp​(x∣Θ,Ψ)Fsh​=Esh​(x∣Θ,Φ) 提取器EEE是ResNet50，average pooling被替换为Gem pooling，Θ\\ThetaΘ代表的是ResNet50前三个阶段的网络参数，Ψ\\PsiΨ,Φ\\PhiΦ代表的是ResNet50最后两个个阶段的网络参数。 模态混淆器和鉴别器 模态混淆器 使用了一种基于梯度反转层（GRL）的对抗性模态分类器作为“模态混淆器”。 LCj=−12N∑i=12Nti⋅log⁡p(Cj(GRL(Fshi))),\\mathcal{L}_{C_j} = -\\frac{1}{2N} \\sum_{i=1}^{2N} t_i \\cdot \\log p \\left( C_j \\left( \\text{GRL} \\left( F_{sh}^i \\right) \\right) \\right), LCj​​=−2N1​i=1∑2N​ti​⋅logp(Cj​(GRL(Fshi​))), 模态鉴别器 为了充分学习与模态相关的信息，我们使用模态分类器作为模态鉴别器。此不使用 GRL 的分类器应用于特定分支以提取特定于模态的特征。分类损失公式如下： LDj=−12N∑i=12Nti⋅log⁡p(Dj(Fspi)),\\mathcal{L}_{D_j} = -\\frac{1}{2N} \\sum_{i=1}^{2N} t_i \\cdot \\log p \\left( D_j \\left( F_{sp}^i \\right) \\right), LDj​​=−2N1​i=1∑2N​ti​⋅logp(Dj​(Fspi​)), 两个模块的总损失可以表示如下： LC=∑j=1KLCj,LD=∑j=1KLDjL_C = \\sum_{j=1}^K L_{C_j},\\qquad L_D = \\sum_{j=1}^K L_{D_j} LC​=j=1∑K​LCj​​,LD​=j=1∑K​LDj​​ 为了有效地提取特定于模态和模态共享特征，我们将这些模态分类器损失与标准 ReID 损失 LreidL_{reid}Lreid​ 相结合，其中包括交叉熵和硬三元组损失。这些应用于特定于模态和模态共享分支，如下所示： Lsp=Lreid(fsp)+LD,Lsh=Lreid(fsh)+LCL_{sp} = L_{reid}(f_{sp}) + L_D,\\qquad L_{sh} = L_{reid}(f_{sh}) + L_C Lsp​=Lreid​(fsp​)+LD​,Lsh​=Lreid​(fsh​)+LC​ 其中f∈RB×Cf\\in\\mathbb{R}^{B\\times C}f∈RB×C代表的是F∈RB×C×H×WF\\in\\mathbb{R}^{B\\times C\\times H\\times W}F∈RB×C×H×W经过pooling后的结果。 最终我们模型的基本损失可以表示为： Lb=Lsh+LspL_b=L_{sh}+L_{sp} Lb​=Lsh​+Lsp​ 信息净化器（Information Purifier，IP） 信息净化器 (IP) 旨在最小化风格方差的影响，同时保留特定模态特征中的身份感知和判别知识。IP集成了实例归一化(IN)，众所周知，它可以减少域差异。然而，重要的是要认识到 IN 不可避免地导致一些判别特征的损失，这可能会阻碍 ReID 的高性能。 首先对FspF_{sp}Fsp​使用IN得到F^sp\\hat{F}_{sp}F^sp​: F^sp=IN(Fsp)=Fsp−E[Fsp]Var[Fsp]+ϵ\\hat{F}_{sp} = IN(F_{sp}) = \\frac{F_{sp} - E[F_{sp}]}{\\sqrt{Var[F_{sp}]} + \\epsilon} F^sp​=IN(Fsp​)=Var[Fsp​]​+ϵFsp​−E[Fsp​]​ 然后同SENet的方法一样，我们生成了两个channel-wise的masks mem_eme​和mrm_rmr​: me=σ(W2δ(W1g(Fsp))),mr=σ(W4δ(W3g(F^sp))),\\mathbf{m}_e = \\sigma \\left( \\mathbf{W}_2 \\delta \\left( \\mathbf{W}_1 g \\left( \\mathbf{F}_{sp} \\right) \\right) \\right), \\quad \\mathbf{m}_r = \\sigma \\left( \\mathbf{W}_4 \\delta \\left( \\mathbf{W}_3 g \\left( \\hat{\\mathbf{F}}_{sp} \\right) \\right) \\right), me​=σ(W2​δ(W1​g(Fsp​))),mr​=σ(W4​δ(W3​g(F^sp​))), 他们的作用分别是，mem_eme​代表的是增强判别特征，mrm_rmr​代表的是减少attention mask的差异（reduction of discrepancies attention mask）。所以我们可以得到更强的特殊特征Fspd+F_{sp}^{d+}Fspd+​,以及更小的模态差异特征F^spm−\\hat{F}_{sp}^{m-}F^spm−​(计算如上图所示) 随后，我们计算判别增强损失（discrimination enhancing loss）LeL_eLe​和差异减少损失（discrimination enhancing loss）LrL_rLr​分别用于监督mem_eme​与mrm_rmr​的生成： Le=Softplus(h(Csp(fspd+))−h(Csp(fsp)))\\mathcal{L}_e = \\text{Softplus} \\left( h \\left( C_{sp} \\left( f_{sp}^{d+} \\right) \\right) - h \\left( C_{sp} \\left( f_{sp} \\right) \\right) \\right) Le​=Softplus(h(Csp​(fspd+​))−h(Csp​(fsp​))) Lr=Softplus(d(f^sp,Vm−,f^sp,Im−)−d(f^sp,V,f^sp,I))\\mathcal{L}_r = \\text{Softplus} \\left( d \\left( \\hat{f}_{sp,V}^{m^-}, \\hat{f}_{sp,I}^{m^-} \\right) - d \\left( \\hat{f}_{sp,V}, \\hat{f}_{sp,I} \\right) \\right) Lr​=Softplus(d(f^​sp,Vm−​,f^​sp,Im−​)−d(f^​sp,V​,f^​sp,I​)) 这里LeL_eLe​希望能够生成比FspF_{sp}Fsp​更具有区别性特征的Fspd+F_{sp}^{d+}Fspd+​,LrL_rLr​则希望生成的F^spm−\\hat{F}_{sp}^{m-}F^spm−​相比F^sp\\hat{F}_{sp}F^sp​有更小的模态差异。 Softplus(⋅)=ln(1+exp⁡(⋅))\\text{Softplus}(\\cdot) = \\text{ln}(1+\\exp(\\cdot))Softplus(⋅)=ln(1+exp(⋅))，旨在是损失函数都限制为正数 最后将他们结合起来就可以得到purified modality-specific feature: F~sp=me⊙F^spm−+mr⊙Fspd+.\\tilde{F}_{sp} = \\mathbf{m}_e \\odot \\hat{F}_{sp}^{m^-} + \\mathbf{m}_r \\odot F_{sp}^{d^+}. F~sp​=me​⊙F^spm−​+mr​⊙Fspd+​. 最后这一模块的损失函数可以表示为如下形式： Lip=Le+Lr+Lreid(f~sp)L_{ip} = L_e + L_r + L_{reid}(\\tilde{f}_{sp}) Lip​=Le​+Lr​+Lreid​(f~​sp​) 隐式知识蒸馏（IKD） 为了确保模态共享特征全面学习和集成隐式信息，我们通过 TGSA 从 feature 级别以及CSA 从 logit 级别执行蒸馏。 Triplet Graph Structure Alignment (TGSA) 为了赋予共享特征以区分性信息并在特征层面减少模态差异，我们开发了一种三元特征图结构对齐损失。这种方法的动机源于特征图结构包含丰富的关于特征之间关系和分布的信息，例如类间区分性和类内多样性。这些特性被用来挖掘潜在的特征关系并在[17, 36]中增强特征表示。表示特征之间关系的图结构亲和矩阵计算如下： αij=exp⁡(L([l(fi)∥l(fj)]⋅w))∑k∈Niexp⁡(L([l(fi)∥l(fk)]⋅w)),\\alpha_{ij} = \\frac{\\exp \\left( L \\left( \\left[ l \\left( f_i \\right) \\parallel l \\left( f_j \\right) \\right] \\cdot w \\right) \\right)}{\\sum_{k \\in \\mathcal{N}_i} \\exp \\left( L \\left( \\left[ l \\left( f_i \\right) \\parallel l \\left( f_k \\right) \\right] \\cdot w \\right) \\right)}, αij​=∑k∈Ni​​exp(L([l(fi​)∥l(fk​)]⋅w))exp(L([l(fi​)∥l(fj​)]⋅w))​, 其中L代表的是LeakyReLU，[⋅∣∣⋅][\\cdot||\\cdot][⋅∣∣⋅]代表的是拼接操作，Ni\\mathcal{N}_iNi​表示用于为第 i 个样本归一化的邻居样本。l(⋅)l(\\cdot)l(⋅)是特征维度转化层，www是全连接层。 由于我们利用图结构来对齐和提取知识，而不是增强特征，并且欧氏空间分布对特征更具意义，因此我们用欧氏距离代替线性变换来计算注意力分数，并重新定义两组特征的图结构表达如下： A(a;b)={αij}i,j∈N=exp⁡(D(fai,fbj))∑k∈Nexp⁡(D(fai,fbk)),\\mathbf{A}_{(a;b)} = \\left\\{ \\alpha_{ij} \\right\\}_{i,j \\in \\mathcal{N}} = \\frac{\\exp \\left( D \\left( f_a^i, f_b^j \\right) \\right)}{\\sum_{k \\in \\mathcal{N}} \\exp \\left( D \\left( f_a^i, f_b^k \\right) \\right)}, A(a;b)​={αij​}i,j∈N​=∑k∈N​exp(D(fai​,fbk​))exp(D(fai​,fbj​))​, 具体而言，三元组图结构对齐损失Ltgsa\\mathcal{L}_{\\text{tgsa}}Ltgsa​ 是专为跨模态重识别（ReID）开发的，用于对齐两种不同的模态类型，使它们能够符合相同的图结构分布并减少模态差异。该损失包含两个自模态亲和矩阵和一个跨模态亲和矩阵，后者确保了图结构分布的整体一致性，如图3所示。通过使用Kullback-Leibler（KL）散度，这三个矩阵成对对齐。因此，两种不同模态类型的对齐损失 ( Ltgsa\\mathcal{L}_{\\text{tgsa}}Ltgsa​ ) 定义为： Ltgsa(a;b)=∑p=1P∑k=1K(KL(A(a;a)pk,A(b;b)pk)+KL(A(a;a)pk,A(a;b)pk)+KL(A(a;b)pk,A(b;b)pk)).\\mathcal{L}_{\\text{tgsa}}^{(a;b)} = \\sum_{p=1}^{P} \\sum_{k=1}^{K} \\left( \\text{KL} \\left( A_{(a;a)}^{pk}, A_{(b;b)}^{pk} \\right) + \\text{KL} \\left( A_{(a;a)}^{pk}, A_{(a;b)}^{pk} \\right) + \\text{KL} \\left( A_{(a;b)}^{pk}, A_{(b;b)}^{pk} \\right) \\right). Ltgsa(a;b)​=p=1∑P​k=1∑K​(KL(A(a;a)pk​,A(b;b)pk​)+KL(A(a;a)pk​,A(a;b)pk​)+KL(A(a;b)pk​,A(b;b)pk​)). 其中ApkA^{p_k}Apk​代表的含义是第p个人的第k个样本的图分布。 为了将判别隐式模式特定知识传达为特征级共享特征，通过TGSA在同质特征上的两个分支上的蒸馏损失可以表示为： Ltgsa(a;b)=∑p=1P∑k=1K(KL(A(a;a)pk,A(b;b)pk)+KL(A(a;a)pk,A(a;b)pk)+KL(A(a;b)pk,A(b;b)pk)).\\mathcal{L}_{\\text{tgsa}}^{(a;b)} = \\sum_{p=1}^{P} \\sum_{k=1}^{K} \\left( \\text{KL} \\left( A_{(a;a)}^{pk}, A_{(b;b)}^{pk} \\right) + \\text{KL} \\left( A_{(a;a)}^{pk}, A_{(a;b)}^{pk} \\right) + \\text{KL} \\left( A_{(a;b)}^{pk}, A_{(b;b)}^{pk} \\right) \\right). Ltgsa(a;b)​=p=1∑P​k=1∑K​(KL(A(a;a)pk​,A(b;b)pk​)+KL(A(a;a)pk​,A(a;b)pk​)+KL(A(a;b)pk​,A(b;b)pk​)). Class Semantic Alignment(CSA) CSA用于将隐式模态特定知识的语义信息提取到模态共享分支中，以增强共享特征的特征表示。CSA 在 logit 级别对两个分支之间的同质特征进行操作。分类器背后的 logit 矩阵可以表述为： Zsp=Csp(fsp),Zsh=Csh(fsh),\\mathbf{Z}_{sp} = C_{sp} \\left( \\mathbf{f}_{sp} \\right), \\quad \\mathbf{Z}_{sh} = C_{sh} \\left( \\mathbf{f}_{sh} \\right), Zsp​=Csp​(fsp​),Zsh​=Csh​(fsh​), CSA损失和之前的TGSA损失非常的相似，可以表示为下式： Lcsa=∑i=1N(KL(Zsh,Vi,Zsp,Vi)+KL(Zsh,Ii,Zsp,Ii)).\\mathcal{L}_{\\text{csa}} = \\sum_{i=1}^{N} \\left( \\text{KL} \\left( \\mathbf{Z}_{sh,V}^i, \\mathbf{Z}_{sp,V}^i \\right) + \\text{KL} \\left( \\mathbf{Z}_{sh,I}^i, \\mathbf{Z}_{sp,I}^i \\right) \\right). Lcsa​=i=1∑N​(KL(Zsh,Vi​,Zsp,Vi​)+KL(Zsh,Ii​,Zsp,Ii​)). 模态差异减少损失（MDR） Modality Discrepancy Reduction (MDR)的目标是为了保证模态共享特征的不变表示，进一步利用TGSA和CSA来减少模态共享分支内的模态差异，如下所示： Lmdr=Ltgsa(sh,V;sh,I)+∑i=1NKL(Zsh,Vi,Zsh,Ii).\\mathcal{L}_{\\text{mdr}} = \\mathcal{L}_{\\text{tgsa}}^{(sh,V;sh,I)} + \\sum_{i=1}^{N} \\text{KL} \\left( \\mathbf{Z}_{sh,V}^i, \\mathbf{Z}_{sh,I}^i \\right). Lmdr​=Ltgsa(sh,V;sh,I)​+i=1∑N​KL(Zsh,Vi​,Zsh,Ii​). 这样，模态共享分支的可见特征和红外特征可以从特征级和语义级实现相互学习。它使两种模态特征相互对齐，同时减轻模态差距并保持模态共享特征的不变性。 综上，一共分别对齐了： 红外模态sh和sp（logit+feature） 可见光模态sh和sp（logit+feature） 双模态的sh（logit+feature） 优化 最终，通过从特定于模态的特征中连续提取隐式判别知识，并始终减少模态共享特征中的模态差异，我们可以获得更具辨别力和不变的模态共享特征。 IDKL的最终优化函数可以被表示为如下所示： Ltotal=Lb+λ1Lip+λ2Ltgsa+λ3Lcsa+Lmdr,\\mathcal{L}_{\\text{total}} = \\mathcal{L}_b + \\lambda_1 \\mathcal{L}_{ip} + \\lambda_2 \\mathcal{L}_{\\text{tgsa}} + \\lambda_3 \\mathcal{L}_{\\text{csa}} + \\mathcal{L}_{\\text{mdr}}, Ltotal​=Lb​+λ1​Lip​+λ2​Ltgsa​+λ3​Lcsa​+Lmdr​, 其中,λ1,λ2,λ3\\lambda_1,\\lambda_2,\\lambda_3λ1​,λ2​,λ3​为超参数。 实验结果 展示出来结果非常不错，但是如果 总结 idea非常不错，首次使用双分支网络对共享和特有特征进行提取的思路很好，但是文章没有提自己使用了rerank策略使其效果增长了10个点的事情，以及GRL并没有真正在代码中使用过，感觉文章大小问题还是有些多。","tags":["ReID","2024","VI-ReID","CVPR","IDKL","rerank"],"categories":["ReID","VI-ReID"]},{"title":"【论文笔记】DNDM","path":"/2024/09/26/MudSynth/ReID/【论文笔记】DNDM/","content":"文章基本信息 文章名称：Day-Night Cross-domain Vehicle Re-identification 发表会议/年份：CVPR 2024 作者：Hongchao Li, Jingong Chen, Aihua Zheng, Yong Wu, Yonglong Luo 单位：Anhui Normal University, Anhui University 摘要 本文提出了一种新颖的昼夜双域调制（DNDM）车辆再识别框架，解决了跨昼夜性能问题（之前工作都是在良好光照下）。它包括一个夜间域眩光抑制模块和一个双域结构增强模块，以增强低光环境下的车辆特征。通过开发跨域类别感知模块，本文促进了两个域中外观和结构特征的互动，并提供了包含昼夜图像的新数据集DN-Wild和平衡数据集DN-348。实验结果表明，该框架在昼夜跨域车辆再识别中的鲁棒性。 之前工作存在的问题 跨昼夜性能的忽视 热（近）红外摄像机的高成本和环境光干扰（无法像行人重识别转化为VIReID） 数据集中的样本不平衡 跨域特征差异带来的挑战 类别内差异大导致的泛化能力不足 主要贡献/创新 我们提供了两个标准化基准数据集，DN-Wild和DN-348，以促进DN-ReID的研究。这些基准数据集将免费向学术研究公众开放。 我们提出了昼夜双域调制（DNDM）框架，集成了眩光抑制、结构增强和类别感知的训练，以动态调制昼夜跨域车辆特征。 在具有挑战性的基准数据集DN-348和DN-Wild上进行的详尽实验验证了我们的DNDM在昼夜跨域车辆ReID问题上的优越性能和潜力。 方法 3.1 Model Architecture 提出的DNDM框架利用ResNet-50作为骨干网络，从昼夜图像中提取特征。为了解决夜间眩光问题，框架引入了夜间域眩光抑制（Night-domain Glare Suppression, NGS）模块。双域结构增强（Dual-domain Structure Enhancement, DSE）模块聚合局部窗口的梯度，捕捉多样的结构表示。跨域类别感知（Cross-domain Class Awareness, CCA）模块促进昼夜跨域特征的相互作用，增强外观和结构表示在骨干网络各个阶段的有效利用。 3.2 Baseline 昼夜跨域车辆再识别（DN-ReID）旨在检索昼夜环境中感兴趣的车辆。给定一对车辆图像 I={(IDay,INight),y}I = \\{(I^{Day}, I^{Night}), y\\}I={(IDay,INight),y}，其中 IDayI^{Day}IDay 和 INightI^{Night}INight 分别是输入的白天和夜间车辆图像， yyy 是相关的车辆身份标签。由骨干网络编码的相应多阶段特征张量表示为 TsmT_s^mTsm​，其中 s∈{0,1,2,3,4}s \\in \\{0, 1, 2, 3, 4\\}s∈{0,1,2,3,4}， m∈{Day,Night}m \\in \\{Day, Night\\}m∈{Day,Night}。如图3(a)所示，遵循ResNet-50骨干网络，我们使用全局平均池化（GAP）层来获得相应的特征向量 fm=GAP(T4m)f^m = GAP(T_4^m)fm=GAP(T4m​)。网络随后针对交叉熵损失 LceL_{ce}Lce​ 和三元组损失 LtriL_{tri}Ltri​ 进行优化。交叉熵损失公式如下： Lce=−ylog⁡(Softmax(FCclass(fm))),L_{ce} = -y \\log(\\text{Softmax}(FC_{class}(f^m))), Lce​=−ylog(Softmax(FCclass​(fm))), 其中 FCclassFC_{class}FCclass​ 表示预测分类结果的全连接层，Softmax 是获取归一化概率的函数。值得注意的是，fDayf^{Day}fDay 和 fNightf^{Night}fNight 共享相同的 FCclassFC_{class}FCclass​ 层。三元组损失公式如下： Ltri=max⁡(0,dijp+margin−dikn),L_{tri} = \\max(0, d_{ij}^p + \\text{margin} - d_{ik}^n), Ltri​=max(0,dijp​+margin−dikn​), 其中 (i,j,k)(i, j, k)(i,j,k) 表示每个训练批次内的一个困难三元组。对于白天锚定样本 iii，jjj 来自相应的夜间正样本集，kkk 来自白天负样本集。对于夜间锚定样本 iii，jjj 来自相应的白天正样本集，kkk 来自夜间负样本集。dijp/diknd_{ij}^p / d_{ik}^ndijp​/dikn​ 表示正/负样本对的成对距离，margin = 0.3 表示三元组距离边界。 尽管上述骨干网络可以提取车辆特征，但它并不能有效解决车灯眩光、低光环境和域差异带来的挑战。为了解决DN-ReID中的挑战，我们引入了夜间域眩光抑制（NGS）模块、双域结构增强（DSE）模块和跨域类别感知（CCA）模块。 总结 昼夜跨域车辆再识别（DN-ReID）提出了利用骨干网络提取昼夜环境中的车辆特征，优化交叉熵损失和三元组损失。然而，传统方法无法有效解决车灯眩光、低光环境和域差异的问题。为此，本文引入了夜间域眩光抑制（NGS）模块、双域结构增强（DSE）模块和跨域类别感知（CCA）模块，以应对这些挑战。 3.3 NGS 借鉴研究[2, 19]中强调的视觉提示概念，视觉提示的整合在识别任务的确切细节时至关重要。我们提出了一个夜间域眩光抑制（Night-domain Glare Suppression, NGS）模块，该模块利用眩光提示引导注意力到无眩光区域，并减少车灯眩光的影响。给定一个夜间车辆图像 INightI^{Night}INight，我们首先通过卷积块提取特征张量 T0NightT_0^{Night}T0Night​: T0Night=Mxp2×2(ReLU(BN(conv7×7(INight)))),T_0^{Night} = M_{xp2\\times2}(\\text{ReLU}(\\text{BN}(\\text{conv}_{7\\times7}(I^{Night})))), T0Night​=Mxp2×2​(ReLU(BN(conv7×7​(INight)))), 其中 conv7×7\\text{conv}_{7\\times7}conv7×7​ 表示一个7×7卷积操作，BN表示批归一化操作，ReLU表示修正线性单元，Mxp2×2M_{xp2\\times2}Mxp2×2​ 表示2×2最大池化操作。 同时，我们将夜间车辆图像 INightI^{Night}INight 转换为灰度图像 IG=rgb2gray(INight)I^G = \\text{rgb2gray}(I^{Night})IG=rgb2gray(INight)。然后，我们应用亮度阈值220来识别夜间图像中的高亮区域。在识别高亮区域后，我们将相邻像素组合成区域，同时丢弃像素较少的区域。最后，我们从初始夜间图像中获得二进制掩码 MGM^GMG，其中1表示受眩光影响的像素，0表示未受眩光影响的像素。该二进制掩码作为输入包含在眩光抑制模块中，并与特征张量 T0NightT_0^{Night}T0Night​ 进行调制。具体来说，我们将特征张量 T0NightT_0^{Night}T0Night​ 与二进制掩码 MGM^GMG 连接，然后将它们输入到一个可学习的投影向量 V∈R(C0+1)×1V\\in \\mathbb{R}^{(C^0+1)\\times 1}V∈R(C0+1)×1 中： M0Night=Sigmoid(concat(T0Night,MG)V),M_0^{Night} = \\text{Sigmoid}(\\text{concat}(T_0^{Night}, M^G)V), M0Night​=Sigmoid(concat(T0Night​,MG)V), 其中 M0Night∈RH0×W0×1M_0^{Night}\\in\\mathbb{R}^{H^0\\times W^0\\times 1}M0Night​∈RH0×W0×1 表示学习到的域抑制掩码，Sigmoid指的是S形函数。 除了夜间图像外，我们的NGS模块还受到白天图像的指导。基本思想是利用一个虚拟掩码 zeros(MG)\\text{zeros}(M^G)zeros(MG) 来提示无眩光白天特征和有眩光夜间特征之间的差异： M0Day=Sigmoid(concat(T0Day,zeros(MG))V),M_0^{Day} = \\text{Sigmoid}(\\text{concat}(T_0^{Day}, \\text{zeros}(M^G))V), M0Day​=Sigmoid(concat(T0Day​,zeros(MG))V), 在此过程中，输入之一是 T0NightT_0^{Night}T0Night​ 和 MGM^GMG 的连接。另一个输入是一个无眩光的白天特征张量 T0DayT_0^{Day}T0Day​，与一个全零掩码 zeros(MG)\\text{zeros}(M^G)zeros(MG) 连接。基于域抑制掩码 M0mM_0^mM0m​，最终的抑制过程可以表示为： T0m=T0m−αM0m⊙T0m,T_0^m = T_0^m - \\alpha M_0^m \\odot T_0^m, T0m​=T0m​−αM0m​⊙T0m​, 其中 T0mT_0^mT0m​ 表示经过眩光抑制操作后的昼夜特征，α=0.5\\alpha = 0.5α=0.5 是用于平衡原始特征和弱化特征的超参数。图3(b)展示了我们NGS模块的结果，说明了眩光区域的成功分离。在执行上述操作后，我们将 T0mT_0^mT0m​ 反馈到骨干网络中以获取相应的特征张量 TsmT_s^mTsm​。 总结 本文提出的夜间域眩光抑制（NGS）模块利用视觉提示有效减少车灯眩光的影响。通过对夜间图像进行特征提取、灰度转换和亮度阈值处理，我们生成了二进制掩码来识别受眩光影响的区域。该掩码用于调制特征张量，从而增强无眩光区域的特征表示。NGS模块还结合了白天图像信息，以提示昼夜特征之间的差异。最终，通过这些操作，成功实现了昼夜特征的分离和增强，显著提高了车辆再识别的准确性。 代码实现 1 3.4 DSE 常见的车辆再识别网络主要关注提取车辆外观特征。然而，外观特征容易受到低光环境的影响。为了提高昼夜车辆图像对的特征一致性，我们引入了双域结构增强（Dual-domain Structure Enhancement, DSE）模块。DSE模块的主要思想是通过逐像素梯度从外观特征中提取结构信息。具体来说，DSE模块处理中间特征图 TsmT_s^mTsm​，并计算每个像素位置 xxx 及其周围区域大小为 N×NN \\times NN×N 的逐像素非负局部梯度 GGG: G(x,c,d)=max⁡(0,Tsm(x+d,c)−Tsm(x,c))G(x, c, d) = \\max(0, T_s^m(x + d, c) - T_s^m(x, c)) G(x,c,d)=max(0,Tsm​(x+d,c)−Tsm​(x,c)) 其中 c∈[1,Cs]c \\in [1, C^s]c∈[1,Cs] 表示通道维度的索引，d∈[−dn,dn]×[−dn,dn]d \\in [-d_n, d_n]\\times[-d_n, d_n]d∈[−dn​,dn​]×[−dn​,dn​] 表示每个像素 xxx 在其周围区域的邻居位置。区域大小为 N×NN \\times NN×N，dn=(N−1)/2d_n = (N-1)/2dn​=(N−1)/2。此外，我们提出了一种特征加权操作，将详细的局部梯度整合成一个简明的结构描述符，使得能够同时从两个域中学习几何结构： S(x,c,d)=G(x,c,d)1+∑dG(x,c,d)Tsm(x+d,c),S(x, c, d) = \\frac{G(x, c, d)}{1 + \\sum_d G(x, c, d)} T_s^m(x + d, c), S(x,c,d)=1+∑d​G(x,c,d)G(x,c,d)​Tsm​(x+d,c), S(x,c)=∑dS(x,c,d),S(x, c) = \\sum_d S(x, c, d), S(x,c)=d∑​S(x,c,d), 其中 S∈RHs×Ws×CsS \\in \\mathbb{R}^{H^s \\times W^s \\times C^s}S∈RHs×Ws×Cs 具有与原始特征张量 TsmT_s^mTsm​ 相同的空间和通道尺寸。梯度引导的特征加权操作将邻居特征聚合成结构特征，从而将其空间维度从 N×NN \\times NN×N 降至 1×11 \\times 11×1。这种转换将原始局部梯度 GGG 转换为结构描述符 SSS。简而言之，结构描述符由加权的外观描述符导出。然后，我们利用结构描述符作为外观描述符的附加输入： Ts‾m=Tsm+βS,\\overline{T_s}^m = T_s^m + \\beta S, Ts​​m=Tsm​+βS, 其中 Ts‾m\\overline{T_s}^mTs​​m 表示结构增强操作后的昼夜特征，超参数 β=0.25\\beta = 0.25β=0.25 用于平衡原始特征和增强特征。 代码实现 123456789101112131415161718192021222324252627282930class GradientComputation4(nn.Module): def __init__(self): super(GradientComputation4, self).__init__() self.conv_layers = &#123;&#125; self.relu = nn.ReLU() def get_masked_conv(self, channels): if channels not in self.conv_layers: mask_conv = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=5, padding=2, groups=channels, bias=False) mask = 1 * torch.ones(1, 1, 5, 5) mask[0, 0, 2, 2] = -24 # 要将所有求和后-最中间24次，所以是1-25次 mask_conv.weight.data = mask.repeat(channels, 1, 1, 1) for param in mask_conv.parameters(): param.requires_grad = False self.conv_layers[channels] = mask_conv return self.conv_layers[channels] def forward(self, img): b, c, h, w = img.shape mask_conv = self.get_masked_conv(c) mask_conv.cuda() masked_output = mask_conv(img) masked_output = self.relu(masked_output) sum_output = masked_output / 24.0 # 做平均 x = 0.9 * img + 0.25 * sum_output # 然后按照该式子进行加权就可以了 return x 总结 双域结构增强（DSE）模块通过逐像素梯度从车辆图像的外观特征中提取结构信息，以提高昼夜图像对的特征一致性。DSE模块计算每个像素及其周围区域的局部梯度，将局部梯度整合成结构描述符，并将其作为附加输入，增强外观特征表示。通过这种方式，DSE模块在低光环境下改善了车辆再识别的鲁棒性和准确性。 3.5 CCA 在我们的网络中，我们首先使用ResNet-50从昼夜跨域车辆图像中提取外观特征。然后，我们应用眩光抑制模块来减少夜间图像中车灯眩光的影响。此外，我们引入结构增强模块来改进外观特征。然而，这些模块未能考虑昼夜域之间的差异。 为了应对昼夜域间差异，我们为DN-ReID问题引入了跨域类别感知（Cross-domain Class Awareness, CCA）模块。给定昼夜跨域车辆特征 Tsm∈RHs×Ws×CsT_s^m \\in \\mathbb{R}^{H^s \\times W^s \\times C^s}Tsm​∈RHs×Ws×Cs，其中 s∈{1,2,3,4}s \\in \\{1,2,3,4\\}s∈{1,2,3,4}，我们使用卷积层将其转换为投影 PsmP_s^mPsm​。这个调整旨在匹配全连接层 FCclassFC_{class}FCclass​ 的输入大小，如公式(1)所述。数学表达如下： Psm=BN(conv1×1(Tsm)),P_s^m = \\text{BN}(\\text{conv}_{1 \\times 1}(T_s^m)), Psm​=BN(conv1×1​(Tsm​)), 其中 conv1×1\\text{conv}_{1 \\times 1}conv1×1​ 表示1×1卷积操作，BN表示批归一化操作。受类激活映射（CAM）操作的启发，CAM可以突出类特定的判别区域。我们引入投影 PsmP_s^mPsm​ 来计算昼夜图像的类激活映射： Aim=Sigmoid(FCclass(Psm)),A_i^{m} = \\text{Sigmoid}(FC_{class}(P_s^m)), Aim​=Sigmoid(FCclass​(Psm​)), 其中 C′C&#x27;C′ 代表训练集中类的总数。对于第 yyy 类的类激活映射分别表示为 AyDayA_y^{Day}AyDay​ 和 AyNightA_y^{Night}AyNight​，其中 yyy 表示车辆身份标签。Sigmoid函数用于归一化CAM。 为了促进昼夜跨域特征的相互作用，我们建议在训练阶段交换昼夜样本的类感知信息： PsDay=PsDay⊙AyNight,P_s^{Day} = P_s^{Day} \\odot A_y^{Night}, PsDay​=PsDay​⊙AyNight​, PsNight=PsNight⊙AyDay,P_s^{Night} = P_s^{Night} \\odot A_y^{Day}, PsNight​=PsNight​⊙AyDay​, 其中 ⊙\\odot⊙ 表示元素级别的乘法。重要的是，这种交换过程仅在训练阶段发生，测试阶段不交换类激活映射。为了确保 PsDayP_s^{Day}PsDay​ 和 PsNightP_s^{Night}PsNight​ 具有相同的通道数，我们将 conv1×1+BN+ReLU\\text{conv}_{1 \\times 1} + \\text{BN} + \\text{ReLU}conv1×1​+BN+ReLU 操作纳入CCA模块。结果特征表示为： Tsm=Tsm+ReLU(BN(conv1×1(Psm))).T_s^m = T_s^m + \\text{ReLU}(\\text{BN}(\\text{conv}_{1 \\times 1}(P_s^m))). Tsm​=Tsm​+ReLU(BN(conv1×1​(Psm​))). 总体损失 我们采用广泛使用的ResNet-50作为骨干网络。在第一个块之前集成了提出的夜间域眩光抑制（NGS）模块。此外，我们在每个卷积块之后集成了双域结构增强（DSE）模块和跨域类别感知（CCA）模块。整个网络以端到端方式进行训练。总体损失函数如下： L=Lce+Ltri.\\mathcal{L} = \\mathcal{L}_{ce} + \\mathcal{L}_{tri}. L=Lce​+Ltri​. 总结 跨域类别感知（CCA）模块旨在解决昼夜域之间的差异。通过将车辆特征转换为投影并计算类激活映射，CCA模块促进了昼夜样本之间的特征交互。结合夜间域眩光抑制（NGS）模块和双域结构增强（DSE）模块，CCA模块增强了车辆再识别系统在昼夜跨域场景中的鲁棒性。通过端到端训练，网络在特征提取和域适应方面表现出色，显著提升了再识别的准确性。 实验结果 DN-348上的测试结果 比较了DNDM和最先进的VI-ReID方法在DN-348数据集上的性能。DNDM在处理夜间车辆图像时表现更佳，尤其是在昼夜转换和夜昼转换设置中显著优于VI-ReID方法。通过夜间领域眩光抑制和双领域结构增强，DNDM有效提升了夜间车辆图像的特征学习能力。研究表明，DN-ReID在夜间匹配车辆图像方面具有挑战性，但潜力巨大。 DN-Wild上的测试结果 本文比较了DNDM和最先进的方法在DN-Wild数据集上的性能。DNDM在昼夜转换和夜昼转换设置中均表现出色，显著优于ReID强基线BOT。通过眩光抑制、结构增强和类别感知的训练，DNDM有效地学习了昼夜跨域特征。然而，PMT在DN-Wild数据集上的表现不如在DN-348数据集上，这表明仅考虑域间差异是不够的，需解决样本不平衡问题。总体上，DNDM在大规模数据集上显示了强泛化能力。 消融实验 每个组件的有效性 通过在DN-348数据集上的消融研究，本文验证了NGS、DSE和CCA模块对模型性能的显著贡献。单独启用NGS模块时，Rank-1性能达到69.0%；启用NGS和DSE模块时，性能提升至69.9%；三者结合时，性能达到70.7%。这些结果证明了各模块单独及联合使用时的一致性性能提升。 ResNet-50的哪个阶段插入DSE模块和CCA模块的影响 本文分析了在ResNet-50的不同阶段插入DSE和CCA模块对性能的影响。实验表明，随着模块在ResNet-50各阶段的逐步集成，DN-348数据集的Rank-1分数从69.6%提升至70.7%，mAP从47.0%提升至47.5%。这些结果验证了昼夜双域调制框架在学习昼夜跨域信息以提升DN-ReID性能方面的有效性。 Other Analysis 超参数分析：为了评估两个超参数的影响，我们进行了定量比较并在图4中报告了结果。α和β的不同值显着影响NGS和DSE模块的性能。据观察，当 α 和 β 值分别设置为 0.5 和 0.25 时，可实现最佳性能。 可视化研究：为了进一步分析我们DNDM的有效性，我们在DN-348数据集上进行实验，以计算跨身份和同身份距离的频率。图5（a，b）分别显示了基线方法和提出的DNDM获取的距离分布。将图5（b）与图5（a）进行比较，我们可以观察到δ1 &lt; δ2。这表明使用所提出的方法，跨身份和同身份距离显著分离。 此外，我们使用T-SNE方法在二维特征空间中可视化了20辆车的特征分布。在图5（c，d）中，可以明显看出，所提出的DNDM显著减少了同一身份白天和夜间图像之间的距离，并成功地最小化了域间差异。 总结 据我们所知，这是首个解决昼夜跨域车辆重识别（DN-ReID）问题的研究。我们贡献了两个新的DN-ReID数据集，并提出了一种创新的DN-ReID方法。与白天到白天的车辆重识别相比，DN-ReID面临车灯眩光、低光环境和域间差异带来的挑战。因此，我们提出了昼夜双域调制（DNDM）网络，该网络结合了眩光抑制、结构增强和类别感知的学习，以动态调制昼夜跨域车辆特征。广泛的实验表明，所提出方法具有良好的性能。 此外，基于我们的研究，我们强调了DN-ReID的几个重要发现。首先，为夜间的车辆图像注释是一项挑战。其次，增强夜间车辆图像中的特征被证明是有效的。最后，考虑昼夜跨域数据识别相同ID的能力是值得的。未来，我们将增强上述组件，以推进DN-ReID的最新技术，并探索无标签的DN-ReID。","tags":["ReID","2024","CVPR","Vehicle-ReID"],"categories":["ReID","Others"]},{"title":"【论文笔记】DEEN","path":"/2024/09/26/MudSynth/ReID/【论文笔记】DEEN/","content":"文章基本信息 文章名称：Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-identification 发表会议/年份：CVPR 2023 作者：Yukang Zhang, Hanzi Wang 单位：Fujian Key Laboratory of Sensing and Computing for Smart City,School of Informatics, Xiamen University, 361005, P.R. China.2Key Laboratory of Multimedia Trusted Perception and Efficient Computing,Ministry of Education of China, Xiamen University, 361005, P.R. China.3Shanghai Artificial Intelligence Laboratory, Shanghai, 200232, China. 摘要 对于可见光-红外行人重识别（VI-ReID）任务，主要挑战之一是可见光（VIS）和红外（IR）图像之间的模态差距。然而，训练样本通常有限，而模态差距过大，导致现有方法无法有效挖掘多样化的跨模态线索。为了解决这个限制，我们在嵌入空间中提出了一种新颖的增强网络，称为多样化嵌入扩展网络（DEEN）。所提出的 DEEN 可以有效地生成多样化的嵌入来学习信息丰富的特征表示并减少可见光和红外图像之间的模态差异。此外，VIReID模型可能会受到剧烈光照变化的严重影响，而所有现有的VIReID数据集都是在足够的光照下捕获的，没有明显的光照变化。因此，我们提供了一个低光跨模态（LLCM）数据集，其中包含由 9 个 RGB/IR 相机捕获的 1,064 个身份的 46,767 个边界框。在 SYSU-MM01、RegDB 和 LLCM 数据集上进行的大量实验表明，所提出的 DEEN 相对于其他几种最先进的方法具有优越性。代码和数据集发布于： https://github.com/ZYK100/LLCM 之前工作存在的问题 之前方法主要通过两种类型的方法来减少VI-ReID问题中所存在的模态差异。特征级方法，图像级方法。但是这些方法存在以下问题： 特征级方法，由于模态差异大，无法直接投入到一个共享空间中去 虽然图像级方法可以减少模态差异，但生成的跨模态图像通常伴有噪声，这是由于缺乏VIS-IR图像对所致。 主要贡献/创新 提出了一种新颖的多样化嵌入扩展（DEE）模块，具有中心引导对挖掘（CPM）损失，以生成更多嵌入来学习多样化的特征表示。我们是第一个在 VIReID 的嵌入空间中增强嵌入的人。此外，我们还提出了一种有效的多级特征聚合（MFA）块来挖掘潜在的通道和空间特征表示。（DEE MFA CPM） 通过将 DEE、CPM 损失和 MFA 纳入端到端学习框架，我们提出了一种有效的多样化嵌入扩展网络（DEEN），可以有效减少 VIS 和 IR 图像之间的模态差异。 我们收集了一个低光跨模态（LLCM）数据集，其中包含在光照变化和低照度环境下捕获的 1,064 个身份的 46,767 张图像。 LLCM数据集具有更多新的重要特征，可以促进VIReID研究走向实际应用。 大量实验表明，在三个具有挑战性的数据集上，所提出的 DEEN 优于 VIReID 任务的其他最先进方法。 方法 整体流程图 上图展示了所提DEEN网络的流程框架，包括 DEE 模块和 MFA 块。 DEE 模块可以通过新颖的 CPM 损失生成更多嵌入，以学习不同的特征表示。 MFA 块可以聚合来自不同阶段的嵌入，以挖掘不同的通道和空间特征表示。 模型架构 上图概述了所提出的多样化嵌入扩展网络（DEEN），该网络利用双流 ResNet-50 网络作为主干。VIS-IR 特征被馈送到所提出的多样化嵌入扩展（DEE）中模块来生成更多嵌入。然后，提出了中心引导对挖掘（CPM）损失，以使生成的嵌入尽可能多样化，以学习信息丰富的特征表示。此外，我们采用有效的 MFA 块来聚合不同阶段的特征，以挖掘不同的通道和空间特征交涉。在训练阶段，批量归一化（BN）层之前和之后的所有特征都被输入到不同的损失中，以共同优化 DEEN。 Diverse Embedding Expansion Module（DEE） 所提出的DEE模块用于生成更多的嵌入，以缓解由于训练数据不足而带来的问题，该模块使用多分支卷积生成结构。具体来说，对于每个DEE分支，我们首先使用三个3x3的膨胀卷积层φ3×31\\varphi_{3 \\times 3}^{1}φ3×31​、φ3×32\\varphi_{3 \\times 3}^{2}φ3×32​、φ3×33\\varphi_{3 \\times 3}^{3}φ3×33​，这些层具有不同的膨胀比率（1, 2, 3），以将特征图 f\\mathbf{f}f 的数量减少到其自身大小的1/4，然后我们通过将这些特征图合并成一个特征图来获得特征图，接着通过ReLU激活层 FReLUF_{\\text{ReLU}}FReLU​ 来提高DEE的非线性表示能力。然后，将另一个卷积层 θ1×1\\theta_{1 \\times 1}θ1×1​ 应用于获得的特征图以改变其尺寸与 f\\mathbf{f}f 相同。因此，第 iii 个分支生成的嵌入 f+i\\mathbf{f}_{+}^{i}f+i​ 可以写成如下形式： f+i=θ1×1(FReLU(φ3×31(f)+φ3×32(f)+φ3×33(f)))\\mathbf{f}_{+}^{i} = \\theta_{1 \\times 1} ( F_{\\text{ReLU}} (\\varphi_{3 \\times 3}^{1}(\\mathbf{f}) + \\varphi_{3 \\times 3}^{2}(\\mathbf{f}) + \\varphi_{3 \\times 3}^{3}(\\mathbf{f}))) f+i​=θ1×1​(FReLU​(φ3×31​(f)+φ3×32​(f)+φ3×33​(f))) 然后，所有生成的嵌入被连接在一起并用作骨干网络下一阶段的输入。 代码实现如下： 12345678910111213141516171819202122232425262728293031class DEE_module(nn.Module): def __init__(self, channel, reduction=16): super(DEE_module, self).__init__() self.FC11 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=1, bias=False, dilation=1) self.FC11.apply(weights_init_kaiming) self.FC12 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=2, bias=False, dilation=2) self.FC12.apply(weights_init_kaiming) self.FC13 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=3, bias=False, dilation=3) self.FC13.apply(weights_init_kaiming) self.FC1 = nn.Conv2d(channel//4, channel, kernel_size=1) self.FC1.apply(weights_init_kaiming) self.FC21 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=1, bias=False, dilation=1) self.FC21.apply(weights_init_kaiming) self.FC22 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=2, bias=False, dilation=2) self.FC22.apply(weights_init_kaiming) self.FC23 = nn.Conv2d(channel, channel//4, kernel_size=3, stride=1, padding=3, bias=False, dilation=3) self.FC23.apply(weights_init_kaiming) self.FC2 = nn.Conv2d(channel//4, channel, kernel_size=1) self.FC2.apply(weights_init_kaiming) self.dropout = nn.Dropout(p=0.01) def forward(self, x): x1 = (self.FC11(x) + self.FC12(x) + self.FC13(x))/3 x1 = self.FC1(F.relu(x1)) x2 = (self.FC21(x) + self.FC22(x) + self.FC23(x))/3 x2 = self.FC2(F.relu(x2)) out = torch.cat((x, x1, x2), 0) out = self.dropout(out) return out FCnm的主要差别在不同的dilation和不同的padding大小 Center-Guided Pair Mining Loss(CPM) 从上面的操作可以看出，DEE 模块只能使用多分支卷积块生成更多的嵌入。然而，该操作无法有效地获得多样化的嵌入。因此，我们应用以下三个属性来限制生成的嵌入尽可能多样化，以有效减少可见光和红外图像之间的模态差异： 生成的embedding应该尽可能多样化，以有效地学习信息丰富的特征表示。（意味目标函数需要拉开生成embedding和原始embedding之间的距离） 生成的embedding应有助于减少可见光和红外图像之间的模态差异。（意味目标函数需要拉近VIS生成embedding和IR原始embedding的距离，以及 IR生成embedding和VIS原始embedding的距离） 类内距离应该小于类间距离（由于上面第二点的作用，可能会导致不同的模态之间的距离会小于相同模态之间的距离。这是不好的，需要满足第三点）。 上图为CPM的工作示意图，对于由VIS模态生成的嵌入，CPM损失可以表示为： L(fv,fn,fv+i)=[D(fnj,fv+i,j)−D(fvj,fv+i,j)−D(fvj,fvk)]+,(2)\\mathcal{L}(f_v, f_n, f_{v+}^{i}) = [D(f_{n}^{j}, f_{v+}^{i,j}) - D(f_{v}^{j}, f_{v+}^{i,j}) - D(f_{v}^{j}, f_{v}^{k})]_{+}, \\quad (2) L(fv​,fn​,fv+i​)=[D(fnj​,fv+i,j​)−D(fvj​,fv+i,j​)−D(fvj​,fvk​)]+​,(2) 其中 D(⋅,⋅)D(\\cdot, \\cdot)D(⋅,⋅) 是两个嵌入之间的欧几里得距离。fvf_vfv​ 和 fnf_nfn​ 是来自VIS和IR模态的原始嵌入，fv+if_{v+}^{i}fv+i​ 是从VIS模态的第i个分支生成的嵌入。j和k是minibatch中的不同身份，且 [z]+=max⁡(z,0)[z]_{+} = \\max(z, 0)[z]+​=max(z,0)。在公式（2）中，第一个项可以将生成的嵌入 fv+if_{v+}^{i}fv+i​ 向原始IR的嵌入 fnf_nfn​ 拉近，以减少 fv+if_{v+}^{i}fv+i​ 和 fnf_nfn​ 之间的模态差异。第二项可以将生成的嵌入 fv+f_{v+}fv+​ 推离VIS的嵌入 fvf_vfv​，以使 fvf_vfv​ 学习到信息丰富的特征表示。第三项可以使类内距离小于类间距离。 然后，我们使用每个类的嵌入中心 cvc_vcv​ 和 cnc_ncn​ 来使生成的嵌入中心 cv+ic_{v+}^{i}cv+i​ 和 cn+ic_{n+}^{i}cn+i​ 更具判别力，并引入一个边距项 α\\alphaα 来平衡公式（2）中的三个项。因此，对于来自VIS的嵌入，CPM损失表示为： L(cv,cn,cv+i)=[D(cnj,cv+i,j)−D(cvj,cv+i,j)−D(cvj,cvk)+α]+,(3)\\mathcal{L}(c_v, c_n, c_{v+}^{i}) = [D(c_{n}^{j}, c_{v+}^{i,j}) - D(c_{v}^{j}, c_{v+}^{i,j}) - D(c_{v}^{j}, c_{v}^{k}) + \\alpha]_{+}, \\quad (3) L(cv​,cn​,cv+i​)=[D(cnj​,cv+i,j​)−D(cvj​,cv+i,j​)−D(cvj​,cvk​)+α]+​,(3) 类似地，对于由IR生成的嵌入的类中心 cn+ic_{n+}^{i}cn+i​，我们有： L(cv,cn,cn+i)=[D(cv,cn+j)−D(cn,cn+j)−D(cnj,cnk)+α]+,(4)\\mathcal{L}(c_v, c_n, c_{n+}^{i}) = [D(c_{v}, c_{n+}^{j}) - D(c_{n}, c_{n+}^{j}) - D(c_{n}^{j}, c_{n}^{k}) + \\alpha]_{+}, \\quad (4) L(cv​,cn​,cn+i​)=[D(cv​,cn+j​)−D(cn​,cn+j​)−D(cnj​,cnk​)+α]+​,(4) 因此，最终的CPM损失可以表示为： Lcpm=L(cv,cn,cv+i)+L(cv,cn,cn+i).(5)\\mathcal{L}_{cpm} = \\mathcal{L}(c_v, c_n, c_{v+}^{i}) + \\mathcal{L}(c_v, c_n, c_{n+}^{i}). \\quad (5) Lcpm​=L(cv​,cn​,cv+i​)+L(cv​,cn​,cn+i​).(5) 此外，为了确保不同分支生成的嵌入能够捕获不同的信息丰富的特征表示，我们强制这些不同分支生成的嵌入彼此正交，以最小化重叠元素。因此，正交损失可以表示为： Lort=∑m=1i−1∑n=m+1i(f+mTf+n),(6)\\mathcal{L}_{ort} = \\sum_{m=1}^{i-1} \\sum_{n=m+1}^{i} (\\mathbf{f}_{+}^{m{T}} \\mathbf{f}_{+}^{n}), \\quad (6) Lort​=m=1∑i−1​n=m+1∑i​(f+mT​f+n​),(6) 其中m,nm,nm,n分别代表的是第几个分支生成的embedding。 我们认为通过正交损失可以强制生成的嵌入学习更多信息的特征表示。 Multistage Feature Aggregation Block（MFA） 不同层次特征的聚合已被证明对语义分割、分类和检测任务有帮助。为了从不同阶段聚合特征以挖掘多样的通道级和空间特征表示，我们结合了一个有效的通道-空间多阶段特征聚合（MFA）模块，以聚合多阶段特征，灵感来自于。 接下来，我们详细阐述了MFA模块的细节，如上图所示。具体来说，我们在骨干网络的每个阶段为通道-空间聚合模块考虑了两种类型的源特征：阶段前的低级特征图 fl∈RCl×Hl×Wl\\mathbf{f}_{l} \\in \\mathbb{R}^{C_{l} \\times H_{l} \\times W_{l}}fl​∈RCl​×Hl​×Wl​ 和阶段后的高级特征图 fh∈RCh×Hh×Wh\\mathbf{f}_{h} \\in \\mathbb{R}^{C_{h} \\times H_{h} \\times W_{h}}fh​∈RCh​×Hh​×Wh​，其中C、W和H分别表示通道数、宽度和高度。首先，我们使用三个1x1卷积层 ψq1,ψv1,ψk1\\psi_{q}^{1}, \\psi_{v}^{1}, \\psi_{k}^{1}ψq1​,ψv1​,ψk1​ 将 f\\mathbf{f}f 转换为三个紧凑的嵌入：ψq1(fh)\\psi_{q}^{1}(\\mathbf{f}_{h})ψq1​(fh​), ψv1(fl)\\psi_{v}^{1}(\\mathbf{f}_{l})ψv1​(fl​) 和 ψk1(fl)\\psi_{k}^{1}(\\mathbf{f}_{l})ψk1​(fl​)。然后，通过矩阵乘法和softmax计算通道相似度矩阵 Mc∈RC′×C′\\mathbf{M}^{c} \\in \\mathbb{R}^{C&#x27; \\times C&#x27;}Mc∈RC′×C′： Mc=Fsoftmax(ψq1(fh)×ψk1(fl)).(7)\\mathbf{M}^{c} = F_{\\text{softmax}}(\\psi_{q}^{1}(\\mathbf{f}_{h}) \\times \\psi_{k}^{1}(\\mathbf{f}_{l})). \\quad (7) Mc=Fsoftmax​(ψq1​(fh​)×ψk1​(fl​)).(7) 因此，我们通过矩阵乘法恢复 ψv1(fl)\\psi_{v}^{1}(\\mathbf{f}_{l})ψv1​(fl​) 和 Mc\\mathbf{M}^{c}Mc 的通道维度来实现通道级多阶段特征聚合。在这之后，另一个1x1卷积层 ωc\\omega^{c}ωc 被应用于将上述特征图的尺寸转换为 fh\\mathbf{f}_{h}fh​ 的尺寸。最后，我们通过矩阵加法将 fh\\mathbf{f}_{h}fh​ 加到它上面来获得输出： fhc=ωc(ψv1(fl)×Mc)+fh.(8)\\mathbf{f}_{h}^{c} = \\omega^{c}(\\psi_{v}^{1}(\\mathbf{f}_{l}) \\times \\mathbf{M}^{c}) + \\mathbf{f}_{h}. \\quad (8) fhc​=ωc(ψv1​(fl​)×Mc)+fh​.(8) 之后，通过上述操作获得的 fhc\\mathbf{f}_{h}^{c}fhc​ 和低级特征图 fl\\mathbf{f}_{l}fl​ 被用来执行空间特征聚合操作，这类似于通道级多阶段特征聚合操作。最后，我们得到MFA的输出如下： fss=ωs(ψv2(fl)×Ms)+fh,(9)\\mathbf{f}_{s}^{s} = \\omega^{s}(\\psi_{v}^{2}(\\mathbf{f}_{l}) \\times \\mathbf{M}^{s}) + \\mathbf{f}_{h}, \\quad (9) fss​=ωs(ψv2​(fl​)×Ms)+fh​,(9) 其中 ωs\\omega^{s}ωs 和 ψv2\\psi_{v}^{2}ψv2​ 是两个1x1卷积层，Ms\\mathbf{M}^{s}Ms 是空间相似度矩阵。 Multi-Loss Optimization 除了所提出的 Lcpm\\mathcal{L}_{cpm}Lcpm​ 和 Lort\\mathcal{L}_{ort}Lort​ 外，我们还结合了交叉熵损失 Lce\\mathcal{L}_{ce}Lce​ 和三重态损失 Ltri\\mathcal{L}_{tri}Ltri​ 以端到端的方式联合优化网络，通过最小化这四个损失的总和 Ltotal\\mathcal{L}_{total}Ltotal​，其公式如下： Ltotal=Lce+Ltri+λ1Lcpm+λ2Lort,(10)\\mathcal{L}_{total} = \\mathcal{L}_{ce} + \\mathcal{L}_{tri} + \\lambda_1 \\mathcal{L}_{cpm} + \\lambda_2 \\mathcal{L}_{ort}, \\quad (10) Ltotal​=Lce​+Ltri​+λ1​Lcpm​+λ2​Lort​,(10) 其中 λ1\\lambda_1λ1​ 和 λ2\\lambda_2λ2​ 是控制损失项相对重要性的系数。 LLCM数据集 简介 在本文中，我们收集了一个新的具有挑战性的低光跨模态数据集，称为 LLCM 数据集。 LLCM数据集利用部署在弱光环境中的9个摄像头网络，可以在白天捕获VIS图像，在夜间捕获IR图像。为了保护个人隐私信息，我们利用 MTCNN来获取人脸的边界框并模糊这些区域。我们确保每个带注释的身份都被可见光和红外摄像机捕获。上图显示了 LLCM 数据集的一些示例。与现有的VIReID数据集相比，LLCM数据集具有以下新的重要特征：首先，LLCM数据集中的图像是在VIS和IR模态的复杂低光环境下捕获的，其中包含严重的光照变化和是现实场景中常见的问题。如上图所示，恶劣的光照条件会改变人衣服的颜色并导致衣服纹理信息的丢失，这给VIReID带来了巨大的挑战。其次，LLCM 数据集具有大量的身份和边界框。该数据集包含 1,064 个身份的 46,767 个边界框，使其成为目前最大的 VIReID 数据集。第三，LLCM数据集是从一月到四月的100多天收集的，考虑了不同的气候条件和布料风格。长期数据收集有助于研究不同气候和服装风格下的 VIReID 任务，从而增加了 VIReID 模型的泛化性。 评估协议 我们将LLCM数据集按约2:1的比例划分为训练集和测试集。训练集包含30,921个边界框，共713个身份（其中16,946个边界框来自可见光（VIS）模态，13,975个边界框来自红外（IR）模态）；测试集包含13,909个边界框，共351个身份（其中8,680个边界框来自可见光模态，7,166个边界框来自红外模态）。与RegDB数据集相似，我们使用从可见光到红外（VIS to IR）和从红外到可见光（IR to VIS）两种模式评估VI-ReID模型的性能。在测试阶段，我们随机选择每个身份的一个图像，形成画廊集用于模型性能的评估。我们随机分割画廊集并进行10次以上的评估，报告平均性能。 实验结果 可以看到DEEN在三个数据集上相比之前的方法都取得了SOTA的效果 消融实验 各组件消融 为了评估DEEN中各组件的贡献，进行了消融研究。结果显示，尽管DEE模块略微提升了基线性能，但效果有限。然而，结合CPM损失后，DEE显著提升了模型性能，并减少了VIS和IR图像之间的模态差异。此外，MFA模块通过聚合不同阶段的特征，进一步提高了性能。综合DEE、CPM和MFA的端到端学习框架在两个具有挑战性的VIRReID数据集上表现出显著的性能提升，表明这些模块能互相受益，生成多样化的嵌入。 DEE插入位置消融 实验研究了在ResNet-50的不同阶段插入DEE模块对DEEN性能的影响。结果表明，当DEE模块插入在stage-0到stage-3之后时，性能逐步提高，说明模态差距减少，DEE在网络更深层的生成能力更强。在stage-3之后插入DEE在LLCM和SYSU-MM01数据集上达到了最佳效果。然而，在stage-4之后插入DEE时，性能显著下降，因为CPM损失直接作用于嵌入，增大了生成嵌入与原始嵌入之间的距离，增加了模型优化难度。因此，除非特别指定，我们默认将DEE插入在ResNet-50的stage-3之后。 DEE分支数消融 研究表明，DEE模块的性能随着分支数量从2增加到3而逐渐提高，因生成了更多的嵌入来减少模态差距。然而，当分支数量超过3时，性能因冗余特征过多而下降。因此，具有三个分支的DEE在LLCM和SYSU-MM01数据集上表现最佳，表明这是生成多样化嵌入的最佳配置。默认情况下，DEE模块使用三个分支。 MFA与NL对比消融 上表实验结果表明，MFA块比Non-Local块在Rank-1准确率和mAP上分别高出1.1%和2.2%，验证了MFA块的有效性。此外，MFA块和DEE模块在生成多样化嵌入、减少VIS和IR图像之间的模态差距方面相辅相成。 超参数消融 主要是对λ1\\lambda_1λ1​,λ2\\lambda_2λ2​和α\\alphaα三个超参数进行消融实验，最后发现λ1=0.8\\lambda_1=0.8λ1​=0.8，λ2=0.1\\lambda_2=0.1λ2​=0.1，α=0.2\\alpha=0.2α=0.2能够取得更好的实验结果。 可视化 特征分布 为了研究 DEEN 有效的原因，我们在 LLCM 数据集上可视化类间和类内距离，如上图 (a-e) 所示。比较上图（c-e）和上图（a-b），类间和类内距离的均值（即垂直线）被MFA、DEE和DEEN推开，其中δ1 &lt;δ2 &lt;δ3并且δ1&lt;δ2&lt;δ4&lt;δ5。这表明，与初始特征（图7（a））和基线特征（图7（b））的类内距离相比，DEEN的类内距离显着减小。因此，DEEN可以有效地减少可见光和红外图像之间的模态差异。同时，我们还在上图（f-j）中用t-SNE可视化2D特征空间中的特征分布，这表明MFA、DEE和DEEN可以有效地区分和聚合同一个人的特征嵌入，并减少模态差异。 检索结果 为了进一步显示 DEEN 的有效性，为了进一步展示DEEN的有效性，我们在图8中展示了DEEN在我们的LLCM数据集上的一些检索结果。对于每个检索案例，绿色框表示与给定查询对应的正确匹配图像，而红色框表示不正确的匹配图像。总体而言，与基线相比，DEEN可以有效地改进排序结果，使更多正确匹配的图像排在前面的位置。 总结 在本文中，我们提出了一种新颖的多样化嵌入扩展网络（DEEN），用于VIRelD任务的嵌入空间。所提出的DEEN可以生成多样化的嵌入，并挖掘多样的通道级和空间嵌入，以学习信息丰富的特征表示，从而减少VIS和IR图像之间的模态差异。此外，我们还提供了一个具有挑战性的低光跨模态（LLCM）数据集，该数据集包含更多新的重要特征，可以进一步促进VIRelD研究向实际应用迈进。对SYSU-MM01、RegDB和LLCM数据集的广泛实验表明，所提出的DEEN在多种最新方法中具有优越性。","tags":["ReID","VI-ReID","CVPR","2023","DEEN"],"categories":["ReID","VI-ReID"]},{"title":"【论文笔记】AUL","path":"/2024/09/26/MudSynth/ReID/【论文笔记】AUL/","content":"文章基本信息 文章名称：Adaptive Uncertainty-Based Learning for Text-Based Person Retrieval 发表会议/年份：AAAI 2024 作者：Shenshen Li, Chen He, Xing Xu*, Fumin Shen, Yang Yang, Heng Tao Shen 单位：School of Computer Science and Engineering and Center for Future Media, University of Electronic Science and Technology of China, China 摘要 基于文本的行人检索旨在根据文本描述从图库中检索特定的行人图像。主要挑战是如何在显著的类内变化和最小的类间变化情况下克服固有的异质模态差距。现有的方法通常采用视觉-语言预训练或注意力机制，从噪声输入中学习适当的跨模态对齐。尽管取得了显著进展，当前的方法不可避免地存在两个缺陷：1) 匹配歧义，主要源于不可靠的匹配对；2) 单方面的跨模态对齐，源于缺乏探索一对多对应关系，即粗粒度语义对齐。这些关键问题显著降低了检索性能。为此，我们提出了一种新颖的框架，称为基于自适应不确定性的学习（Adaptive Uncertainty-Based Learning，AUL），用于从不确定性角度进行基于文本的行人检索。具体来说，我们的AUL框架由三个关键组件组成：1) 不确定性感知匹配过滤，利用主观逻辑（Subjective Logic） 有效减少不可靠匹配对的干扰，并选择高置信度的跨模态匹配进行训练；2) 基于不确定性的对齐优化，不仅通过构建不确定性表示来模拟粗粒度对齐，还进行渐进学习，以适当地结合粗粒度和细粒度对齐；3) 跨模态掩码建模，旨在探索视觉和语言之间更全面的关系。大量实验表明，我们的AUL方法在监督、弱监督和域泛化设置下的三个基准数据集上始终实现了最先进的性能。我们的代码可在 https://github.com/CFM-MSG/Code-AUL 获取。 之前工作存在的问题 匹配模糊性：由于仅考虑一对一匹配，忽略了语言和视觉之间的一对多对应关系，导致匹配模糊，限制了模型的性能和泛化能力。 单方面的跨模态对齐：这些方法往往无法全面捕捉视觉和语言之间的关系，导致对齐过程单方面且不完整。 不可靠的匹配对：由于类内变化大和类间变化小引入的固有数据噪声，基于相似性选择的跨模态匹配对往往不准确，可能会错误地将负样本识别为真实标签，进一步降低了匹配的准确性。 主要贡献/创新 基于上述观察，提出了一种名为自适应不确定性学习（AUL，Adaptive Uncertainty-based Learning）的新框架，用于从不确定性角度进行基于文本的人员检索。我们的主要贡献可以总结如下： 不确定性感知匹配过滤策略：通过仔细考虑匹配不确定性，我们设计了一种不确定性感知匹配过滤（Uncertainty-aware Matching Filtration）策略，该策略利用主观逻辑（Subjective Logic）自适应地选择高置信度的跨模态匹配，减轻不可靠匹配对对训练的干扰。 基于不确定性的对齐优化模块：我们提出了一个基于不确定性的对齐优化（Uncertainty-based Alignment Refinement）模块，该模块不仅通过构建不确定性表示来模拟粗粒度对齐，还逐步组织多粒度对齐。 跨模态掩码建模模块：我们部署了一个跨模态掩码建模（Cross-modal Masked Modeling）模块，通过全面的跨模态交互重构图像和文本模态信号，进一步探索两种模态之间的对应关系。 方法 Preliminary 基于文本的人员检索任务的目标是从候选库中分辨并检索到与提供的文本查询最匹配的人员图像。为了获取正确的行人图像，我们提出的框架侧重于通过学习文本描述与对应人员图像之间的相似性来促进精确的对齐。 形式上，我们定义{Ii,Ti}\\{I_i, T_i\\}{Ii​,Ti​}为训练数据集中的图像-文本对。每对包括一张人员图像IiI_iIi​及其对应的文本描述TiT_iTi​。我们首先将图像IiI_iIi​输入到图像编码器中，生成一系列视觉特征{vicls,vi1,…,vin}\\{v_{i}^{cls}, v_{i}^{1}, \\ldots, v_{i}^{n}\\}{vicls​,vi1​,…,vin​}，其中viclsv_{i}^{cls}vicls​作为全局视觉特征，{vi1,…,vin}\\{v_{i}^{1}, \\ldots, v_{i}^{n}\\}{vi1​,…,vin​}表示视觉补丁特征。此外，我们利用文本编码器获得一系列文本表示{ticls,ti1,…,tin}\\{t_{i}^{cls}, t_{i}^{1}, \\ldots, t_{i}^{n}\\}{ticls​,ti1​,…,tin​}，其中ticlst_{i}^{cls}ticls​和{ti1,…,tin}\\{t_{i}^{1}, \\ldots, t_{i}^{n}\\}{ti1​,…,tin​}分别表示全局文本特征和标记特征。 Uncertainty-aware Matching Filtration(UMF) 模块目的： 使用主观逻辑对文本图像对匹配的不确定性进行建模，用于筛选可靠匹配，减轻由于不可靠匹配对 带来的不确定性影响 主观逻辑背景 主观逻辑（Subjective Logic，SL）提供了对Dempster-Shafer理论（Yager 和 Liu 2008）不确定性分配原则的形式化表示，建模为狄利克雷分布。因此，它提供了利用SL理论量化不确定性的方法，在严格建立的理论框架内进行。具体来说，我们首先获得针对第iii个单元预测的证据向量ei\\mathbf{e}_iei​。然后我们为每个单元建模不确定性u\\mathbf{u}u和信任质量p={pk}k=1N\\mathbf{p} = \\{p_k\\}_{k=1}^Np={pk​}k=1N​，其公式如下： pk=ekS,u=NS,p_k = \\frac{e_k}{S}, \\quad u = \\frac{N}{S}, pk​=Sek​​,u=SN​, 其中，S=∑k=1N(ek+1)S = \\sum_{k=1}^{N}(e_k + 1)S=∑k=1N​(ek​+1)可以被认为是狄利克雷分布的强度，而信任概率pkp_kpk​对应于狄利克雷分布α={ek+1}k=1N\\alpha = \\{e_k + 1\\}_{k=1}^Nα={ek​+1}k=1N​的参数。注意，不确定性u\\mathbf{u}u与总证据呈反比关系。最后，由α\\alphaα描述的狄利克雷分布可以定义为： D(p∣α)={1B(α)∏j=1Npjαj−1for p∈SN,0otherwise, D(\\mathbf{p}|\\alpha) = \\begin{cases} \\frac{1}{B(\\alpha)} \\prod_{j=1}^{N} p_j^{\\alpha_j - 1} &amp; \\text{for } \\mathbf{p} \\in \\mathbb{S}_N, \\\\ 0 &amp; \\text{otherwise}, \\end{cases} D(p∣α)={B(α)1​∏j=1N​pjαj​−1​0​for p∈SN​,otherwise,​ 其中B(α)B(\\alpha)B(α)表示N维贝塔函数，SN\\mathbb{S}_NSN​是N维单纯形。 不确定性感知学习 为了有效减轻由于不可靠匹配对带来的不确定性影响，需要对匹配不确定性进行建模。尽管主观逻辑（Subjective Logic, SL）理论在不确定性建模方面显示了显著的进展，但直接将其应用于基于文本的人员检索并不合适。为了将SL扩展到这一特定任务，初步步骤是表示第iii个文本与第jjj个图像之间的跨模态匹配证据eij=exp⁡f(Sim(ticls,vicls))e_{ij} = \\exp^{f(Sim(t_i^{cls}, v_i^{cls}))}eij​=expf(Sim(ticls​,vicls​))，其中Sim(⋅)Sim(\\cdot)Sim(⋅)和fff分别表示余弦相似度和ReLU函数。第iii个文本的所有匹配的总证据ei\\mathbf{e}_iei​可以表示为ei={eij}j=1N\\mathbf{e}_i = \\{e_{ij}\\}_{j=1}^Nei​={eij​}j=1N​。 根据前一部分提到的主观逻辑，我们获得了αi\\alpha_iαi​并将匹配不确定性u\\mathbf{u}u建模如下： αi=ei+1,u=NS,\\alpha_i = e_i + 1, \\quad \\mathbf{u} = \\frac{N}{S}, αi​=ei​+1,u=SN​, 其中，S=∑i=1N(αi)S = \\sum_{i=1}^{N}(\\alpha_i)S=∑i=1N​(αi​)可以被视为狄利克雷分布的强度。基于获得的匹配不确定性，我们执行不确定性感知学习，自适应地过滤不可靠的匹配对并选择高置信度的跨模态匹配。具体来说，我们设计了具有不确定性感知动态权重函数φ(m(i))\\varphi(m(i))φ(m(i))的交叉熵损失Lu\\mathcal{L}_uLu​，在优化过程中为具有较低匹配不确定性的跨模态匹配分配较大的权重，为具有较高匹配不确定性的匹配分配较小的权重，从而减少由不可靠匹配对造成的负面影响。损失函数Lu\\mathcal{L}_uLu​可以表示为： Lu=λ∑i=1Nφ(m(i))Yi(log⁡(Si)−log⁡(αi)),\\mathcal{L}_u = \\lambda \\sum_{i=1}^{N} \\varphi(m(i)) Y_i \\left( \\log(S_i) - \\log(\\alpha_i) \\right), Lu​=λi=1∑N​φ(m(i))Yi​(log(Si​)−log(αi​)), 其中，λ\\lambdaλ是超参数，YiY_iYi​是第iii个样本的one-hot标签，φ(m(i))=m(i)N∈(0,1]\\varphi(m(i)) = \\frac{m(i)}{N} \\in (0, 1]φ(m(i))=Nm(i)​∈(0,1]，m(i)m(i)m(i)表示通过按降序排列不确定性u\\mathbf{u}u获得的第iii个跨模态匹配的序号。 模块亮点：不同于之前ReID问题所采用的triplet等损失函数，他是一个比较软的约束条件，triplet等函数从标签判定，标签不一致，直接权重就是0，这里通过相似度计算的不确定性进行约束，或许能让模型拥有更强的泛化性能 Uncertainty-based Alignment Refinement 模块目的： Uncertainty-based Alignment Refinement（UAR）模块旨在解决视觉和语言之间缺乏一对多对应关系的问题，导致检索性能退化。通过构建具有不确定性的视觉表示，并采用渐进学习方法，UAR 模块在训练过程中逐步优化粗粒度和细粒度对齐，最终提高跨模态检索性能。 由于视觉和语言之间缺乏一对多的对应关系，现有方法主要集中于探索单方面的跨模态对齐，即一对一的对应关系，导致检索性能退化。为了解决这一限制，我们提出了基于不确定性的对齐优化（Uncertainty-based Alignment Refinement，UAR）模块，该模块模拟粗粒度对齐，并采用渐进学习以易到难的方式协同优化粗粒度和细粒度对齐。 不确定性表示构建：鉴于NNN个图像-文本对的全局表示(vcls,tcls)(v^{cls}, t^{cls})(vcls,tcls)，我们首先需要显式构建具有不确定性的视觉表示，这通过将高斯噪声添加到原始特征分布中来实现。高斯噪声的均值μ\\muμ和标准差σ\\sigmaσ从原始特征vclsv^{cls}vcls中得出。然后我们通过将生成的高斯噪声添加到归一化后的特征vˉcls\\bar{v}^{cls}vˉcls来构建具有不确定性的视觉表示v^cls\\hat{v}^{cls}v^cls，其公式如下： v^cls=αv⋅vˉcls+βv,\\hat{v}^{cls} = \\alpha_v \\cdot \\bar{v}^{cls} + \\beta_v, v^cls=αv​⋅vˉcls+βv​, 其中，αv\\alpha_vαv​和βv\\beta_vβv​是引入噪声的不确定性向量，αv∼N(1,σ)\\alpha_v \\sim N(1, \\sigma)αv​∼N(1,σ)，βv∼N(μ,σ)\\beta_v \\sim N(\\mu, \\sigma)βv​∼N(μ,σ)，vˉcls\\bar{v}^{cls}vˉcls是归一化后的特征： vˉcls=vcls−μσ.\\bar{v}^{cls} = \\frac{v^{cls} - \\mu}{\\sigma}. vˉcls=σvcls−μ​. 渐进对齐学习：基于获得的不确定性视觉表示v^icls\\hat{v}^{cls}_iv^icls​和文本表示ticlst^{cls}_iticls​，我们采用InfoNCE损失（InfoNCE loss）Linfo\\mathcal{L}_{info}Linfo​（Lee, Kim, and Han 2021；Yang et al. 2023）进行粗粒度对齐，并进一步探索一对多对应关系。粗粒度对齐损失可以定义为： Lca=Linfo(v^icls,ticls)2σ2+12log⁡σ2,\\mathcal{L}_{ca} = \\frac{\\mathcal{L}_{info} (\\hat{v}^{cls}_i, t^{cls}_i)}{2\\sigma^2} + \\frac{1}{2} \\log \\sigma^2, Lca​=2σ2Linfo​(v^icls​,ticls​)​+21​logσ2, Linfo(v^icls,ticls)=−log⁡exp⁡(sim(v^icls,ticls)/τ)exp⁡(sim(v^icls,ticls)/τ)+∑k=1Nexp⁡(sim(v^icls,tnegcls)/τ)\\mathcal{L}_{info}(\\hat{v}_i^{cls}, t_i^{cls}) = -\\log \\frac{\\exp(\\text{sim}(\\hat{v}_i^{cls}, t_i^{cls})/\\tau)}{\\exp(\\text{sim}(\\hat{v}_i^{cls}, t_i^{cls})/\\tau) + \\sum_{k=1}^{N} \\exp(\\text{sim}(\\hat{v}_i^{cls}, t_{neg}^{cls})/\\tau)}Linfo​(v^icls​,ticls​)=−logexp(sim(v^icls​,ticls​)/τ)+∑k=1N​exp(sim(v^icls​,tnegcls​)/τ)exp(sim(v^icls​,ticls​)/τ)​ 对于细粒度对齐，即一对一对应关系，我们设计了一种成对损失函数Lfa\\mathcal{L}_{fa}Lfa​以缓解密集采样机制的负面影响（Zhou et al. 2023）。成对损失函数使用一个负样本tnegclst^{cls}_{neg}tnegcls​写为： Lfa=−log⁡ψ(v^icls,ticls)ψ(v^icls,ticls)+ψ(v^icls,tnegcls),\\mathcal{L}_{fa} = - \\log \\frac{\\psi (\\hat{v}^{cls}_i, t^{cls}_i)}{\\psi (\\hat{v}^{cls}_i, t^{cls}_i) + \\psi (\\hat{v}^{cls}_i, t^{cls}_{neg})}, Lfa​=−logψ(v^icls​,ticls​)+ψ(v^icls​,tnegcls​)ψ(v^icls​,ticls​)​, 直观上，进行细粒度对齐比粗粒度对齐更具挑战性。因此，我们的策略是在训练过程中逐步为粗粒度对齐分配较高的权重，而为细粒度对齐分配较低的权重，最终逐步逆转这种分配。我们提出了对齐渐进学习（Alignment Progressive Learning，APL），将动态权重引入损失函数，使其逐步关注多粒度对齐，并在优化过程中按“易到难”的方式逐步优化目标La\\mathcal{L}_aLa​，如下所示： La=∑i=1Nφ(m(i))(γLca+(1−γ)Lfa),\\mathcal{L}_a = \\sum_{i=1}^{N} \\varphi(m(i)) \\left( \\gamma \\mathcal{L}_{ca} + (1 - \\gamma) \\mathcal{L}_{fa} \\right), La​=i=1∑N​φ(m(i))(γLca​+(1−γ)Lfa​), 其中，γ=exp⁡(−γ0⋅epochtotal_epoch)\\gamma = \\exp\\left( -\\frac{\\gamma_0 \\cdot \\text{epoch}}{\\text{total\\_epoch}} \\right)γ=exp(−total_epochγ0​⋅epoch​)，γ0\\gamma_0γ0​是初始权重。 Cross-modal Masked Modeling 模块目的： 受MAE的启发，通过自监督学习提升模型的性能，增加两种模态的交互，消除模态差异造成的影响 为了增强图像和文本之间的交互，我们设计了跨模态掩码建模（Cross-modal Masked Modeling，CMM），通过使用掩码输入来重构一种模态的内在信号，该输入依赖于图像和文本模态的未掩码输入。CMM可以进一步分为两个部分：跨模态掩码图像建模（Cross-modal Masked Image Modeling，CMIM）和跨模态掩码语言建模（Cross-modal Masked Language Modeling，CMLM）。 以CMIM为例，按照MAE（He等，2022）的方式，我们获得掩码图像的表示Vmi={vij}j=1nu\\mathbf{V}_{mi} = \\{v_{i}^{j}\\}_{j=1}^{n_u}Vmi​={vij​}j=1nu​​，其中nun_unu​表示未掩码标记的数量。然后，我们利用包括多头交叉注意力层和三层变换器块的跨模态编码器fef_efe​，根据掩码图像Vmi\\mathbf{V}_{mi}Vmi​和原始文本表示Ei={tij}j=1n\\mathbf{E}_i = \\{t_i^j\\}_{j=1}^nEi​={tij​}j=1n​的表示，预测所有原始标记。最后，通过图像跨模态解码器fdf_dfd​将预测结果映射回RGB图像空间，解码器的结构与编码器相同，后接一个线性层。CMIM的总过程表示为： Lcmim=1Ω(Ii)∥Ii−fd(fe(Vmi,Ei))∥1,\\mathcal{L}_{cmim} = \\frac{1}{\\Omega(I_i)} \\|I_i - f_d(f_e(\\mathbf{V}_{mi}, \\mathbf{E}_i))\\|_1, Lcmim​=Ω(Ii​)1​∥Ii​−fd​(fe​(Vmi​,Ei​))∥1​, 其中，Ω(⋅)\\Omega(\\cdot)Ω(⋅)表示像素的数量，损失函数Lcmim\\mathcal{L}_{cmim}Lcmim​基于L1L_1L1​损失。 类似于CMIM，给定掩码文本表示Emi\\mathbf{E}_{mi}Emi​和原始视觉表示Vi\\mathbf{V}_iVi​，我们利用交叉熵损失函数H\\mathcal{H}H来测量预测与掩码文本标记Emi\\mathbf{E}_{mi}Emi​之间的距离，即进行跨模态掩码语言建模。CMM的目标可以计算为： Lcmm=Lcmim+H(ymi,ftd(fte(Vi,Emi))),\\mathcal{L}_{cmm} = \\mathcal{L}_{cmim} + \\mathcal{H}(y_{mi}, f_{td}(f_{te}(\\mathbf{V}_i, \\mathbf{E}_{mi}))), Lcmm​=Lcmim​+H(ymi​,ftd​(fte​(Vi​,Emi​))), 其中，ymiy_{mi}ymi​是第iii个掩码标记的one-hot标签，ftef_{te}fte​与CMIM的跨模态编码器相同，ftdf_{td}ftd​是分类器头。通过最小化Lcmm\\mathcal{L}_{cmm}Lcmm​，模型被迫通过跨模态交互执行原始信号的重构。此过程有效促进了图像和文本模态之间更深层次关系的探索。 最终，训练的总损失Ltotal\\mathcal{L}_{total}Ltotal​表示为： Ltotal=Lu+La+Lcmm.\\mathcal{L}_{total} = \\mathcal{L}_u + \\mathcal{L}_a + \\mathcal{L}_{cmm}. Ltotal​=Lu​+La​+Lcmm​. 实验结果 消融实验 如表4所示，我们得出以下结论： 对比No.0和No.3，表明我们提出的不确定性感知匹配过滤（UMF）显著提升了检索性能。这再次证明，引入主观逻辑（SL）理论来建模跨模态匹配模糊的不确定性对于筛选高置信度对齐是有效的，使我们的模型能够专注于可靠的检索结果。 No.5的模型性能优于No.1，特别是在R@5和R@10方面。这表明基于不确定性的对齐优化（UAR）通过应用基于高斯噪声的不确定性表示可以有效地探索一对多的对应关系。此外，UAR采用的渐进学习方法能够适当地结合粗粒度和细粒度对齐。 从No.6和No.3的比较中，我们推测添加跨模态掩码建模（CMM）对检索性能有更大的影响。一个可能的原因是，通过进一步的跨模态交互进行掩码语言建模（MLM）和掩码图像建模（MIM），在视觉和语言之间的细粒度和相关关系挖掘方面带来了额外的优势。 关于选择CMLM还是CMIM的分析 对CMLM和CMIM选择的分析。我们进一步探讨了CMLM和CMIM各自的重要性。如图4所示，我们可以观察到： 含有Lcmlm\\mathcal{L}_{cmlm}Lcmlm​的消融模型（w/Lcmlm\\mathcal{L}_{cmlm}Lcmlm​）表现优于基线模型。我们认为，性能提升的原因在于图像和文本之间的充分交互，这有助于弥合视觉和语言之间显著的模态差距。 然而，仅应用CMLM的Lcmlm\\mathcal{L}_{cmlm}Lcmlm​损失并不如同时应用CMIM和CMLM（即w/ CMM）的组合效果好。这表明，将掩码文本和视觉标记同时作为挖掘全面跨模态关系的锚点是不可或缺的。 对UAR中渐进对齐学习APL的分析 在此，我们研究了我们提出的渐进对齐学习（Alignment Progressive Learning，APL）的进展，APL旨在全面探索一对一和一对多的对应关系。通过观察图3，我们可以发现： 引入动态权重(\\gamma)的效果优于使用平均权重的消融模型（Avg）。我们推测其原因是渐进学习的应用在学习全面的多粒度对齐中起到了重要作用。 所提出的APL初始阶段有效地为粗粒度对齐分配了较高的权重，并逐步转向为细粒度对齐分配更高的权重。 此外，我们进一步探索了以易到难的方式学习多粒度对齐的有效性。特别地，我们比较了利用(\\gamma)和(\\hat{\\gamma})的性能。显然，前者在适当的对齐整合和检索准确性方面表现出更好的适应性。这一发现支持我们的直觉，即引导模型以易到难的方式逐步学习适当的多粒度对齐是比其他方法更合理的。 对匹配不确定性感知动态权重分配的分析 为了进一步验证匹配模糊性的存在及我们提出的UMF的重要性，我们深入研究了各种不确定性感知权重分配与整体性能之间的关系。从图5中得出的观察结果如下： 分布分析清楚地揭示了不可靠匹配对的存在，这些对特征为显著的匹配不确定性。这种不确定性源于显著的类内变化和有限的类间变化，阻碍了检索性能的提升。 为了强调匹配不确定性感知动态权重分配的有效性，我们比较了不同权重分配的性能。将高不确定性的跨模态匹配设为1(⋅)的性能最差，这反映了我们的模型受严重匹配模糊性影响这一动机的合理性。 Qualitative Analysis 如图6所示，我们进行了定性分析，比较了我们提出的AUL方法与近期的APTM方法（Yang等，2023）在前6个检索结果中的表现。根据可视化结果，我们的AUL方法在检索准确性上优于APTM方法。具体来说，我们的方法AUL能够满足细粒度和粗粒度的检索需求，例如“长袖”或“高个子男人”，因为我们提出的UAR能够逐步且全面地获取多粒度语义。此外，AUL还对匹配不确定性进行建模，以量化由类内变化大和类间变化小引起的模糊性，从而减轻不可靠匹配对的干扰，提高性能。 总结 在本文中，我们提出了一种新颖的自适应不确定性学习（Adaptive Uncertainty-based Learning，AUL）方法，从不确定性角度出发进行基于文本的人员检索。我们提出了不确定性感知匹配过滤（Uncertainty-aware Matching Filtration，UMF）来量化并防止由于不可靠匹配对引起的模糊影响。此外，我们设计了基于不确定性的对齐优化（Uncertainty-based Alignment Refinement，UAR）和跨模态掩码建模（Cross-modal Masked Modeling，CMM）来增强对齐学习并关注适当的跨模态关系。在三个基准上的大量实验表明了我们提出的AUL方法的优越性。未来，我们将探索其他策略以进一步提升检索性能。","tags":["ReID","TI-ReID","AAAI","2024","AUL"],"categories":["ReID","TI-ReID"]},{"title":"DDPM 原理推导","path":"/2024/09/26/MudSynth/Diffusion/DDPM 原理推导/","content":"文章基本信息 文章名称：Denoising Diffusion Probabilistic Models 发表会议/年份：NeurIPS 2020 作者：Jonathan Ho, Ajay Jain, Pieter Abbeel 单位：UC Berkeley 前置知识 Markov：当前位置的概率只会受到前一时刻概率影响 正态分布的叠加性eg.N(μ1,σ12)+N(μ2,σ22)=N(μ1+μ2,σ12+σ22)eg. N(\\mu_1,\\sigma_1^2)+N(\\mu_2,\\sigma_2^2) = N(\\mu_1+\\mu_2,\\sigma_1^2+\\sigma_2^2)eg.N(μ1​,σ12​)+N(μ2​,σ22​)=N(μ1​+μ2​,σ12​+σ22​) 贝叶斯：P(A∣B)=P(B∣A)P(A)P(B)P(A|B) = \\frac{P(B|A)P(A)}{P(B)}P(A∣B)=P(B)P(B∣A)P(A)​,P(A∣B,C)=P(B∣A,C)P(A∣C)P(B∣C)P(A|B,C) = \\frac{P(B|A,C)P(A|C)}{P(B|C)}P(A∣B,C)=P(B∣C)P(B∣A,C)P(A∣C)​ 开始推理 前向过程 我们定义每次加入的噪声是一个正态分布，满足如下式子： q(xt∣xt−1)=N(xt;αtxt−1,βtI)q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, \\beta_t I) q(xt​∣xt−1​)=N(xt​;αt​​xt−1​,βt​I) 在DDPM（Denoising Diffusion Probabilistic Models）中，这个公式表示从时间步骤 t−1t-1t−1 到时间步骤 ttt 的状态转移概率。具体来说，q(xt∣xt−1)q(x_t | x_{t-1})q(xt​∣xt−1​) 表示给定前一状态 xt−1x_{t-1}xt−1​ 时，状态 xtx_txt​ 的概率分布。 符号 N\\mathcal{N}N 表示高斯（正态）分布，αtxt−1\\sqrt{\\alpha_t} x_{t-1}αt​​xt−1​ 是该高斯分布的均值，而βtI\\beta_t Iβt​I 是分布的协方差，这里 αt\\alpha_tαt​ 和 βt\\beta_tβt​ 是与时间步骤 ttt 相关的系数，III 是单位矩阵，表示协方差矩阵是对角的。 在DDPM中，这个过程通常被用来逐步增加数据的噪声，其中 αt\\alpha_tαt​ 和 βt\\beta_tβt​ 是随时间变化的，通常是减小的，这样随着时间的推移，生成的样本会越来越多地偏离初始样本的分布。简而言之，DDPM的核心是一系列的噪声添加和去噪步骤，该公式描述的是其中噪声添加过程的概率分布。 xt=αtxt−1+βtϵtϵt∼N(0,I)x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{\\beta_t}\\epsilon_t \\quad \\epsilon_t \\sim \\mathcal{N}(0, I) xt​=αt​​xt−1​+βt​​ϵt​ϵt​∼N(0,I) αt=1−βt\\alpha_t = 1-\\beta_t αt​=1−βt​ 我们可以将递归式展开，变为直接O(1)O(1)O(1)计算到任意时间点的正向加噪结果 xt=αtxt−1+βtϵt=αt(αt−1xt−2+βt−1ϵt−1)+βtϵt=⋯=(αt⋯α1)x0+(αt⋯α2)β1ϵ1+(αt⋯α3)β2ϵ2+⋯+αtβt−1ϵt−1+βtϵt⏟N(0,(1−αt2⋯α22)I)\\begin{align*} x_t &amp;= \\alpha_t x_{t-1} + \\beta_t \\epsilon_t \\\\ &amp;= \\alpha_t (\\alpha_{t-1}x_{t-2} + \\beta_{t-1}\\epsilon_{t-1}) + \\beta_t \\epsilon_t \\\\ &amp;= \\cdots \\\\ &amp;= (\\alpha_t \\cdots \\alpha_1)x_0 + \\underbrace{(\\alpha_t \\cdots \\alpha_2)\\beta_1\\epsilon_1 + (\\alpha_t \\cdots \\alpha_3)\\beta_2\\epsilon_2 + \\cdots + \\alpha_t \\beta_{t-1}\\epsilon_{t-1} + \\beta_t \\epsilon_t}_{\\mathcal{N}(0, (1-\\alpha_t^2 \\cdots \\alpha_2^2)I)} \\end{align*} xt​​=αt​xt−1​+βt​ϵt​=αt​(αt−1​xt−2​+βt−1​ϵt−1​)+βt​ϵt​=⋯=(αt​⋯α1​)x0​+N(0,(1−αt2​⋯α22​)I)(αt​⋯α2​)β1​ϵ1​+(αt​⋯α3​)β2​ϵ2​+⋯+αt​βt−1​ϵt−1​+βt​ϵt​​​​ 我们设:α‾t=α1⋯αt\\overline{\\alpha}_t=\\alpha_1\\cdots \\alpha_tαt​=α1​⋯αt​ 化简上式可得： q(xt∣x0)=α‾tx0+1−α‾tϵϵ∼N(0,I)q(x_t|x_0) = \\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\epsilon\\quad\\epsilon\\sim\\mathcal{N}(0,I) q(xt​∣x0​)=αt​​x0​+1−αt​​ϵϵ∼N(0,I) q(xt∣x0)=N(xt;α‾tx0,(1−α‾t)I)q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\overline\\alpha_t} x_0, (1 - \\overline\\alpha_t)I) q(xt​∣x0​)=N(xt​;αt​​x0​,(1−αt​)I) 这里 ϵ\\epsilonϵ 是另一个服从 N(0,1)\\mathcal{N}(0, 1)N(0,1) 的分布，以上便是前向过程涉及到的公式推理 逆向过程 逆向过程又称去噪过程，之前的前向过程是给定x0x_0x0​求任何时候的xtx_txt​即q(xt∣x0)q(x_t|x_0)q(xt​∣x0​)，那么去噪过程所求的分布就是给定任意时刻的分布xtx_txt​求初始分布x0x_0x0​,即p(x0∣xt)p(x_0|x_t)p(x0​∣xt​),通过马尔可夫假设，我们可以对逆向过程进行化简： p(x0∣xt)=p(x0∣x1)p(x1∣x2)⋯p(xt−1∣xt)=∏i=0t−1p(xi∣xi+1)p(x_0 | x_t) = p(x_0 | x_1)p(x_1 | x_2) \\cdots p(x_{t-1} | x_t) = \\prod_{i=0}^{t-1} p(x_i | x_{i+1}) p(x0​∣xt​)=p(x0​∣x1​)p(x1​∣x2​)⋯p(xt−1​∣xt​)=i=0∏t−1​p(xi​∣xi+1​) 那如何求解p(xt−1∣xt)p(x_{t-1}|x_t)p(xt−1​∣xt​)呢，前面的加噪过程我们已经推出了q(xt∣xt−1)q(x_t|x_{t-1})q(xt​∣xt−1​),我们可以通过贝叶斯公式把它们利用起来： p(xt−1∣xt)=p(xt∣xt−1)p(xt−1)p(xt)p(x_{t-1}|x_t) = \\frac{p(x_t|x_{t-1})p(x_{t-1})}{p(x_t)} p(xt−1​∣xt​)=p(xt​)p(xt​∣xt−1​)p(xt−1​)​ 注意：这里的(去噪)p和上面的(加噪)q只是对分布的一种符号记法,它们是等价的. 然后就又有一个新的问题，p(xt−1)p(x_{t-1})p(xt−1​)和p(xt)p(x_t)p(xt​)是未知的，但根据之前的正向过程推到，我们是知道p(xt−1∣x0)p(x_{t-1}|x_0)p(xt−1​∣x0​)以及p(xt∣x0)p(x_t|x_0)p(xt​∣x0​),因此下面的式子我们是可以推出的。 p(xt−1∣xt,x0)=p(xt∣xt−1,x0)p(xt−1∣x0)p(xt∣x0)p(x_{t-1} | x_t, x_0) = \\frac{p(x_t | x_{t-1}, x_0) p(x_{t-1} | x_0)}{p(x_t | x_0)} p(xt−1​∣xt​,x0​)=p(xt​∣x0​)p(xt​∣xt−1​,x0​)p(xt−1​∣x0​)​ 因为我们定义了DDPM是一个markov过程，所以上式中的p(xt∣xt−1,x0)p(x_t|x_{t-1},x_0)p(xt​∣xt−1​,x0​)可以等价于p(xt∣xt−1)p(x_t|x_{t-1})p(xt​∣xt−1​) 。这样上式就可以化简为： p(xt−1∣xt,x0)=p(xt∣xt−1)p(xt−1∣x0)p(xt∣x0)p(x_{t-1}|x_t,x_0) = \\frac{p(x_t|x_{t-1})p(x_{t-1}|x_0)}{p(x_t|x_0)} p(xt−1​∣xt​,x0​)=p(xt​∣x0​)p(xt​∣xt−1​)p(xt−1​∣x0​)​ OK, 然后下面我们来整理一下右侧式子中的每个p的表达式，看看左侧ppp最后是一个什么分布。 首先是p(xt−1∣x0)p(x_{t-1}|x_0)p(xt−1​∣x0​)和p(xt∣x0)p(x_t|x_0)p(xt​∣x0​),他们的表达式我们在正向过程中已经推导过了 p(xt−1∣x0)=αˉt−1x0+1−αˉt−1ϵ∼N(αˉt−1x0,1−αˉt−1)p(x_{t-1}|x_0) = \\sqrt{\\bar\\alpha_{t-1}}x_0+\\sqrt{1-\\bar\\alpha_{t-1}}\\epsilon\\sim\\mathcal{N}(\\sqrt{\\bar{\\alpha}_{t-1}}x_0,1-\\bar\\alpha_{t-1}) p(xt−1​∣x0​)=αˉt−1​​x0​+1−αˉt−1​​ϵ∼N(αˉt−1​​x0​,1−αˉt−1​) p(xt∣x0)=αˉtx0+1−αˉtϵ N(αˉtx0,1−αˉt)p(x_t|x_0) = \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon~\\mathcal{N}(\\sqrt{\\bar\\alpha_t}x_0,1-\\bar\\alpha_t) p(xt​∣x0​)=αˉt​​x0​+1−αˉt​​ϵ N(αˉt​​x0​,1−αˉt​) 然后是p(xt∣xt−1)p(x_t|x_{t-1})p(xt​∣xt−1​)，就是原始的正向递推过程： p(xt∣xt−1)=αtxt−1+1−αtϵ∼N(αtxt−1,1−αt)p(x_t|x_{t-1}) = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon\\sim\\mathcal{N}(\\sqrt\\alpha_tx_{t-1},1-\\alpha_t) p(xt​∣xt−1​)=αt​​xt−1​+1−αt​​ϵ∼N(α​t​xt−1​,1−αt​) 这样我们不难推出： p(xt−1∣xt,x0)∝exp⁡(−12((xt−αtxt−1)2βt)+(xt−1−αˉt−1x0)21−αˉt−1−(xt−αˉtx0)21−αˉt)p(x_{t-1}|x_t, x_0) \\propto \\exp\\left(-\\frac{1}{2}(\\frac{\\left(x_t - \\sqrt{\\alpha_{t}}x_{t-1}\\right)^2}{\\beta_t}) + \\frac{(x_{t-1}-\\sqrt{\\bar\\alpha_{t-1}}x_0)^2}{1-\\bar\\alpha_{t-1}} - \\frac{(x_t - \\sqrt{\\bar\\alpha_t }x_0)^2}{1-\\bar\\alpha_t}\\right) p(xt−1​∣xt​,x0​)∝exp(−21​(βt​(xt​−αt​​xt−1​)2​)+1−αˉt−1​(xt−1​−αˉt−1​​x0​)2​−1−αˉt​(xt​−αˉt​​x0​)2​) 可以发现上式p(xt−1∣xt,x0)p(x_{t-1}|x_t, x_0)p(xt−1​∣xt​,x0​)也是符合正态分布表达式，整理得： p(xt−1∣xt,x0)=N(xt−1;αt(1−αˉt−1)1−αˉtxt+αˉt−1(1−αt)1−αˉtx0,(1−αˉt−11−αˉt(1−αt))I)p(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1}; \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1 - \\bar\\alpha_t}x_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1 - \\alpha_t)}{1 - \\bar\\alpha_t}x_0, \\left(\\frac{ 1 - \\bar\\alpha_{t-1}}{ 1 - \\bar\\alpha_t} ( 1-\\alpha_t )\\right)I\\right) p(xt−1​∣xt​,x0​)=N(xt−1​;1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​αˉt−1​​(1−αt​)​x0​,(1−αˉt​1−αˉt−1​​(1−αt​))I) 上式看着较为复杂，稍微调整一下： p(xt−1∣xt,x0)=N(xt−1;μ,σ2)p(x_{t-1}|x_t, x_0) = \\mathcal{N}\\left(x_{t-1};\\mu,\\sigma^2\\right) p(xt−1​∣xt​,x0​)=N(xt−1​;μ,σ2) μ=αt(1−αˉt−1)1−αˉtxt+αˉt−1(1−αt)1−αˉtx0\\mu = \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1 - \\bar\\alpha_t}x_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1 - \\alpha_t)}{1 - \\bar\\alpha_t}x_0 μ=1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​αˉt−1​​(1−αt​)​x0​ σ2=1−αˉt−11−αˉt(1−αt)\\sigma^2 = \\frac{ 1 - \\bar\\alpha_{t-1}}{ 1 - \\bar\\alpha_t} ( 1-\\alpha_t ) σ2=1−αˉt​1−αˉt−1​​(1−αt​) 我们先整理一下思路，现在我们推出的p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)是真实的条件分布，目标是让模型学到的条件分布pθ(xt−1∣xt)p_{\\theta}(x_{t-1}|x_t)pθ​(xt−1​∣xt​)尽可能的接近真实的条件分布p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)。从上式可以看到方差是个固定量,那么我们要做的就是让 p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​) 与 pθ(xt−1∣xt)p_{\\theta}(x_{t-1}|x_t)pθ​(xt−1​∣xt​) 的均值尽可能的对齐。 但观察均值公式，不难发现其中的x0x_0x0​是未知的，这是我们不希望看到的情况，但是结合我们之前已经推出的： xt=α‾tx0+1−α‾tϵϵ∼N(0,I)x_t= \\sqrt{\\overline{\\alpha}_t}x_0+\\sqrt{1-\\overline{\\alpha}_t}\\epsilon\\quad\\epsilon\\sim\\mathcal{N}(0,I) xt​=αt​​x0​+1−αt​​ϵϵ∼N(0,I) 将x0x_0x0​移至左边,得到关于x0x_0x0​的表达式 x0=1αˉt(xt−1−αˉtϵ)x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}}(x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon) x0​=αˉt​​1​(xt​−1−αˉt​​ϵ) 代入μ\\muμ表达式中可得： μ=αt(1−αˉt−1)1−αˉtxt+αˉt−1(1−αt)1−αˉt1αˉt(xt−1−αˉtϵ)=αt(1−αˉt−1)1−αˉtxt+(1−αt)1−αˉt1αt(xt−1−αˉtϵ)=αt(1−αˉt−1)+(1−αt)αt(1−αˉt)xt−(1−αt)1−αˉtαt(1−αˉt)ϵ=1−αˉtαt(1−αˉt)xt−1−αtαt1−αˉtϵ=1αtxt−1−αtαt1−αˉtϵ\\begin{align*} \\mu &amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\left(x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\right) \\\\ &amp;= \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} x_t + \\frac{(1 - \\alpha_t)}{1 - \\bar{\\alpha}_t} \\frac{1}{\\sqrt{\\alpha_t}} \\left(x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon \\right) \\\\ &amp;= \\frac{\\alpha_t(1 - \\bar{\\alpha}_{t-1}) + (1 - \\alpha_t)}{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_t)} x_t - \\frac{(1 - \\alpha_t) \\sqrt{1 - \\bar{\\alpha}_t}}{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_t)} \\epsilon \\\\ &amp;= \\frac{1 - \\bar{\\alpha}_t}{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_t)} x_t - \\frac{1 - \\alpha_t }{\\sqrt{\\alpha_t}\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\\\ &amp;= \\frac{1}{\\sqrt{\\alpha_t}} x_t - \\frac{1 - \\alpha_t}{\\sqrt{\\alpha_t} \\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\end{align*} μ​=1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​αˉt−1​​(1−αt​)​αˉt​​1​(xt​−1−αˉt​​ϵ)=1−αˉt​αt​​(1−αˉt−1​)​xt​+1−αˉt​(1−αt​)​αt​​1​(xt​−1−αˉt​​ϵ)=αt​​(1−αˉt​)αt​(1−αˉt−1​)+(1−αt​)​xt​−αt​​(1−αˉt​)(1−αt​)1−αˉt​​​ϵ=αt​​(1−αˉt​)1−αˉt​​xt​−αt​​1−αˉt​​1−αt​​ϵ=αt​​1​xt​−αt​​1−αˉt​​1−αt​​ϵ​ 经过上述化简，我们成功将μ(x0,xt)⇒μ(xt,ϵ)\\mu(x_0,x_t)\\Rightarrow \\mu(x_t,\\epsilon)μ(x0​,xt​)⇒μ(xt​,ϵ) 此时，式子中未知的部分只剩下ϵ\\epsilonϵ,这样对齐均值的问题转化为了已知xt,tx_t,txt​,t使用神经网络( ϵθ(xt,t)\\epsilon_\\theta(x_t,t)ϵθ​(xt​,t) )预测加入的噪声ϵ\\epsilonϵ问题，从而我们也知道了优化目标——最小化∣∣ϵ−ϵθ(xt,t)∣∣2||\\epsilon-\\epsilon_\\theta(x_t,t)||^2∣∣ϵ−ϵθ​(xt​,t)∣∣2 μ≃μθ(xt,t)=1αtxt−1−αtαt1−αˉtϵθ(xt,t)\\mu \\simeq \\mu_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{\\alpha_t} \\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_\\theta(\\mathbf{x}_t, t) μ≃μθ​(xt​,t)=αt​​1​xt​−αt​​1−αˉt​​1−αt​​ϵθ​(xt​,t) 以上便是DDPM整个流程的公式推导。 代码部分 详细代码可以看我fork的仓库Mudrobot/Diffusion_models_tutorial (github.com)中的Diffusers_library.ipynb文件进行理解，这里主要展示和讲解一下两个重要的过程以及训练和推理部分。 正向过程 12345678def q_sample(self, x_start, t, noise=None):\tif noise is None: noise = torch.randn_like(x_start) # 噪声采样\tsqrt_alphas_cumprod_t = self._extract(self.sqrt_alphas_cumprod, t, x_start.shape)\tsqrt_one_minus_alphas_cumprod_t = self._extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\treturn sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise 最后一个return就是xt=αtxt−1+βtϵtϵt∼N(0,I)x_t = \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{\\beta_t}\\epsilon_t \\quad \\epsilon_t \\sim \\mathcal{N}(0, I)xt​=αt​​xt−1​+βt​​ϵt​ϵt​∼N(0,I) 可以使用下面代码可视化一下输出结果： 12345678for idx, t in enumerate([0, 50, 100, 200, 499]): x_noisy = gaussian_diffusion.q_sample(x_start, t=torch.tensor([t])) noisy_image = (x_noisy.squeeze().permute(1, 2, 0) + 1) * 127.5 noisy_image = noisy_image.numpy().astype(np.uint8) plt.subplot(1, 5, 1 + idx) plt.imshow(noisy_image) plt.axis(&quot;off&quot;) plt.title(f&quot;t=&#123;t&#125;&quot;) 训练目标 训练目标如之前最后所说，就是使用神经网络(第7行的model)去预测之前公式中对图像添加的噪声ϵ\\epsilonϵ,然后目标函数就是最小化模型预测的ϵθ(xt,t)\\epsilon_\\theta(x_t,t)ϵθ​(xt​,t)和ϵ\\epsilonϵ的均方误差。 代码如下： 123456789# compute train lossesdef train_losses(self, model, x_start, t):\t# generate random noise\tnoise = torch.randn_like(x_start)\t# get x_t\tx_noisy = self.q_sample(x_start, t, noise=noise)\tpredicted_noise = model(x_noisy, t)\tloss = F.mse_loss(noise, predicted_noise)\treturn loss 训练部分 理解到逆向过程后，接着我们来看一下训练过程，其实就是对于每个batch中的所有图像都需要随机采样一个时间点，用于计算要加的噪声量。 1234567891011121314151617181920# trainepochs = 10for epoch in range(epochs): for step, (images, labels) in enumerate(train_loader): optimizer.zero_grad() batch_size = images.shape[0] images = images.to(device) # sample t uniformally for every example in the batch t = torch.randint(0, timesteps, (batch_size,), device=device).long() loss = gaussian_diffusion.train_losses(model, images, t) if step % 200 == 0: print(&quot;Loss:&quot;, loss.item()) loss.backward() optimizer.step() 逆向过程 训练的时候模型学习的是ϵ\\epsilonϵ, 但推理的时候模型需要根据当前的时间步对预测的ϵ\\epsilonϵ做变换得到当前步的预测噪声ϵt\\epsilon_tϵt​的μ\\muμ，从而采样当前步的降噪噪声ϵt∈N(μ,σ)\\epsilon_t\\in\\mathcal{N}(\\mu,\\sigma)ϵt​∈N(μ,σ) (σ\\sigmaσ已知) 预测当前时间步噪声均值和方差的代码如下所示： 1234567891011121314151617181920# Compute the mean and variance of the diffusion posterior: q(x_&#123;t-1&#125; | x_t, x_0)def q_posterior_mean_variance(self, x_start, x_t, t):\tposterior_mean = ( self._extract(self.posterior_mean_coef1, t, x_t.shape) * x_start + self._extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\t)\tposterior_variance = self._extract(self.posterior_variance, t, x_t.shape)\tposterior_log_variance_clipped = self._extract(self.posterior_log_variance_clipped, t, x_t.shape)\treturn posterior_mean, posterior_variance, posterior_log_variance_clipped# compute predicted mean and variance of p(x_&#123;t-1&#125; | x_t)def p_mean_variance(self, model, x_t, t, clip_denoised=True):\t# predict noise using model\tpred_noise = model(x_t, t)\t# get the predicted x_0: different from the algorithm2 in the paper\tx_recon = self.predict_start_from_noise(x_t, t, pred_noise)\tif clip_denoised: x_recon = torch.clamp(x_recon, min=-1., max=1.)\tmodel_mean, posterior_variance, posterior_log_variance = \\ self.q_posterior_mean_variance(x_recon, x_t, t)\treturn model_mean, posterior_variance, posterior_log_variance 采样并消除噪声代码如下所示： 第10行中，对数方差乘0.5取指数的含义是，将对数方差转化为方差后开根号得标准差。 公式是：σ=σ2=elog⁡(σ2)=e0.5⋅log⁡(σ2)\\sigma = \\sqrt{\\sigma^2} = \\sqrt{e^{\\log(\\sigma^2)}} = e^{0.5 \\cdot \\log(\\sigma^2)}σ=σ2​=elog(σ2)​=e0.5⋅log(σ2) 1234567891011@torch.no_grad()def p_sample(self, model, x_t, t, clip_denoised=True):\t# predict mean and variance\tmodel_mean, _, model_log_variance = self.p_mean_variance(model, x_t, t, clip_denoised=clip_denoised)\tnoise = torch.randn_like(x_t)\t# no noise when t == 0\tnonzero_mask = ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))\t# compute x_&#123;t-1&#125;\tpred_img = model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\treturn pred_img 推理部分 理解了上面的逆向过程后，推理部分的生成就非常好理解了。就是不断的调用p_sample函数逐渐从一个全噪声生成图像的过程(主函数先调用sample)。 1234567891011121314151617# denoise: reverse diffusion@torch.no_grad()def p_sample_loop(self, model, shape):\tbatch_size = shape[0]\tdevice = next(model.parameters()).device\t# start from pure noise (for each example in the batch)\timg = torch.randn(shape, device=device)\timgs = []\tfor i in tqdm(reversed(range(0, timesteps)), desc=&#x27;sampling loop time step&#x27;, total=timesteps): img = self.p_sample(model, img, torch.full((batch_size,), i, device=device, dtype=torch.long)) imgs.append(img.cpu().numpy())\treturn imgs# sample new images@torch.no_grad()def sample(self, model, image_size, batch_size=8, channels=3):\treturn self.p_sample_loop(model, shape=(batch_size, channels, image_size, image_size)) 如果要控制diffusion生成特定类别的图像，会使用classifier free guidance方法，后续会讲解。 该ipynb文件最后对于MNIST手写数据集的生成效果如下：","tags":["Diffusion","IMPORTANT","DDPM","NeurIPS","2020"],"categories":["Diffusion"]},{"title":"DDIM 原理推导","path":"/2024/09/26/MudSynth/Diffusion/DDIM 原理推导/","content":"学习整理自：diffusion model(二)：DDIM技术小结 (denoising diffusion implicit model) | 莫叶何竹🍀 (myhz0606.com)，欢迎阅读原文 文章基本信息 文章名称：Denoising Diffusion Implicit Models 发表会议/年份：ICLR 2021 作者：Jiaming Song, Chenlin Meng &amp; Stefano Ermon 单位：Stanford University 背景 尽管去噪扩散概率模型（DDPM）无需对抗训练即可实现高质量图像生成，但其采样过程依赖于马尔可夫假设，需要较多的时间步才能得到较好的生成效果。本文介绍的去噪扩散隐式模型（DDIM）是一种更有效的迭代隐式概率模型，训练过程与DDPM相同，但采样过程比DDPM快10到50倍。 DDPM为何慢 从DDPM中我们知道，其扩散过程（前向过程，或加噪过程，forward process）被定义为一个马尔可夫过程： q(x1:T∣x0):=∏t=1Tq(xt∣xt−1),q(x_{1:T}|x_0) := \\prod_{t=1}^{T} q(x_t|x_{t-1}), q(x1:T​∣x0​):=t=1∏T​q(xt​∣xt−1​), 其中： q(xt∣xt−1)=N(xt;αtxt−1,(1−αt)I)q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1-\\alpha_t)\\mathbf{I}) q(xt​∣xt−1​)=N(xt​;αt​​xt−1​,(1−αt​)I) 通过这样设置，前向过程有一个很好的性质，可以通过 x0x_0x0​ 得到任意时刻 xtx_txt​ 的分布，而无需繁琐的链式计算： q(xt∣x0):=∫q(x1:t∣x0)dx1:(t−1)=N(xt;αtˉx0,(1−αtˉ)I)(1)q(x_t|x_0) := \\int q(x_{1:t}|x_0) dx_{1:(t-1)} = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha_t}}x_0, (1-\\bar{\\alpha_t})\\mathbf{I})\\tag{1} q(xt​∣x0​):=∫q(x1:t​∣x0​)dx1:(t−1)​=N(xt​;αt​ˉ​​x0​,(1−αt​ˉ​)I)(1) 其去噪过程（也有叫逆向过程，reverse process）也是一个马尔可夫过程： pθ(x0:T)=p(xT)∏t=1Tpθ(xt−1∣xt),p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^{T} p_\\theta(x_{t-1}|x_t), pθ​(x0:T​)=p(xT​)t=1∏T​pθ​(xt−1​∣xt​), 其中： p(xT):=N(0,I),p(x_T) := \\mathcal{N}(0, \\mathbf{I}), p(xT​):=N(0,I), 并且： pθ(xt−1∣xt)=N(xt−1;μθ(xt,t),σtI)p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t\\mathbf{I}) pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),σt​I) 从式 (1) 可以看出，当 ttt 足够大时， q(xt∣x0)q(x_t|x_0)q(xt​∣x0​) 对所有 x0x_0x0​ 都收敛于标准高斯分布。因此DDPM在去噪过程中定义： pθ(xT):=N(0,I)p_\\theta(x_T) := \\mathcal{N}(0, \\mathbf{I}) pθ​(xT​):=N(0,I) 并且采用一个较大的采样时间步数 TTT。在对 pθ(xt−1∣xt)p_\\theta(x_{t-1}|x_t)pθ​(xt−1​∣xt​) 的推导中，DDPM用到了一阶马尔可夫假设，使得 p(xt∣xt−1,x0)=p(xt∣xt−1)p(x_t|x_{t-1}, x_0) = p(x_t|x_{t-1})p(xt​∣xt−1​,x0​)=p(xt​∣xt−1​)，因此重建的步长非常长，导致速度慢。 DDIM推理 DDPM速度慢的本质原因是对马尔可夫假设的依赖，导致重建需要较多的步长。那么不用一阶马尔可夫假设，有没有另一种方法推导出采样分布p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)呢？ DDIM所提出的初衷就是： 维持DDPM前向推理过程中的马尔可夫假设（可以直接使用DDPM中所训练的噪声预测模型） 改变DDPM反向推理中的马尔可夫假设让采样分布的推导不依赖马尔可夫假设（这样不需要一步一步推回去） DDIM采样分布求解 回到我们的目标,如何推出式子左边： p(xt−1∣xt,x0)=p(xt∣xt−1,x0)⋅p(xt−1∣x0)p(xt∣x0)p(x_{t-1} | x_t, x_0) = \\frac{p(x_t | x_{t-1}, x_0) \\cdot p(x_{t-1} | x_0)}{p(x_t | x_0)} p(xt−1​∣xt​,x0​)=p(xt​∣x0​)p(xt​∣xt−1​,x0​)⋅p(xt−1​∣x0​)​ 但是这里右边式子中p(xt∣xt−1,x0)p(x_t|x_{t-1},x_0)p(xt​∣xt−1​,x0​)我们是不知道的，为了求解式子左边，在DDPM中我们是根据一阶马尔可夫假设假设了p(xt∣xt−1,x0)=p(xt∣xt−1)p(x_t|x_{t-1},x_0)=p(x_t|x_{t-1})p(xt​∣xt−1​,x0​)=p(xt​∣xt−1​)。从而推出左边p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)为正态分布，然后得出答案。 根据DDPM的结果参考，采样分布p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)是一个高斯分布，且均值是x0,xtx_0,x_tx0​,xt​的线性函数。 在DDIM中，为了不依赖p(xt∣xt−1,x0)p(x_t|x_{t-1},x_0)p(xt​∣xt−1​,x0​)（马尔可夫假设），作者做出了更为大胆的假设，作者假设p(xt−1∣xt,x0)p(x_{t-1}|x_t,x_0)p(xt−1​∣xt​,x0​)为任意正态分布，只需要满足下述等式即可： p(xt−1∣xt,x0)=N(xt−1;λx0+kxt,σt2I)p(x_{t-1}|x_t,x_0)=\\mathcal{N}(x_{t-1};\\lambda x_0+kx_t,\\sigma^2_tI) p(xt−1​∣xt​,x0​)=N(xt−1​;λx0​+kxt​,σt2​I) 该采样分布有3个自由变量λ,k,σt\\lambda,k,\\sigma_tλ,k,σt​，但是DDIM要维持与DDPM一致的正向推理分布：q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t|x_0)=\\mathcal{N}(x_t;\\sqrt{\\bar\\alpha_t}x_0,(1-\\bar{\\alpha}_t)I)q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I) 。 根据数学归纳法，只需要保证q(xt−1∣x0)=N(xt−1;αˉt−1x0,(1−αˉt−1)I)q(x_{t-1}|x_0) = \\mathcal{N}(x_{t-1};\\sqrt{\\bar\\alpha_{t-1}}x_0,(1-\\bar{\\alpha}_{t-1})I)q(xt−1​∣x0​)=N(xt−1​;αˉt−1​​x0​,(1−αˉt−1​)I). 所以最终问题变为了，已知q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t|x_0)=\\mathcal{N}(x_t;\\sqrt{\\bar\\alpha_t}x_0,(1-\\bar{\\alpha}_t)I)q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I)，请寻找p(xt−1∣xt,x0)=N(xt−1;λx0+kxt,σt2I)p(x_{t-1}|x_t,x_0)=\\mathcal{N}(x_{t-1};\\lambda x_0+kx_t,\\sigma^2_tI)p(xt−1​∣xt​,x0​)=N(xt−1​;λx0​+kxt​,σt2​I)的一组解λ∗,k∗,σt∗\\lambda^*,k^*,\\sigma_t^*λ∗,k∗,σt∗​，使得q(xt−1∣x0)=N(xt−1;αˉt−1x0,(1−αˉt−1)I)q(x_{t-1}|x_0) = \\mathcal{N}(x_{t-1};\\sqrt{\\bar\\alpha_{t-1}}x_0,(1-\\bar{\\alpha}_{t-1})I)q(xt−1​∣x0​)=N(xt−1​;αˉt−1​​x0​,(1−αˉt−1​)I)。 即使得下式成立： ∫xtp(xt−1∣xt,x0)q(xt∣x0)dxt=q(xt−1∣x0)\\int_{x_t} p(x_{t-1} | x_t, x_0) q(x_t | x_0) dx_t = q(x_{t-1} | x_0) ∫xt​​p(xt−1​∣xt​,x0​)q(xt​∣x0​)dxt​=q(xt−1​∣x0​) 可以使用待定系数法进行求解 首先对xt−1x_{t-1}xt−1​和ϵt−1′\\epsilon&#x27;_{t-1}ϵt−1′​进行采样得：xt−1=λx0+kxt+σtϵt−1′ϵt−1′∼N(0,I)x_{t-1} = \\lambda x_0+kx_t+\\sigma_t\\epsilon&#x27;_{t-1}\\quad\\epsilon&#x27;_{t-1}\\sim\\mathcal{N}(0,I)xt−1​=λx0​+kxt​+σt​ϵt−1′​ϵt−1′​∼N(0,I) 根据q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar\\alpha_t}x_0, (1 - \\bar\\alpha_t)I)q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I),可采样xt,ϵt′∼N(0,I)x_t,\\epsilon_t&#x27;\\sim\\mathcal{N}(0,I)xt​,ϵt′​∼N(0,I): xt=αˉtx0+1−αˉtϵt′x_t = \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1 - \\bar\\alpha_t} \\epsilon_t&#x27; xt​=αˉt​​x0​+1−αˉt​​ϵt′​ 我们直接带入： xt−1=λx0+k(αˉtx0+1−αˉtϵt′)+σtϵt−1′x_{t-1} = \\lambda x_0 + k\\left(\\sqrt{\\bar{\\alpha}_{t}}x_0 + \\sqrt{1-\\bar\\alpha_t }\\epsilon_t&#x27;\\right) + \\sigma_t \\epsilon_{t-1}&#x27; xt−1​=λx0​+k(αˉt​​x0​+1−αˉt​​ϵt′​)+σt​ϵt−1′​ 合并同类项得到： xt−1=(λ+kαˉt)x0+(k2(1−αˉt)+σt2)ϵˉt−1ϵˉt−1∼N(0,I)x_{t-1} = (\\lambda + k\\sqrt{\\bar{\\alpha}_t})x_0 +\\sqrt{(k^2(1-\\bar\\alpha_t)+\\sigma_t^2)} \\bar\\epsilon_{t-1} \\quad \\quad \\bar\\epsilon_{t-1} \\sim \\mathcal{N}(0, I) xt−1​=(λ+kαˉt​​)x0​+(k2(1−αˉt​)+σt2​)​ϵˉt−1​ϵˉt−1​∼N(0,I) 根据q(xt∣x0)=N(xt;αˉtx0,(1−αˉt)I)q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar\\alpha_t}x_0, (1 - \\bar\\alpha_t)I)q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I),可采样xt−1,ϵt−1∼N(0,I)x_{t-1},\\epsilon_{t-1}\\sim\\mathcal{N}(0,I)xt−1​,ϵt−1​∼N(0,I): xt−1=αˉt−1x0+1−αˉt−1ϵt−1x_{t-1} = \\sqrt{\\bar\\alpha_{t-1}} x_0 + \\sqrt{1 - \\bar\\alpha_{t-1}} \\epsilon_{t-1} xt−1​=αˉt−1​​x0​+1−αˉt−1​​ϵt−1​ 结合上面两个式子，不难得到： {λ+kαˉt=αˉt−1k2(1−αˉt)+σt2=1−αˉt−1\\begin{cases} \\lambda + k\\sqrt{\\bar{\\alpha}_t} = \\sqrt{\\bar{\\alpha}_{t-1}} \\\\ k^2(1 - \\bar{\\alpha}_t) + \\sigma_t^2 = 1 - \\bar{\\alpha}_{t-1} \\end{cases} {λ+kαˉt​​=αˉt−1​​k2(1−αˉt​)+σt2​=1−αˉt−1​​ 然后解得： k∗=1−αˉt−1−σt21−αˉtk^* = \\frac{\\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}}{\\sqrt{1 - \\bar{\\alpha}_t}} k∗=1−αˉt​​1−αˉt−1​−σt2​​​ λ∗=αˉt−1−1−αˉt−1−σt2αˉt1−αˉt\\lambda^* = \\sqrt{\\bar{\\alpha}_{t-1}} - \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\frac{\\sqrt{\\bar{\\alpha}_t}}{\\sqrt{1 - \\bar{\\alpha}_t}} λ∗=αˉt−1​​−1−αˉt−1​−σt2​​1−αˉt​​αˉt​​​ σt∗=σ\\sigma_t^* = \\sigma σt∗​=σ 综上将上面三个参数k∗,λ∗,σt∗k^*,\\lambda^*,\\sigma_t^*k∗,λ∗,σt∗​带入我们可以得到： p(xt−1∣xt,x0)=N(xt−1;αˉt−1x0+1−αˉt−1−σt2xt−αˉtx01−αˉt,σt2I)p(x_{t-1}|x_t,x_0) = \\mathcal{N}\\left(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2}\\frac{ x_t- \\sqrt{\\bar{\\alpha}_t}x_0}{\\sqrt{1 - \\bar{\\alpha}_t}}, \\sigma_t^2I\\right) p(xt−1​∣xt​,x0​)=N(xt−1​;αˉt−1​​x0​+1−αˉt−1​−σt2​​1−αˉt​​xt​−αˉt​​x0​​,σt2​I) 综上，说明可以找到一组解满足题述条件。其中，不同的σt\\sigma_tσt​对应不同的生成过程。由于前向过程没变，故可以直接用DDPM训练的噪声预测模型。采样过程如下： xt−1=αˉt−1x0+1−αˉt−1−σt2(xt−αˉtx01−αˉt)+σtϵx_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\left( \\frac{x_t - \\sqrt{\\bar{\\alpha}_t} x_0}{\\sqrt{1 - \\bar{\\alpha}_t}} \\right) + \\sigma_t \\epsilon xt−1​=αˉt−1​​x0​+1−αˉt−1​−σt2​​(1−αˉt​​xt​−αˉt​​x0​​)+σt​ϵ 然后同DDPM，我们将x0x_0x0​用xtx_txt​和ϵθ\\epsilon_\\thetaϵθ​替代。 x0=1αˉt(xt−1−αˉtϵθ(xt,t))x_0 = \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left( x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t) \\right) x0​=αˉt​​1​(xt​−1−αˉt​​ϵθ​(xt​,t)) 最终化简得: xt−1=αˉt−1xt−1−αˉtϵθ(xt,t)αˉt+1−αˉt−1−σt2ϵθ(xt,t)+σtϵx_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\frac{ x_t - \\sqrt{1 - \\bar\\alpha_t} \\epsilon_{\\theta}(x_t, t)}{\\sqrt{\\bar\\alpha_t}} + \\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2 }\\epsilon_{\\theta}(x_t, t) + \\sigma_t \\epsilon xt−1​=αˉt−1​​αˉt​​xt​−1−αˉt​​ϵθ​(xt​,t)​+1−αˉt−1​−σt2​​ϵθ​(xt​,t)+σt​ϵ 其中只有σt\\sigma_tσt​是未知的，需要注意以下两个σt\\sigma_tσt​的特殊取值： 当σt=(1−αˉt−1)/(1−αˉt)1−αˉt/αˉt−1\\sigma_t=\\sqrt{(1-\\bar\\alpha_{t-1})/(1-\\bar\\alpha_t)}\\sqrt{1-\\bar\\alpha_t/\\bar\\alpha_{t-1}}σt​=(1−αˉt−1​)/(1−αˉt​)​1−αˉt​/αˉt−1​​,此时的生成过程与DDPM一致 当σt\\sigma_tσt​为0时，此时采样过程中添加的随机噪声项为0，采样过程是确定的，就是作者所提出的DDIM，此时的递推公式为： xt−1=αˉt−1xt−1−αˉtϵθ(xt,t)αˉt+1−αˉt−1ϵθ(xt,t)x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}\\frac{ x_t - \\sqrt{1 - \\bar\\alpha_t} \\epsilon_{\\theta}(x_t, t)}{\\sqrt{\\bar\\alpha_t}} + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\epsilon_{\\theta}(x_t, t) xt−1​=αˉt−1​​αˉt​​xt​−1−αˉt​​ϵθ​(xt​,t)​+1−αˉt−1​​ϵθ​(xt​,t) DDIM如何加速采样 上面我们已经推导了DDIM如何从状态ttt推导到状态t−1t-1t−1，但是由于DDIM的反向过程没有受到马尔可夫的限制，因此他其实是可以从状态t直接推到前面任意一个状态的，从状态ttt推导到前面的状态t−m(m&lt;t)t-m(m &lt; t)t−m(m&lt;t)可以表示为如下式子： xt−m=αˉt−mxt−1−αˉtϵθ(xt,t)αˉt+1−αˉt−mϵθ(xt,t)x_{t-m} = \\sqrt{\\bar{\\alpha}_{t-m}} \\frac{x_{t} -\\sqrt{1 - \\bar\\alpha_{t}} \\epsilon_{\\theta}(x_{t}, t)}{\\sqrt{\\bar\\alpha_{t}}} + \\sqrt{1 - \\bar{\\alpha}_{t-m}} \\epsilon_{\\theta}(x_{t}, t) xt−m​=αˉt−m​​αˉt​​xt​−1−αˉt​​ϵθ​(xt​,t)​+1−αˉt−m​​ϵθ​(xt​,t) 一般加速就是将原先是一步一步预测变成nnn步nnn步进行预测。 论文中也展示了使用不同的n所带来的结果，可以看到DDIM在较小采样步长时就能达到较好的生成效果。如CIFAR10 S=50就达到了S=1000的90%的效果，与之相对DDPM只能达到10%左右的FID效果。 其中，dim(L)dim(L)dim(L)表示的是采样序列的长度 DDIM区别于DDPM两个重要的特性 采样一致性 我们知道DDIM将σt\\sigma_tσt​​设置为0，这让采样过程是确定的，只受​xTx_TxT​影响。作者发现，当给定xTx_TxT​​，不同的的采样时间序列τ\\tauτ所生成图片都很相近，xTx_TxT​​似乎可以视作生成图片的隐编码信息。 有个小trick，我们在实际的生成中可以先设置较小的采样步长(迭代次数)进行生成，若生成的图片是我们想要的，则用较大的步长重新生成高质量的图片。 语义插值效应(sementic interpolation effect) 即然xTx_TxT​​可能是生成图片的隐空间编码，那么它是否具备其它隐概率模型(如GAN2，VAE)所观察到的语义插值效应呢? 首先从高斯分布采样两个随机变量xT(0),xT(1)x_T^{(0)},x_T^{(1)}xT(0)​,xT(1)​​，并用他们做图像生成得到下图最左侧与最右侧的结果。随后用球面线性插值方法（spherical linear interpolation，Slerp）对xT(1),xT(2)x_T^{(1)},x_T^{(2)}xT(1)​,xT(2)​他们进行插值，得到一系列中间结果: xT(α)=sin⁡((1−α)θ)sin⁡(θ)xT(0)+sin⁡(αθ)sin⁡(θ)xT(1)x_{T}^{(\\alpha)} = \\frac{\\sin((1 - \\alpha) \\theta)}{\\sin(\\theta)} x_{T}^{(0)} + \\frac{\\sin(\\alpha \\theta)}{\\sin(\\theta)} x_{T}^{(1)} xT(α)​=sin(θ)sin((1−α)θ)​xT(0)​+sin(θ)sin(αθ)​xT(1)​ 其中θ=arccos⁡((xT(0))TxT(1)∥xT(0)∥∥xT(1)∥)\\theta = \\arccos \\left( \\frac{(x_{T}^{(0)})^T x_{T}^{(1)}}{\\| x_{T}^{(0)} \\| \\| x_{T}^{(1)} \\|} \\right)θ=arccos(∥xT(0)​∥∥xT(1)​∥(xT(0)​)TxT(1)​​) ,结果如下所示，可以明确看出来还是有一定语义插值效应的。","tags":["Diffusion","DDIM","ICLR","2021","IMPORTANT"],"categories":["Diffusion"]},{"title":"Hello World","path":"/2024/09/26/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment a=aba = \\frac{a}{b} a=ba​ w(n)=0.54−0.46cos⁡(2πnN),0≤n≤Nw(n)=0.54-0.46\\cos(2\\pi\\frac{n}{N}),\\quad 0\\le n\\le N w(n)=0.54−0.46cos(2πNn​),0≤n≤N 试试行内a=abca=\\frac{a}{\\frac{b}{c}}a=cb​a​ 这样行吗a=asa=\\frac{a}{s}a=sa​"},{"title":"Git入门教程","path":"/wiki/Git/Git入门教程.html","content":"分布式版本控制 每个人都拥有全部的代码！不会因为服务器损坏或者网络问题，造成不能工作的情况。 所有版本信息仓库全部同步到本地的每个用户，这样就可以在本地查看所有版本历史，可以离线在本地提交，只需在联网时push到相应的服务器或其他用户那里。由于每个用户那里保存的都是所有的版本数据，只要有一个用户的设备没有问题就可以恢复所有的数据，但这增加了本地存储空间的占用。 Git时目前世界上最先进的分布式版本控制系统。 因为Git Bash我们在日常中是使用最多的，而Git Bash基础命令风格是基于Linux系统的，所以这里我们先介绍一些基本的Linux命令 基本Linux命令 cd：改变目录 cd ..：回到上一级目录 pwd：显示当前所在的目录的路径 ls：列出当前目录中的所有文件 ll：同上也是列出当前目录中的所有文件，但是相比于ls列出的内容会更为详细。 touch：新建一个文件。eg：touch index.js就会在当前目录下新建一个index.js的文件。 rm：删除一个文件。eg：rm index.js就会在将当前目录下的index.js文件删除。 mkdir：新建一个目录，就是新建一个文件夹。 rm-r：删除一个文件夹，eg：rm-r src删除src目录 `rm-rf /` 切勿在Linux电脑中尝试此命令，f是递归删除的意思，这里的含义就是将根目录下的所有文件夹全部删掉。 mv移动文件，eg:mv index.html src 命令前面的index.html文件是我们要移动的文件。src是目标文件夹，文件和目标文件夹必须在同一目录下。 reset：重新初始化终端/清屏。 clear：清屏。 history：查看命令历史。 help：帮助 exit：退出 #：表示注释 Git的必要配置 查看不同级别的配置文件 12345# 查看系统config$ git config --system --list# 查看当前用户(global)配置$ git config --global --list 设置用户名和邮箱 12$ git config --global user.name &quot;xxxxx&quot; #用户名$ git config --global user.email 2604932485@qq.com #邮箱 查看用户名和邮箱 Git基本理论 Git本地有三个工作区域：工作目录（Working Directory）、暂存区（Stage/Index）、资源库（Repository或Git Directory）。如果在加上远程的git仓库（Remote Directory）就可以分为四个工作区域，文件在这四个区域之间的转换关系如下： 基本工作流程 在工作目录中添加、修改文件 将需要进行版本管理的文件放入暂存区（git add .） 将暂存区域的文件提交到git仓库（git commit） 将本地仓库推送到远程git仓库（git push） 因此，git管理的文件有三种状态：已修改（modified），已暂存（staged），已提交（committed） Git项目搭建 本地搭建仓库 远程克隆仓库 本地搭建仓库 创建一个写的仓库，我们先在git终端中使用cd命令进入我们想要创建仓库的目录，然后我们使用如下命令将一个当前目录（正常的的文件夹）初始化成git仓库 12# 将当前目录文件夹初始化为一个git仓库$ git init 执行后可以看到，在当前目录中多出了一个.git目录，关于版本等的所有信息都在这个目录里面。 克隆远程仓库到本地 使用git clone命令将远程服务器上的仓库完全镜像一份到本地 12# 克隆一个项目和他的整个代码历史（版本信息）$ git clone [url] Git文件操作 版本控制就是对文件的版本控制，要对文件进行修改、提交等操作，首先要知道文件当前在什么状态，不然可能会提交了现在还不想提交的文件，或者要提交的文件没有提交上。 文件的4种状态 Untracked：未跟踪，此文件在文件夹中，但并没有加入到git库中，不参与版本控制。通过git add状态变为Staged Unmodify：文件已经入库(history状态)，未修改，即版本库中的文件快照内容与文件夹中完全一致，这种类型的文件有两种去处，如果它被修改，而变为Modified，如果使用git rm移出版本库，则成为Untracked文件 Modified：文件已修改，仅仅是修改，并没有进行其他的操作，这个文件也有两个去处，通过git add可进入Staged状态，使用git checkout则丢弃修改过，返回到unmodify状态，这个git checkout即从库中取出文件，覆盖当前修改！ Staged：暂存状态，执行git commit则将修改同步到库中，这时库中的文件和本地文件又变为一致，文件为Unmodify状态。执行git reset HEAD filename取消暂存，文件状态变为Modified 查看文件的状态 1234567# 查看指定文件状态$ git status [filename]# 查看所有文件的状态$ git status 上传提交文件 1234# 添加所有文件到暂存区$ git add .# 提交暂存区中的内容$ git commit -m &quot;xxx&quot; # -m 后跟着的是提交信息 回滚 12345# 对于尚未提交的处于暂存区中的文件，如果要撤销修改$ git checkout [filename]# 对于已经提交的一次commit，如果要撤销这次提交$ git reset HEAD^1 下面我们举一个完整的案例，就是将一个空的文件夹初始化成一个git仓库，然后新建一个test.txt文件并将其加入到仓库中的整个过程。 我们采用本地初始化仓库的方式，首先在电脑桌面上新建文件夹ex1，然后右键该文件夹进行git bash 可以发现git init后该文件夹中多出来一个隐藏文件夹，然后我们在该文件夹下新建一个test.txt文件。 然后进入git终端，clear后执行下面的命令，结果如下图 现在我们的文件成功进入了暂存区，下面进行提交操作，结果如下 可以发现最后查看状态的时候已经显示working tree clean。所以证明提交成功！ 忽略文件——.gitgnore 有些时候我们不想把某些文件纳入版本控制中，比如数据库文件，临时文件，设计文件等 这时我们可以在主目录下建立&quot;.gitgnore&quot;文件，此文件有如下规则 文件中的空行或以＃开始的行都会被忽略。 可以使用Linux通配符。 星号（*）代表任意多个字符，问号（?）代表一个字符，方括号（[abc]）代表可选字符范围，大括号（{string1,string2,…}）代表可选的字符串等。 如果名称的最前面有一个感叹号（!），表示例外规则，将不被忽略。 如果名称的最前面是一个路径分隔符（/），表示要忽略的文件在此目录下，而子目录中的文件不忽略。 如果名称的最后面是一个路径分隔符（/），表示要忽略的是此目录下该名称的子目录，而非文件（默认文件或目录都忽略） 123456# 为注释*.txt # 忽略所有.txt结尾的文件!lib.txt # 但是lib.txt除外/temp # 仅忽略项目根目录下的temp文件夹，不包括其他目录tempbuild/ # 忽略build目录下的所有文件doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 创建远程仓库 以GitHub为例，其实非常的简单，只需要跟着GitHub的步骤一步一步进行设置就可以成功创建一个仓库了。 填写好仓库名以及仓库描述后，点击创建仓库。GitHub会提供下面的代码，让你将本地的仓库关联到远端这个新创建的仓库。（关联已有仓库代码用最下面三行代码） 最常用的两个命令： 1234# 推送当前分支最新的提交到远程$ git push# 拉取远程分支最新的提交到本地$ git pull Git分支 Git分支中常用命令 123456789101112131415161718192021222324252627# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支(已当前分支为基础)$ git checkout -b [branch]# 单纯地切换到某个分支$ git checkout [branch-name]# 合并指定分支到当前分支$ git merge [branch]# 放弃这次合并$ git merge --abort# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 多个分支如果并行执行，就会导致我们的代码不冲突，也就是同时存在多个版本！ 如果同一个文件在合并分支时都被修改了，则会引起冲突：解决的办法是我们可以修改冲突文件后重新提交！选择要保留他的代码还是你的代码！ master主分支一般非常稳定，用来发布新的版本，一般情况下不允许在上面进行开发，工作一般情况下在新建的dev分支上工作，工作完成后，比如要发布，则可将dev分支合并到主分支master上来。 Vscode 插件推荐 GitLens — Git supercharged Git History Diff"},{"title":"InfoNCE","path":"/wiki/DL-Techniques/InfoNCE.html","content":"在深度学习中，对比学习（Contrastive Learning）是一种强大的方法，用于学习无监督或自监督表示。InfoNCE（Information Noise Contrastive Estimation）是一种用于对比学习的损失函数，它在表征学习中发挥了重要作用。本文将详细介绍InfoNCE loss的定义、原理及其在实际应用中的作用。 基本介绍 InfoNCE损失最初由Aaron van den Oord等人在其论文《Representation Learning with Contrastive Predictive Coding》中提出。它的主要思想是通过最大化目标样本和正样本之间的相似度，同时最小化目标样本与一组负样本之间的相似度，从而学习有用的特征表示。 定义 在对比学习中，我们有一个目标样本（anchor）、一个正样本（positive sample）和一组负样本（negative samples）。InfoNCE损失函数可以定义为： LInfoNCE=−log⁡exp⁡(sim(zi,zi+))exp⁡(sim(zi,zi+))+∑j=1Kexp⁡(sim(zi,zj−))\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_i^+))}{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_i^+)) + \\sum_{j=1}^{K} \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j^-))} LInfoNCE​=−logexp(sim(zi​,zi+​))+∑j=1K​exp(sim(zi​,zj−​))exp(sim(zi​,zi+​))​ 其中： zi\\mathbf{z}_izi​ 是目标样本的表示。 zi+\\mathbf{z}_i^+zi+​ 是正样本的表示。 zj−\\mathbf{z}_j^-zj−​ 是负样本的表示。 sim(⋅,⋅)\\text{sim}(\\cdot, \\cdot)sim(⋅,⋅) 是相似度函数，通常采用余弦相似度或点积。 工作原理 InfoNCE损失通过以下过程来优化模型： 相似度计算：计算目标样本与正样本之间的相似度，以及目标样本与每个负样本之间的相似度。 归一化：将正样本和负样本之间的相似度进行归一化，以确保正样本的相似度在负样本的相似度之上。 最大化对比：通过最大化目标样本和正样本之间的相似度，同时最小化目标样本与负样本之间的相似度，模型能够学习到更好的特征表示。 应用场景 InfoNCE loss在以下几个领域中得到了广泛应用： 自然语言处理（NLP）：用于学习词向量和句子表示，例如在GPT和BERT等模型中。 计算机视觉（CV）：用于无监督学习图像表示，如SimCLR和MoCo等方法。 语音处理：用于学习音频信号的表示，如在Contrastive Predictive Coding (CPC) 中。 实际应用示例 以下是一个使用PyTorch实现InfoNCE loss的简单示例： 1234567891011121314151617181920212223242526272829import torchimport torch.nn.functional as Fdef info_nce_loss(anchor, positive, negatives, temperature=0.07): # 计算相似度 anchor_positive_similarity = F.cosine_similarity(anchor, positive) anchor_negative_similarity = F.cosine_similarity(anchor.unsqueeze(1), negatives, dim=2) # 计算正样本和负样本的相似度 positives_exp = torch.exp(anchor_positive_similarity / temperature) negatives_exp = torch.exp(anchor_negative_similarity / temperature).sum(dim=1) # 计算InfoNCE loss loss = -torch.log(positives_exp / (positives_exp + negatives_exp)).mean() return loss# 示例输入batch_size = 16embedding_dim = 128num_negatives = 10anchor = torch.randn(batch_size, embedding_dim)positive = torch.randn(batch_size, embedding_dim)negatives = torch.randn(batch_size, num_negatives, embedding_dim)# 计算InfoNCE lossloss = info_nce_loss(anchor, positive, negatives)print(f&#x27;InfoNCE Loss: &#123;loss.item()&#125;&#x27;) 与Triplet Loss的对比 总结 InfoNCE loss在对比学习中扮演了重要角色，它通过对目标样本、正样本和负样本之间的相似度进行对比，从而帮助模型学习到更好的特征表示。无论是在自然语言处理、计算机视觉还是语音处理领域，InfoNCE loss都展示出了其强大的能力和广泛的应用前景。了解和掌握InfoNCE loss的原理和应用，将为从事相关领域的研究人员和工程师提供重要的工具和方法。"},{"title":"交并比 (IOU)","path":"/wiki/DL-Techniques/交并比 (IOU).html","content":"什么是IOU？ IOU (Intersection over Union) 是一个常用的指标，用于评估图像分割或目标检测任务的性能。它计算了预测的边界框或分割区域与实际的边界框或分割区域的重叠部分占总区域的比例。具体来说，IOU 是实际区域与预测区域的交集面积与并集面积之比。 公式如下： IOU=Intersection AreaUnion Area\\text{IOU} = \\frac{\\text{Intersection Area}}{\\text{Union Area}} IOU=Union AreaIntersection Area​ 其中： Intersection Area 是预测区域与实际区域的重叠部分。 Union Area 是预测区域和实际区域的并集部分。 什么是 mIOU？ mIOU (Mean Intersection over Union) 是对多个样本的IOU进行平均，以评估模型在整个数据集上的表现。它通过计算所有样本的IOU，并取平均值来得到。 mIOU 的计算公式如下： mIOU=1N∑i=1NIOUi\\text{mIOU} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{IOU}_i mIOU=N1​i=1∑N​IOUi​ 其中： NNN 是样本的数量。 IOUi\\text{IOU}_iIOUi​ 是第 iii 个样本的 IOU 值。 什么是 oIOU？ oIOU (Overall Intersection over Union) 是通过累计所有样本的交集面积和并集面积来计算的IOU。与 mIOU 不同的是，oIOU 直接计算总的交集面积和并集面积的比例，而不是对单个样本的IOU进行平均。 oIOU 的计算公式如下： oIOU=∑i=1NIntersection Areai∑i=1NUnion Areai\\text{oIOU} = \\frac{\\sum_{i=1}^{N} \\text{Intersection Area}_i}{\\sum_{i=1}^{N} \\text{Union Area}_i} oIOU=∑i=1N​Union Areai​∑i=1N​Intersection Areai​​ 其中： NNN 是样本的数量。 Intersection Areai\\text{Intersection Area}_iIntersection Areai​ 和 Union Areai\\text{Union Area}_iUnion Areai​ 分别是第 iii 个样本的交集面积和并集面积。 IOU、mIOU 和 oIOU 的计算代码 下面是一个用于计算 IOU、mIOU 和 oIOU 的代码示例，实现了一个测试器类，使用方法是初始化后传入每次的预测和目标mask，会计算出iou，最后调用evalute方法可以输出整个测试数据集的mIOU和oIOU值 实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import numpy as npimport torchclass Evaluator(): def __init__(self): self.counters_by_iou = &#123;iou: 0 for iou in [0.5, 0.6, 0.7, 0.8, 0.9]&#125; self.total_intersection_area = 0 self.total_union_area = 0 self.ious_list = [] pass def compute_mask_iou(self, outputs: torch.Tensor, labels: torch.Tensor, EPS=1e-6): assert outputs.shape[0] == 1; assert outputs.shape == labels.shape; assert len(outputs.shape) == 3 outputs = outputs.int(); labels = labels.int() intersection = (outputs &amp; labels).float().sum((1, 2)) # Will be zero if Truth=0 or Prediction=0 union = (outputs | labels).float().sum((1, 2)) # Will be zero if both are 0 iou = (intersection + EPS) / (union + EPS) # EPS is used to avoid division by zero iou, intersection, union = iou.item(), intersection.item(), union.item() for iou_threshold in self.counters_by_iou.keys(): if iou &gt; iou_threshold: self.counters_by_iou[iou_threshold] += 1 self.total_intersection_area += intersection self.total_union_area += union self.ious_list.append(iou) return iou, intersection, union def evaluate(self): num_samples = len(self.ious_list) if num_samples == 0: print(&quot;No samples to evaluate.&quot;) return precision_at_k = np.array(list(self.counters_by_iou.values())) / num_samples overall_iou = self.total_intersection_area / self.total_union_area mean_iou = np.mean(self.ious_list) print(&quot;Evaluation Result&quot;) iou_thresholds = [0.5, 0.6, 0.7, 0.8, 0.9] for iou, prec in zip(iou_thresholds, precision_at_k): print(f&quot;IoU: &#123;iou:.2f&#125;, Precision: &#123;prec:.4f&#125;&quot;) print(&quot;========================================&quot;) print(f&quot;Overall IoU: &#123;overall_iou:.4f&#125;&quot;) print(f&quot;Mean IoU: &#123;mean_iou:.4f&#125;&quot;) 结论 IOU 是评估图像分割和目标检测任务性能的重要指标。通过计算每个样本的 IOU，然后求平均值可以得到 mIOU，而通过计算所有样本的总交集和总并集的比例可以得到 oIOU。理解和计算这些指标有助于全面评估模型在分割任务中的表现。上述代码提供了一个计算 IOU、mIOU 和 oIOU 的实现示例，可以帮助你在实际任务中进行评估。"},{"title":"梯度反转层 (GRL)","path":"/wiki/DL-Techniques/梯度反转层 (GRL).html","content":"梯度反转层（Gradient Reversal Layer, GRL）是一种在对抗训练中常用的技术，特别是在领域自适应任务中。其核心思想是通过在前向传播过程中保持输入不变，而在反向传播过程中将梯度反转，即将梯度乘以一个负数，从而改变参数更新的方向。这样可以迫使特征提取器生成的特征在源域和目标域之间表现得更加一致，使得领域分类器难以区分它们，从而达到领域自适应的目的。梯度反转层的使用可以显著提高模型在不同领域中的泛化能力，解决领域间分布差异导致的问题。 流程介绍 这种机制在如域自适应等一些特定的任务中非常有用，特别是在对抗训练（如领域自适应）中。以下是一个具体的解释： 正常训练过程 在正常的训练过程中，梯度反向传播的步骤如下： 前向传播：计算损失函数的值。 反向传播：计算每个参数对损失函数的梯度。 梯度更新：使用优化器根据梯度更新参数，例如使用随机梯度下降法 (SGD)：θ=θ−η∇L(θ)\\theta = \\theta - \\eta abla L(\\theta) θ=θ−η∇L(θ) 其中 θ\\thetaθ 是参数，η\\etaη 是学习率，∇L(θ) abla L(\\theta)∇L(θ) 是损失函数 LLL 对参数 θ\\thetaθ 的梯度。 使用梯度反转层的训练过程 当使用梯度反转层时，反向传播过程中的梯度会被乘以 −α-\\alpha−α，从而实现反向更新参数。具体步骤如下： 前向传播：梯度反转层对前向传播没有影响，直接传递输入数据。 反向传播：梯度反转层对梯度进行反转，即乘以 −α-\\alpha−α，使得反向传播的梯度变为 −α∇L(θ)-\\alpha abla L(\\theta)−α∇L(θ)。 梯度更新：使用优化器根据反转后的梯度更新参数：θ=θ−η(−α∇L(θ))=θ+ηα∇L(θ)\\theta = \\theta - \\eta (-\\alpha abla L(\\theta)) = \\theta + \\eta \\alpha abla L(\\theta) θ=θ−η(−α∇L(θ))=θ+ηα∇L(θ) 可以看到，参数更新的方向与正常训练相反，且更新的步长由 (\\alpha) 控制。 应用场景 这种反向更新参数的机制在领域自适应任务中非常有用。例如，在训练一个领域适应的神经网络时，我们希望特征提取器提取的特征在源域和目标域上都表现得一致。为此，可以引入一个领域分类器（Domain Classifier），并在其前面添加梯度反转层。在反向传播过程中，梯度反转层会反转领域分类器的梯度，迫使特征提取器提取的特征在源域和目标域之间难以区分，从而实现领域自适应。 实现代码 梯度反转层GRL的实现代码如下： 1234567891011121314151617181920212223242526from torch import nnfrom torch.autograd import Functionclass ReverseGradFunction(Function): @staticmethod def forward(ctx, data, alpha=1.0): ctx.alpha = alpha return data @staticmethod def backward(ctx, grad_outputs): grad = None if ctx.needs_input_grad[0]: grad = -ctx.alpha * grad_outputs return grad, Noneclass ReverseGrad(nn.Module): def __init__(self): super(ReverseGrad, self).__init__() def forward(self, x, alpha=1.0): return ReverseGradFunction.apply(x, alpha) 在领域自适应任务中，梯度反转层（Gradient Reversal Layer, GRL）通常放置在特征提取器和领域分类器（Domain Classifier）之间，而不是在主分类器（Primary Classifier）之前或之后。其目的是对领域分类器的梯度进行反转，从而迫使特征提取器提取的特征在源域和目标域之间表现得更加一致。 具体的网络结构如下： 特征提取器（Feature Extractor）：从输入数据中提取特征。 梯度反转层（Gradient Reversal Layer）：反转反向传播中的梯度。 领域分类器（Domain Classifier）：预测特征来自源域还是目标域。 主分类器（Primary Classifier）：基于提取的特征进行主要任务（如分类）。 这种结构的目的是利用领域分类器的梯度反向更新特征提取器，从而使特征提取器生成的特征在不同领域之间无法区分。 示例代码 以下是一个带有梯度反转层的领域自适应模型示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import torchimport torch.nn as nnimport torch.optim as optimclass FeatureExtractor(nn.Module): def __init__(self): super(FeatureExtractor, self).__init__() self.features = nn.Sequential( nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) def forward(self, x): return self.features(x)class DomainClassifier(nn.Module): def __init__(self): super(DomainClassifier, self).__init__() self.classifier = nn.Sequential( nn.Linear(32 * 14 * 14, 128), nn.ReLU(), nn.Linear(128, 2) # 假设有两个领域 ) def forward(self, x): return self.classifier(x)class PrimaryClassifier(nn.Module): def __init__(self): super(PrimaryClassifier, self).__init__() self.classifier = nn.Sequential( nn.Linear(32 * 14 * 14, 128), nn.ReLU(), nn.Linear(128, 10) # 假设有10个类别 ) def forward(self, x): return self.classifier(x)class FullModel(nn.Module): def __init__(self): super(FullModel, self).__init__() self.feature_extractor = FeatureExtractor() self.domain_classifier = DomainClassifier() self.primary_classifier = PrimaryClassifier() def forward(self, x, alpha=1.0): features = self.feature_extractor(x) features_flattened = features.view(features.size(0), -1) # 对领域分类器使用梯度反转层 domain_output = ReverseGrad()(features_flattened, alpha) domain_output = self.domain_classifier(domain_output) # 对主分类器不使用梯度反转层 primary_output = self.primary_classifier(features_flattened) return primary_output, domain_output# 初始化模型、损失函数和优化器model = FullModel()criterion_class = nn.CrossEntropyLoss()criterion_domain = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)# 生成随机输入数据和标签input_data = torch.randn(8, 1, 28, 28) # 8个28x28的灰度图像labels_class = torch.randint(0, 10, (8,)) # 8个随机主任务标签labels_domain = torch.randint(0, 2, (8,)) # 8个随机领域标签# 前向传播primary_output, domain_output = model(input_data, alpha=0.5)# 计算损失loss_class = criterion_class(primary_output, labels_class)loss_domain = criterion_domain(domain_output, labels_domain)loss = loss_class + loss_domain# 反向传播和优化optimizer.zero_grad()loss.backward()optimizer.step() 解释 特征提取器：提取输入数据的特征。 梯度反转层：对特征提取器输出的特征进行梯度反转，然后输入到领域分类器中。 领域分类器：根据反转后的特征判断其来自哪个领域。 主分类器：根据特征提取器输出的特征进行主要任务的分类。 通过这种结构，梯度反转层会反转领域分类器的梯度，使得特征提取器生成的特征在不同领域之间更难区分，从而实现领域自适应。"},{"title":"结果固定代码","path":"/wiki/DL-Techniques/结果固定代码.html","content":"在训练和测试的时候可以在脚本中加入如下代码，对结果进行固定，确保每次运行脚本结果保持一致： 1234567def seed_torch(seed): os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True 12345678910111213141516171819202122232425262728def seed_torch(seed): &quot;&quot;&quot; Set the random seed for various modules to ensure reproducibility. Args: seed (int): The seed value to be set. &quot;&quot;&quot; # Set PYTHONHASHSEED environment variable os.environ[&#x27;PYTHONHASHSEED&#x27;] = str(seed) # Set random seed for Python&#x27;s built-in random module random.seed(seed) # Set random seed for numpy np.random.seed(seed) # Set random seed for PyTorch torch.manual_seed(seed) # Set random seed for PyTorch (CUDA) if torch.cuda.is_available(): torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if you are using multi-GPU. # Ensure that the cuDNN library&#x27;s benchmark mode is disabled, and that # cuDNN is deterministic. torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True"},{"title":"动态规划","path":"/wiki/Luogu/动态规划.html","content":"【动态规划1】动态规划的引入 P1216 [USACO1.5] [IOI1994]数字三角形 Number Triangles 题目传送门：P1216 [USACO1.5] [IOI1994]数字三角形 Number Triangles - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 非常基础的一道题目，直接递推刷表即可。 123456789101112131415161718192021222324252627282930/* * Problem: P1216 [USACO1.5] [IOI1994]数字三角形 Number Triangles * URL: https://www.luogu.com.cn/problem/P1216 * Description: basic dp * Created by Vegetabot on 2023/12/17. */#include&lt;bits/stdc++.h&gt;#define N 1002using namespace std;vector&lt;vector&lt;int&gt;&gt; dp(N,vector&lt;int&gt;(N,0));vector&lt;vector&lt;int&gt;&gt; nums(N,vector&lt;int&gt;(N,0));int n;int main()&#123; scanf(&quot;%d&quot;,&amp;n); for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;=i;++j)&#123; scanf(&quot;%d&quot;,&amp;nums[i][j]); &#125; &#125; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=i;++j)&#123; dp[i][j] = max(dp[i-1][j-1],dp[i-1][j]) + nums[i-1][j-1]; &#125; &#125; int ans = dp[n][0]; for(int i=1;i&lt;=n;++i) ans = ans&gt;dp[n][i]?ans:dp[n][i]; printf(&quot;%d&quot;,ans); return 0;&#125; P1048 [NOIP2005 普及组] 采药 题目传送门：P1048 [NOIP2005 普及组] 采药 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 01背包板子题，背包相关的知识点建议可以学习网上非常的流行的背包九讲教程 12345678910111213141516171819202122232425262728293031/* * Problem: P1048 [NOIP2005 普及组] 采药 * URL: https://www.luogu.com.cn/problem/P1048 * Description: 01背包 * Created by Vegetabot on 2023/12/17. */#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; dp(101,vector&lt;int&gt;(1001,0));vector&lt;int&gt; times,vals;int M,T;int main()&#123; scanf(&quot;%d%d&quot;,&amp;T,&amp;M); for(int i=0;i&lt;M;++i)&#123; int a,b; scanf(&quot;%d%d&quot;,&amp;a,&amp;b); times.emplace_back(a); vals.emplace_back(b); &#125; for(int i=1;i&lt;=M;++i)&#123; for(int j=0;j&lt;=T;++j)&#123; if (j &lt; times[i-1]) dp[i][j] = dp[i-1][j]; else dp[i][j] = max(dp[i-1][j],dp[i-1][j-times[i-1]]+vals[i-1]); &#125; &#125; int ans = dp[M][0]; for(int i=0;i&lt;=T;++i) ans = ans&gt;dp[M][i]?ans:dp[M][i]; printf(&quot;%d&quot;,ans); return 0;&#125; 感觉emplace_back性能会比push_back要好一些， 以后都用emplace_back好了 同样因为是动态规划问题，所以毋庸置疑的是，这道题目也可以使用记忆化搜索的方式来解决。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940/* * Problem: P1048 [NOIP2005 普及组] 采药 * URL: https://www.luogu.com.cn/problem/P1048 * Description: 01背包 * Created by Vegetabot on 2023/12/17. */#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;vector&lt;int&gt;&gt; dp;vector&lt;int&gt; times, vals;int M, T;int solve(int i, int j) &#123; if (i == 0) return 0; // 边界条件 if (dp[i][j] != -1) return dp[i][j]; // 已经计算过 int notTake = solve(i - 1, j); // 不选择当前物品 int take = 0; if (j &gt;= times[i-1]) &#123; take = vals[i-1] + solve(i - 1, j - times[i-1]); // 选择当前物品 &#125; return dp[i][j] = max(notTake, take); // 记忆化存储并返回结果&#125;int main() &#123; scanf(&quot;%d%d&quot;, &amp;T, &amp;M); dp.assign(M+1, vector&lt;int&gt;(T + 1, -1)); // 初始化记忆化数组 times.resize(M); vals.resize(M); for (int i = 0; i &lt; M; ++i) &#123; scanf(&quot;%d%d&quot;, &amp;times[i], &amp;vals[i]); &#125; printf(&quot;%d&quot;, solve(M, T)); // 从第0个物品开始，总限制为T return 0;&#125; 当然01背包问题还可以使用滚动数组对空间进行优化，这里就不再赘述了（因为写的比较多）"},{"title":"png图片填充脚本","path":"/wiki/Others/png图片填充脚本.html","content":"脚本作用：将给定PNG图像中透明部分填充为白色 123456789101112131415161718192021from PIL import Imageoriginal_path = &#x27;xxxx&#x27;original_file = &#x27;cc.png&#x27;target_path = &#x27;xxx&#x27;# 打开原始PNG图像image = Image.open(original_path+original_file).convert(&quot;RGBA&quot;)# 创建一个白色背景white_background = Image.new(&quot;RGBA&quot;, image.size, &quot;WHITE&quot;)# 将原始图像粘贴到白色背景上，透明部分将被白色填充white_background.paste(image, (0, 0), image)# 将结果转换为RGB模式并保存final_image = white_background.convert(&quot;RGB&quot;)final_image.save(target_path+original_file)# 显示结果图像（可选）final_image.show()"},{"title":"PyTorch CNN实战","path":"/wiki/PyTorch/PyTorch CNN实战.html","content":"本博客主要使用PyTorch先实现一个简单的CNN对CIFAR-10数据集对图片物体进行分类操作，然后实现一个ResNet同样也是应用于CIFAR-10数据集对物体进行分类操作。 在开始之前我们先介绍一下本次实验所使用的数据集CIFAR-10 CIFAR-10 CIFAR-10 是由 Hinton 的学生 Alex Krizhevsky 和 Ilya Sutskever 整理的一个用于识别普适物体的小型数据集。一共包含 10 个类别的 RGB 彩色图 片：飞机（ a叩lane ）、汽车（ automobile ）、鸟类（ bird ）、猫（ cat ）、鹿（ deer ）、狗（ dog ）、蛙类（ frog ）、马（ horse ）、船（ ship ）和卡车（ truck ）。图片的尺寸为 32×32 ，数据集中一共有 50000 张训练图片和 10000 张测试图片（每一类物体有6000张照片）。 CIFAR-10 的图片样例如图所示。 与 MNIST 数据集中目比， CIFAR-10 具有以下不同点： • CIFAR-10 是 3 通道的彩色 RGB 图像，而 MNIST 是灰度图像。 • CIFAR-10 的图片尺寸为 32×32， 而 MNIST 的图片尺寸为 28×28，比 MNIST 稍大。 • 相比于手写字符， CIFAR-10 含有的是现实世界中真实的物体，不仅噪声很大，而且物体的比例、 特征都不尽相同，这为识别带来很大困难。 直接的线性模型如 Softmax 在 CIFAR-10 上表现得很差。 下面这幅图就是列举了10各类，每一类展示了随机的10张图片： 实现简单的CNN 加载数据集 步骤分为两步，首先是导入数据集（这一步也包含了数据的增强，比如旋转切割之类的），然后就是使用Dataloader导入数据。对于训练集和测试集都要导入数据，但是要记住，有个bool类型的参数要设置的不同。 12345678910111213141516171819batchsz = 32cifar_train =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, # 这个True代表是训练集 transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor() ]), download=True )cifar_train = DataLoader(cifar_train,batch_size=batchsz,shuffle=True)cifar_test =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor() ]), download=True )cifar_test = DataLoader(cifar_test,batch_size=batchsz,shuffle=True) 然后我们可以使用以下两行代码来简单测试一下我们的数据是否导入成功了： 12x,label = iter(cifar_train).next()print(&#x27;x:&#x27;,x.shape) 实现LeNet-5网络结构 根据我们前面所学的理论知识，我们知道LeNet-5主要分为两个部分，卷积部分和全连接部分，中间有一个Flatten操作，我们可以采用如下这种方式对网络结构进行实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class LeNet5(nn.Module): &#x27;&#x27;&#x27; for CIFAR10 dataset &#x27;&#x27;&#x27; def __init__(self): super(LeNet5,self).__init__() self.conv_unit = nn.Sequential( # input: [batchsz,3,32,32] nn.Conv2d(3,6,kernel_size=5, stride=1, padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), nn.Conv2d(6,16,kernel_size=5,stride=1,padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), # 然后下面就是卷积层转全连接层了，这里需要一个打平操作 ) # Flatten # fc unit self.fc_unit=nn.Sequential( nn.Linear(16*5*5,120), nn.ReLU(inplace=True), nn.Linear(120,84), nn.ReLU(inplace=True), nn.Linear(84,10) ) # 以下是测试用的代码，用来测试卷积神经网络最后输出的图片大小是多少 # [batchsz,3,32,32] tmp = torch.randn(2,3,32,32) out = self.conv_unit(tmp) # 测试输出:[batchsz,16,5,5] print(&#x27;conv out&#x27;,out.shape) def forward(self,x): &#x27;&#x27;&#x27; :param input : [batchsz,3,32,32] :return logits &#x27;&#x27;&#x27; batchsz = x.size(0) # [batchsz,3,32,32] =&gt; [batchsz,16,5,5] x = self.conv_unit(x) x = x.view(batchsz,16*5*5) #view(batchsz,-1) # [batchsz,16*5*5] =&gt; [b,10] logits = self.fc_unit(x) return logits 可以发现上面我们使用了两个Sequential然后是把他们在forward中连起来的。或许你可能觉得这样写有一点麻烦，那么我们也可以把Flatten操作继承一个nn.module类，这样他就可以被放入Sequential中了，然后我们就有了如下的写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import torchfrom torch import nnimport torch.nn.functional as Fclass Flatten(nn.Module): def __init__(self): super(Flatten,self).__init__() def forward(self,input): return input.view(input.size(0),-1)class LeNet5(nn.Module): &#x27;&#x27;&#x27; for CIFAR10 dataset &#x27;&#x27;&#x27; def __init__(self): super(LeNet5,self).__init__() self.nn_unit = nn.Sequential( # input: [batchsz,3,32,32] nn.Conv2d(3,6,kernel_size=5, stride=1, padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), nn.Conv2d(6,16,kernel_size=5,stride=1,padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), # Flatten Flatten(), # fc unit nn.Linear(16*5*5,120), nn.ReLU(inplace=True), nn.Linear(120,84), nn.ReLU(inplace=True), nn.Linear(84,10) # 然后下面就是卷积层转全连接层了，这里需要一个打平操作 ) # 以下是测试用的代码 # [batchsz,3,32,32] tmp = torch.randn(2,3,32,32) out = self.nn_unit(tmp) print(&#x27;nn out&#x27;,out.shape) def forward(self,x): &#x27;&#x27;&#x27; :param input : [batchsz,3,32,32] :return logits &#x27;&#x27;&#x27; # [batchsz,3,32,32] =&gt; [batchsz,16,5,5] logits = self.nn_unit(x) return logitsdef main(): net = LeNet5() #测试网络结构（网络能否跑通） tmp = torch.randn(2,3,32,32) out = net(tmp) print(&#x27;LeNet out&#x27;,out.shape)if __name__ == &#x27;__main__&#x27;: main() 正如上面代码所写，当我们不确定CNN输入tensor的形状的时候，可以在main函数中写： 1234567def main(): net = LeNet5() #测试网络结构（网络能否跑通） tmp = torch.randn(2,3,32,32) out = net(tmp) print(&#x27;LeNet out&#x27;,out.shape) 或者可以在初始化网络结构的时候也可以进行测试，我们在我们定义的类下的__init__函数中写： 123456# 以下是测试用的代码# [batchsz,3,32,32]tmp = torch.randn(2,3,32,32)out = self.nn_unit(tmp)# 测试输出:[batchsz,16,5,5]print(&#x27;nn out&#x27;,out.shape) 训练部分 和前面的全连接神经网络一样，步骤是一样的。这里多增加了如何使用GPU进行训练，以及采用了与前面不一样的CrossEntropyLoss()作为criteon，使用Adam作为优化器。 123456789101112131415161718192021222324252627# 定义GPUdevice = torch.device(&#x27;cuda&#x27;)# 导入模型model = LeNet5().to(device) # 将模型放到显卡上print(model)# 设置criteoncriteon=nn.CrossEntropyLoss()# 设置优化器(目标优化参数，学习率)optimizer=optim.Adam(model.parameters(),lr=1e-3)# 训练函数for epoch in range(1000): #训练轮数 for batchidx,(x,label) in enumerate(cifar_train): # x: [batchsz,3,32,32] # label: [batchsz] x, label =x.to(device),label.to(device) # 将数据放到显卡上 logits=model(x) loss = criteon(logits,label) # backprop optimizer.zero_grad() # 清零 loss.backward() # 反向传播 optimizer.step() # 更新一次 `enumerate`的作用如下，相当于是对可以枚举的对象前面加上索引： 123arr=[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]for idx,item in enumerate(arr): print(idx,&quot;:&quot;,item) 12340 : a1 : b2 : c3 : d 实现Val 我们每次训练完了一个epoch都要进行Val，这样我们才知道我们的模型是否训练的合适。Val部分其实和之前的全连接神经网络是差不多的，这里不再赘述。 123456789101112131415# valtotal_correct=0total_num=0for x,label in cifar_test: x,label=x.to(device),label.to(device) # [batchsz, 10] logits = model(x) # [batchsz] pred = logits.argmax(dim=1) total_correct += torch.eq(pred,label).float().sum().item() # eq 是逐个比较最后输出矩阵大小和label的大小是一样的 total_num += x.size(0) acc =total_correct / total_num print(&#x27;acc&#x27;,acc) 注意torch.eq和torch.equal的区别 Train+Val 整体代码如下： 1234567891011121314151617181920212223242526272829303132333435# Train+Val for epoch in range(1000): for batchidx,(x,label) in enumerate(cifar_train): # x: [batchsz,3,32,32] # label: [batchsz] x, label =x.to(device),label.to(device) # 将数据放到显卡上 logits=model(x) loss = criteon(logits,label) # backprop optimizer.zero_grad() # 清零 loss.backward() # 反向传播 optimizer.step() # 更新一次 # 输出每一轮训练结束后的loss print(epoch,loss.item()) # val total_correct=0 total_num=0 for x,label in cifar_test: x,label=x.to(device),label.to(device) # [b, 10] logits = model(x) # [b] pred = logits.argmax(dim=1) total_correct += torch.eq(pred,label).float().sum().item() # eq 是逐个比较最后输出矩阵大小和label的大小是一样的 total_num += x.size(0) acc =total_correct / total_num print(&#x27;acc&#x27;,acc) 其他细节 Val代码优化 因为Val是做测试的，是不需要梯度信息，反向传播对参数进行优化的，所以我们可以把上述val部分代码包含到with torch.no_grad():中，这段代码相当于告诉PyTorch被这段代码包含的代码是不需要梯度信息的。这么写更加的安全。 12345678910111213141516with torch.no_grad():\t# val total_correct=0 total_num=0 for x,label in cifar_test: x,label=x.to(device),label.to(device) # [batchsz, 10] logits = model(x) # [batchsz] pred = logits.argmax(dim=1) total_correct += torch.eq(pred,label).float().sum().item() # eq 是逐个比较最后输出矩阵大小和label的大小是一样的 total_num += x.size(0) acc =total_correct / total_num print(&#x27;acc&#x27;,acc) 模型模式切换 因为对于我们构造的部分网络中的特定层，这些层在训练和测试中的表现是不一样的，比如BatchNorm层他在训练状态下和测试状态下的行为是有一些差异的，如果我们对网络中的每一层都去执行切换状态的操作是非常麻烦的，但是nn.Module支持对自定义网络整体状态的切换，大大简化了操作。 要想达到这个目的，我们需要添加两行代码 12345678910111213141516171819202122232425262728293031323334353637# Train+Val for epoch in range(1000):+ model.train() for batchidx,(x,label) in enumerate(cifar_train): # x: [batchsz,3,32,32] # label: [batchsz] x, label =x.to(device),label.to(device) # 将数据放到显卡上 logits=model(x) loss = criteon(logits,label) # backprop optimizer.zero_grad() # 清零 loss.backward() # 反向传播 optimizer.step() # 更新一次 # 输出每一轮训练结束后最后一个Batch的loss print(epoch,loss.item()) # val+ model.eval() with torch.no_grad(): total_correct=0 total_num=0 for x,label in cifar_test: x,label=x.to(device),label.to(device) # [b, 10] logits = model(x) # [b] pred = logits.argmax(dim=1) total_correct += torch.eq(pred,label).float().sum().item() # eq 是逐个比较最后输出矩阵大小和label的大小是一样的 total_num += x.size(0) acc =total_correct / total_num print(&#x27;acc&#x27;,acc) 代码汇总 main.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# CIFAR 10import torch import torch.nn as nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision import transformsfrom LeNet5 import LeNet5from torch import optimdef main(): batchsz = 32 cifar_train =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, # 这个True代表是训练集 transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor() ]), download=True ) cifar_train = DataLoader(cifar_train,batch_size=batchsz,shuffle=True) cifar_test =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor() ]), download=True ) cifar_test = DataLoader(cifar_test,batch_size=batchsz,shuffle=True) x,label = iter(cifar_train).next() print(&#x27;x:&#x27;,x.shape) # 定义GPU device = torch.device(&#x27;cuda&#x27;) # 导入模型 model = LeNet5().to(device) # 将模型放到显卡上 print(model) # 设置criteon criteon=nn.CrossEntropyLoss() # 设置优化器(目标优化参数，学习率) optimizer=optim.Adam(model.parameters(),lr=1e-3) # Train+Val for epoch in range(1000): model.train() for batchidx,(x,label) in enumerate(cifar_train): # x: [batchsz,3,32,32] # label: [batchsz] x, label =x.to(device),label.to(device) # 将数据放到显卡上 logits=model(x) loss = criteon(logits,label) # backprop optimizer.zero_grad() # 清零 loss.backward() # 反向传播 optimizer.step() # 更新一次 # 输出每一轮训练结束后最后一个Batch的loss print(epoch,loss.item()) # val model.eval() with torch.no_grad(): total_correct=0 total_num=0 for x,label in cifar_test: x,label=x.to(device),label.to(device) # [b, 10] logits = model(x) # [b] pred = logits.argmax(dim=1) total_correct += torch.eq(pred,label).float().sum().item() # eq 是逐个比较最后输出矩阵大小和label的大小是一样的 total_num += x.size(0) acc =total_correct / total_num print(&#x27;acc&#x27;,acc) if __name__ == &#x27;__main__&#x27;: main() LeNet5.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import torchfrom torch import nnimport torch.nn.functional as Fclass LeNet5(nn.Module): &#x27;&#x27;&#x27; for CIFAR10 dataset &#x27;&#x27;&#x27; def __init__(self): super(LeNet5,self).__init__() self.conv_unit = nn.Sequential( # input: [batchsz,3,32,32] nn.Conv2d(3,6,kernel_size=5, stride=1, padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), nn.Conv2d(6,16,kernel_size=5,stride=1,padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), # 然后下面就是卷积层转全连接层了，这里需要一个打平操作 ) # Flatten # fc unit self.fc_unit=nn.Sequential( nn.Linear(16*5*5,120), nn.ReLU(inplace=True), nn.Linear(120,84), nn.ReLU(inplace=True), nn.Linear(84,10) ) # 以下是测试用的代码，用来测试卷积神经网络最后输出的图片大小是多少 # [batchsz,3,32,32] tmp = torch.randn(2,3,32,32) out = self.conv_unit(tmp) # 测试输出:[batchsz,16,5,5] print(&#x27;conv out&#x27;,out.shape) def forward(self,x): &#x27;&#x27;&#x27; :param input : [batchsz,3,32,32] :return logits &#x27;&#x27;&#x27; batchsz = x.size(0) # [batchsz,3,32,32] =&gt; [batchsz,16,5,5] x = self.conv_unit(x) x = x.view(batchsz,16*5*5) #view(batchsz,-1) # [batchsz,16*5*5] =&gt; [b,10] logits = self.fc_unit(x) return logitsdef main(): net = LeNet5() #测试网络结构（网络能否跑通） tmp = torch.randn(2,3,32,32) out = net(tmp) print(&#x27;LeNet out&#x27;,out.shape)if __name__ == &#x27;__main__&#x27;: main() 第二种方法写的LeNet5 LeNet5pro.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import torchfrom torch import nnimport torch.nn.functional as Fclass Flatten(nn.Module): def __init__(self): super(Flatten,self).__init__() def forward(self,input): return input.view(input.size(0),-1)class LeNet5(nn.Module): &#x27;&#x27;&#x27; for CIFAR10 dataset &#x27;&#x27;&#x27; def __init__(self): super(LeNet5,self).__init__() self.nn_unit = nn.Sequential( # input: [batchsz,3,32,32] nn.Conv2d(3,6,kernel_size=5, stride=1, padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), nn.Conv2d(6,16,kernel_size=5,stride=1,padding=0), nn.AvgPool2d(kernel_size=2,stride=2,padding=0), # Flatten Flatten(), # fc unit nn.Linear(16*5*5,120), nn.ReLU(inplace=True), nn.Linear(120,84), nn.ReLU(inplace=True), nn.Linear(84,10) # 然后下面就是卷积层转全连接层了，这里需要一个打平操作 ) # 以下是测试用的代码，用来测试卷积神经网络最后输出的图片大小是多少 # [batchsz,3,32,32] tmp = torch.randn(2,3,32,32) out = self.nn_unit(tmp) # 测试输出:[batchsz,16,5,5] print(&#x27;nn out&#x27;,out.shape) def forward(self,x): &#x27;&#x27;&#x27; :param input : [batchsz,3,32,32] :return logits &#x27;&#x27;&#x27; # [batchsz,3,32,32] =&gt; [batchsz,16,5,5] logits = self.nn_unit(x) return logitsdef main(): net = LeNet5() #测试网络结构（网络能否跑通） tmp = torch.randn(2,3,32,32) out = net(tmp) print(&#x27;LeNet out&#x27;,out.shape)if __name__ == &#x27;__main__&#x27;: main() 实现ResNet 本次实验以ResNet18为例，但是和论文中的ResNet18还是有些许差异的 ，因为采用的数据集不太一样。 实现残差块 本次残差块的实现和之前的有一些区别，加入了步长，这样就能在提高深度的同时缩小图片，具体代码如下。 需要注意的点是，如果输入通道和输出通道数量不一样，shortcut的路径上可能还是要再加一层卷积层，来使通道数量一样，这样才能相加。 123456789101112131415161718192021222324252627282930313233343536class ResBlk(nn.Module): &#x27;&#x27;&#x27; ResNet Block &#x27;&#x27;&#x27; def __init__(self,ch_in,ch_out,stride=1): &#x27;&#x27;&#x27; :param ch_in :param ch_out &#x27;&#x27;&#x27; super(ResBlk,self).__init__() self.conv1 = nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=stride,padding=1) self.bn1 = nn.BatchNorm2d(ch_out) self.conv2 = nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1) self.bn2 = nn.BatchNorm2d(ch_out) self.extra = nn.Sequential() if ch_out!= ch_in: self.extra=nn.Sequential( nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=stride), nn.BatchNorm2d(ch_out) ) def forward(self,x): &#x27;&#x27;&#x27; :param x: [batchsz,ch,h,w] :return: &#x27;&#x27;&#x27; out = F.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) # short_cut ### 这里一定要写extra！！！ # extra module : [batchsz,ch_in,h,w] =&gt; [batchsz,ch_out,h,w] # element-wise add: out = self.extra(x) + out return out 实现ResNet18网络结构 代码原理和实现LeNet-5一样，这里不再赘述 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class ResNet18(nn.Module): def __init__(self): super(ResNet18, self).__init__() #以下conv1为预处理层 self.conv1 = nn.Sequential( nn.Conv2d(3,64,kernel_size=3,stride=3,padding=0), nn.BatchNorm2d(64) ) # followed 4 blocks # [batchsz,64,h,w] =&gt; [batchsz,128,h,w] self.blk1=ResBlk(64,128,stride=2) # [batchsz,128,h,w] =&gt; [batchsz,256,h,w] self.blk2=ResBlk(128,256,stride=2) # [batchsz,256,h,w] =&gt; [batchsz,512,h,w] self.blk3=ResBlk(256,512,stride=2) # [batchsz,512,h,w] =&gt; [batchsz,512,h,w] self.blk4=ResBlk(512,512,stride=2) self.outlayer = nn.Linear(512, 10) def forward(self,x): &#x27;&#x27;&#x27; :param x: :return: &#x27;&#x27;&#x27; x = F.relu(self.conv1(x)) # [batchsz,64,h,w] =&gt; [batchsz,512,h,w] x = self.blk1(x) x = self.blk2(x) x = self.blk3(x) x = self.blk4(x) # 测试代码 # print(&#x27;after convolution&#x27;,x.shape) # [batchsz,512,2,2] # [batchsz,512,h,w] =&gt; [batchsz,512,1,1] x =F.adaptive_max_pool2d(x,[1,1]) # 测试代码 # print(&#x27;after pool:&#x27;, x.shape) # Flatten x=x.view(x.size(0),-1) # Linear x = self.outlayer(x) return x 注意：中间还是有一个Flatten操作 可以使用上述代码中被注释掉的`#测试代码`部分打出中间层tensor的形状，这样有助于掌握网络的形状。 后续代码 训练部分和Val验证和上面的LeNet-5实现代码完全一样，我们只需要把load的模型更改一下即可。首先导入ResNet 1from ResNet import ResNet18 然后导入模型的地方稍作修改就OK了： 1234# 导入模型# model = LeNet5().to(device) # 将模型放到显卡上model = ResNet18().to(device) # 将模型放到显卡上print(model) 代码优化 归一化&amp;数据增强 数据预处理时加入归一化Normalize提升模型性能 当然也可以在数据处理部分加入数据增强来稍微提高模型性能 12345678910111213141516171819cifar_train =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, # 这个True代表是训练集 transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]) ]), download=True )cifar_train = DataLoader(cifar_train,batch_size=batchsz,shuffle=True)cifar_test =datasets.CIFAR10(&#x27;cifar_data&#x27;,True, transform=transforms.Compose([ transforms.Resize([32,32]), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]), download=True ) 代码汇总 main.py 同上一个汇总的main，按照上面的指示改一点就行。 ResNet.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import torchfrom torch import nnimport torch.nn.functional as Fclass ResBlk(nn.Module): &#x27;&#x27;&#x27; ResNet Block &#x27;&#x27;&#x27; def __init__(self,ch_in,ch_out,stride=1): &#x27;&#x27;&#x27; :param ch_in :param ch_out &#x27;&#x27;&#x27; super(ResBlk,self).__init__() self.conv1 = nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=stride,padding=1) self.bn1 = nn.BatchNorm2d(ch_out) self.conv2 = nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1) self.bn2 = nn.BatchNorm2d(ch_out) self.extra = nn.Sequential() if ch_out!= ch_in: self.extra=nn.Sequential( nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=stride), nn.BatchNorm2d(ch_out) ) def forward(self,x): &#x27;&#x27;&#x27; :param x: [batchsz,ch,h,w] :return: &#x27;&#x27;&#x27; out = F.relu(self.bn1(self.conv1(x))) out = self.bn2(self.conv2(out)) # short_cut ### 这里一定要写extra！！！ # extra module : [batchsz,ch_in,h,w] =&gt; [batchsz,ch_out,h,w] # element-wise add: out = self.extra(x) + out return outclass ResNet18(nn.Module): def __init__(self): super(ResNet18, self).__init__() #以下conv1为预处理层 self.conv1 = nn.Sequential( nn.Conv2d(3,64,kernel_size=3,stride=3,padding=0), nn.BatchNorm2d(64) ) # followed 4 blocks # [batchsz,64,h,w] =&gt; [batchsz,128,h,w] self.blk1=ResBlk(64,128,stride=2) # [batchsz,128,h,w] =&gt; [batchsz,256,h,w] self.blk2=ResBlk(128,256,stride=2) # [batchsz,256,h,w] =&gt; [batchsz,512,h,w] self.blk3=ResBlk(256,512,stride=2) # [batchsz,512,h,w] =&gt; [batchsz,512,h,w] self.blk4=ResBlk(512,512,stride=2) self.outlayer = nn.Linear(512, 10) def forward(self,x): &#x27;&#x27;&#x27; :param x: :return: &#x27;&#x27;&#x27; x = F.relu(self.conv1(x)) # [batchsz,64,h,w] =&gt; [batchsz,512,h,w] x = self.blk1(x) x = self.blk2(x) x = self.blk3(x) x = self.blk4(x) # 测试代码 # print(&#x27;after convolution&#x27;,x.shape) # [batchsz,512,2,2] # [batchsz,512,h,w] =&gt; [batchsz,512,1,1] x =F.adaptive_max_pool2d(x,[1,1]) # 测试代码 # print(&#x27;after pool:&#x27;, x.shape) # Flatten x=x.view(x.size(0),-1) # Linear x = self.outlayer(x) return xdef main(): # 以下为测试代码 tmp = torch.randn(2,3,32,32) model=ResNet18() blk=ResBlk(3,128,stride=4) out = blk(tmp) out = model(tmp) print(out.shape)if __name__==&#x27;__main__&#x27;: main()"},{"title":"PyTorch CNN","path":"/wiki/PyTorch/PyTorch CNN.html","content":"卷积神经网络结构介绍 如果用全连接神经网络处理大尺寸图像具有三个明显的缺点： （1）首先将图像展开为向量会丢失空间信息； （2）其次参数过多效率低下，训练困难； （3）同时大量的参数也很快会导致网络过拟合。 而使用卷积神经网络可以很好地解决上面的三个问题。 卷积神经网络的提出参考了人眼的局部相关性，人眼用于识别物体也是先看一个局部，根据一些特称才能辨别这个物体是什么，特征的识别和像素点的排列位置和情况有关。 根据局部相关性提出的卷积神经网络结构 卷积运算 首先我们使用几张图来直观感受一下卷积是如何运算的 最左边的那个叫做卷积核 更多更详细关于卷积神经网络的基础知识可以访问如下这篇博客：深度学习：卷积神经网络（CNN 常用卷积核 概念解析 输入通道（Input_channels）:输入有多少个通道（多少层输入），比如如果我们使用的是黑白照片，那么就只有一个通道；如果我们使用的是rgb彩色照片那么就有三个通道。 **核通道（Kernel_channels）:**卷积核有多少个通道数量。 核大小（Kernel_size）:卷积核的大小，一般是3×3 步长（Stride）:每次卷积核移动的步长，一般默认是1 填充（Padding）:为了保证卷积运算每次卷积完成以后输入输出图片大小相同，我们可能需要在图片周围填充一圈0。 对于每一个卷积核，其所包含的通道数必须和输入的卷积层的通道数一样！所以上面核通道有两种含义，一种是指每一个卷积核中所含的通道数，还有一种含义是指有多少个卷积核。但是一般情况下都是指后者。 因此对于b个28×28，3通道的输入x: [b,3,28,28]如果我们采用16个3*3的卷积核，则对于一个卷积核他的size是one-k: [3,3,3],对于16个k的总size是multi-k: [16,3,3,3],因为有16个卷积核所以偏置也有16个bias: [16](这里运用了广播)。最终的输出是out: [b,16,28,28]（考虑了padding） PyTorch实现 了解了基本的卷积神经网络后，下面我们尝试使用PyTorch简单的实现一个简单的二维卷积神经网络，所使用的函数是nn.Conv2d其中第一个参数表示的是输入的通道数，后面的参数表示的是输出的通道数。 123456789101112131415161718In [3]: import torch.nn as nnIn [4]: layer=nn.Conv2d(1,3,kernel_size=3,stride=1,padding=0)In [5]: x=torch.rand(123,1,28,28)In [6]: out=layer.forward(x)In [7]: out.shapeOut[7]: torch.Size([123, 3, 26, 26])In [8]: layer=nn.Conv2d(1,12,kernel_size=3,stride=1,padding=1)In [9]: out=layer.forward(x)In [10]: out.shapeOut[10]: torch.Size([123, 12, 28, 28])In [11]: layer=nn.Conv2d(1,12,kernel_size=3,stride=2,padding=1)In [12]: out=layer.forward(x)In [13]: out.shapeOut[13]: torch.Size([123, 12, 14, 14])In [14]: out=layer(x) # 强烈推荐这一种不使用forward的方法In [15]: out.shapeOut[15]: torch.Size([123, 12, 14, 14]) 我们也可以使用下面的代码来看看最后一次这个layer中存放了什么样的weight和bias(接上面的代码) 1234567891011121314151617181920212223242526272829303132333435363738394041424344In [16]: layer.weightOut[16]: Parameter containing:tensor([[[[-0.2432, -0.2371, 0.2480], [-0.2706, -0.2552, -0.0627], [ 0.1960, -0.3049, -0.0273]]], [[[-0.3008, 0.1268, 0.0518], [ 0.0673, 0.2716, -0.1481], [ 0.1075, -0.2905, 0.1106]]], [[[ 0.1498, -0.0681, -0.1162], [ 0.3015, 0.1400, 0.0372], [-0.1401, 0.1158, -0.0872]]], [[[ 0.1543, -0.0323, -0.2023], [ 0.2972, -0.0344, 0.3114], [ 0.2141, -0.0388, -0.0352]]], [[[ 0.1029, 0.0184, -0.2372], [ 0.2758, -0.1239, 0.2429], [-0.0263, -0.2977, -0.2075]]], [[[ 0.2166, -0.1031, -0.2122], [ 0.2123, -0.1492, -0.0831], [-0.0155, -0.0572, -0.2485]]], [[[-0.0441, -0.3298, 0.2896], [ 0.1438, 0.1364, -0.2721], [ 0.0076, 0.0919, -0.1219]]], [[[ 0.3241, 0.1343, 0.0140], [ 0.2732, -0.1690, -0.3122], [ 0.2933, 0.0007, 0.1548]]], [[[ 0.2695, 0.3256, 0.0424], [-0.2905, -0.1272, -0.1482], [-0.1993, 0.1244, 0.1924]]], [[[ 0.1136, -0.0931, -0.2240], [-0.0278, -0.1714, -0.0614], [-0.0067, -0.1385, -0.2265]]], [[[ 0.3065, 0.1684, -0.0113], [-0.0534, 0.0034, 0.0194], [-0.1013, 0.0573, -0.2356]]], [[[-0.1154, -0.0406, 0.0382], [-0.3318, -0.2613, 0.0636], [-0.2066, 0.3052, -0.2338]]]], requires_grad=True)In [17]: layer.weight.shapeOut[17]: torch.Size([12, 1, 3, 3])In [18]: layer.bias.shapeOut[18]: torch.Size([12]) 以上是类API的写法，PyTorch也提供函数API，具体代码如下： 1234567891011In [3]: import torch.nn.functional as FIn [4]: w=torch.rand(16,3,5,5,requires_grad=True)In [5]: b=torch.rand(16,requires_grad=True)In [6]: x=torch.rand(321,3,28,28)In [7]: out=F.conv2d(x,w,b,stride=1,padding=1)In [8]: out.shapeOut[8]: torch.Size([321, 16, 26, 26])In [9]: out=F.conv2d(x,w,b,stride=2,padding=2)In [10]: out.shapeOut[10]: torch.Size([321, 16, 14, 14]) 池化层与采样 下采样（Downsample） 就是将下面的这个大矩阵变成上面的那个小矩阵，其实下采样和是缩小图片有点像 Max Pooling Max Pooling和下采样类似，但是是每次观察一个区域后取该区域值最大的值为该区域的采样值 类似的还有AvgPooling PyTorch 实现 PyTorch同样提供两种类型的API，这里所使用的函数是nn.MaxPool2d和nn.AvgPool2d和avg_pool2d和max_pool2d 123456789101112131415161718In [3]: import torch.nn as nnIn [4]: x=torch.rand(123,16,14,14)In [5]: layer=nn.MaxPool2d(2,stride=2)In [6]: out=layer(x)In [7]: out.shapeOut[7]: torch.Size([123, 16, 7, 7])In [8]: layer=nn.AvgPool2d(2,stride=2)In [9]: out=layer(x)In [10]: out.shapeOut[10]: torch.Size([123, 16, 7, 7])In [11]: import torch.nn.functional as FIn [12]: out=F.max_pool2d(x,2,stride=2)In [13]: out.shapeOut[13]: torch.Size([123, 16, 7, 7])In [14]: out=F.avg_pool2d(x,2,stride=2)In [15]: out.shapeOut[15]: torch.Size([123, 16, 7, 7]) 上采样（Upsample） 就是将上面的这个小矩阵变成下面的那个大矩阵，相当于图片的放大，和下采样正好相反 PyTorch 实现 上采样所使用的函数是interpolate函数 123456789101112131415161718192021222324252627282930313233In [3]: import torch.nn.functional as FIn [4]: x=torch.rand(1,1,3,3)In [5]: xOut[5]: tensor([[[[0.0565, 0.2834, 0.3705], [0.4337, 0.8775, 0.8109], [0.8081, 0.8048, 0.3552]]]])In [6]: out=F.interpolate(x,scale_factor=2,mode=&#x27;nearest&#x27;)In [7]: outOut[7]: tensor([[[[0.0565, 0.0565, 0.2834, 0.2834, 0.3705, 0.3705], [0.0565, 0.0565, 0.2834, 0.2834, 0.3705, 0.3705], [0.4337, 0.4337, 0.8775, 0.8775, 0.8109, 0.8109], [0.4337, 0.4337, 0.8775, 0.8775, 0.8109, 0.8109], [0.8081, 0.8081, 0.8048, 0.8048, 0.3552, 0.3552], [0.8081, 0.8081, 0.8048, 0.8048, 0.3552, 0.3552]]]])In [8]: out.shapeOut[8]: torch.Size([1, 1, 6, 6])In [9]: out=F.interpolate(x,scale_factor=3,mode=&#x27;nearest&#x27;)In [10]: outOut[10]: tensor([[[[0.0565, 0.0565, 0.0565, 0.2834, 0.2834, 0.2834, 0.3705, 0.3705, 0.3705], [0.0565, 0.0565, 0.0565, 0.2834, 0.2834, 0.2834, 0.3705, 0.3705, 0.3705], [0.0565, 0.0565, 0.0565, 0.2834, 0.2834, 0.2834, 0.3705, 0.3705, 0.3705], [0.4337, 0.4337, 0.4337, 0.8775, 0.8775, 0.8775, 0.8109, 0.8109, 0.8109], [0.4337, 0.4337, 0.4337, 0.8775, 0.8775, 0.8775, 0.8109, 0.8109, 0.8109], [0.4337, 0.4337, 0.4337, 0.8775, 0.8775, 0.8775, 0.8109, 0.8109, 0.8109], [0.8081, 0.8081, 0.8081, 0.8048, 0.8048, 0.8048, 0.3552, 0.3552, 0.3552], [0.8081, 0.8081, 0.8081, 0.8048, 0.8048, 0.8048, 0.3552, 0.3552, 0.3552], [0.8081, 0.8081, 0.8081, 0.8048, 0.8048, 0.8048, 0.3552, 0.3552, 0.3552]]]])In [11]: out.shapeOut[11]: torch.Size([1, 1, 9, 9]) ReLU 卷积神经网络的ReLU函数和之前全连接神经网络所说的ReLU函数一样，在这里是被用于去除响应过小的点。 PyTorch实现 和原来全连接神经网络使用的函数一模一样 1234567891011In [3]: import torch.nn as nnIn [4]: import torch.nn.functional as FIn [5]: x=torch.rand(1,16,7,7)In [6]: layer=nn.ReLU(inplace=True)In [7]: out=layer(x)In [8]: out.shapeOut[8]: torch.Size([1, 16, 7, 7])In [9]: out=F.relu(x)In [10]: out.shapeOut[10]: torch.Size([1, 16, 7, 7]) Batch-Norm（归一化） norm其实归一化就是特征缩放（feature scaling），一般情况我们将特征缩放成以0为均值1为方差的情况。 Batch norm在卷积神经网络中是指，对所有样本的相同层进行归一化处理。 γ,β\\gamma,\\betaγ,β是要参与反向传播的，μ,α\\mu,\\alphaμ,α是不参与反向传播的，是运行中统计出来的数据。当前μ,α\\mu,\\alphaμ,α的计算也和上一次μ,α\\mu,\\alphaμ,α的值有关（指数加权平均） PyTorch 实现 下面实现一个全连接层的Batchnorm，所使用的函数是torch.nn.BatchNorm1d,输入参数表示有多少个特征，在下面的例子中是有16个特征 1234567891011121314In [3]: x=torch.rand(100,16)+0.5In [4]: layer=torch.nn.BatchNorm1d(16)In [5]: layer.running_mean,layer.running_varOut[5]: (tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))In [6]: out=layer(x)In [7]: layer.running_mean,layer.running_varOut[7]: (tensor([0.0961, 0.0963, 0.1018, 0.1019, 0.1005, 0.1021, 0.1000, 0.1006, 0.0965, 0.1005, 0.1054, 0.0996, 0.0940, 0.1024, 0.0938, 0.1022]), tensor([0.9089, 0.9086, 0.9073, 0.9105, 0.9075, 0.9087, 0.9068, 0.9079, 0.9096, 0.9084, 0.9087, 0.9086, 0.9073, 0.9090, 0.9072, 0.9097])) running_mean,running_var统计的是当前输入的数据的均值和方差的 可以发现running_mean一开始并没有直接到真实数据的均值，这就是因为每一次计算的均值和前一次的均值是有关系的缘故。所以当我们这样计算多次后running_mean才会逐渐接近真实的均值（见下面的代码） 123456789101112In [3]: x=torch.randn(100,16)+0.5In [4]: layer=torch.nn.BatchNorm1d(16) In [5]: for i in range(100): out=layer(x)In [6]: layer.running_meanOut[6]: tensor([0.5601, 0.4520, 0.2556, 0.4875, 0.4371, 0.3810, 0.4296, 0.4426, 0.5163, 0.5468, 0.4442, 0.3907, 0.4959, 0.5350, 0.5775, 0.3721])In [7]: layer.running_varOut[7]: tensor([0.8751, 0.9497, 0.8256, 0.9304, 0.9371, 0.8656, 0.8312, 0.9728, 0.9176, 0.9641, 1.2020, 0.8723, 1.3406, 0.8414, 0.9326, 0.9007]) 下面我们进一步来看看BatchNorm对于2d的数据是如何进行操作的，这里所使用的函数是BatchNorm2d，参数代表的含义是有多少个通道。 123456789101112131415161718192021222324252627In [3]: import torch.nn as nnIn [4]: x=torch.randn(3,16,7,7)In [5]: layer=nn.BatchNorm2d(16)In [6]: out=layer(x)In [7]: out.shapeOut[7]: torch.Size([3, 16, 7, 7])In [8]: layer.weightOut[8]: Parameter containing:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)In [9]: layer.biasOut[9]: Parameter containing:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)In [10]: layer.running_meanOut[10]: tensor([-3.1731e-03, -1.3501e-03, -2.2491e-03, 9.6934e-05, -2.5191e-03, -4.5400e-03, -1.0107e-02, 1.9353e-03, -1.2692e-02, 8.8569e-03, 9.0268e-04, -1.6005e-03, -8.5016e-03, -9.6128e-03, 1.5821e-03, 1.9608e-02])In [11]: layer.running_varOut[11]: tensor([1.0157, 0.9935, 0.9906, 1.0245, 1.0056, 1.0123, 1.0102, 1.0013, 0.9842, 0.9849, 0.9979, 0.9973, 1.0035, 0.9945, 0.9796, 1.0021]) `layer.weight`代表的是上面原理图中的$\\gamma$,`layer.bias`代表的是上面原理图中的$\\beta$，`layer.running_mean`代表的是$\\mu$，`layer.running_var`代表的是σ\\sigmaσ 使用vars可以打出层的所有信息。 1234567891011121314151617181920212223242526272829303132In [12]: vars(layer)Out[12]: &#123;&#x27;training&#x27;: True, &#x27;_parameters&#x27;: OrderedDict([(&#x27;weight&#x27;, Parameter containing: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)), (&#x27;bias&#x27;, Parameter containing: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True))]), &#x27;_buffers&#x27;: OrderedDict([(&#x27;running_mean&#x27;, tensor([-3.1731e-03, -1.3501e-03, -2.2491e-03, 9.6934e-05, -2.5191e-03, -4.5400e-03, -1.0107e-02, 1.9353e-03, -1.2692e-02, 8.8569e-03, 9.0268e-04, -1.6005e-03, -8.5016e-03, -9.6128e-03, 1.5821e-03, 1.9608e-02])), (&#x27;running_var&#x27;, tensor([1.0157, 0.9935, 0.9906, 1.0245, 1.0056, 1.0123, 1.0102, 1.0013, 0.9842, 0.9849, 0.9979, 0.9973, 1.0035, 0.9945, 0.9796, 1.0021])), (&#x27;num_batches_tracked&#x27;, tensor(1))]), &#x27;_non_persistent_buffers_set&#x27;: set(), &#x27;_backward_hooks&#x27;: OrderedDict(), &#x27;_is_full_backward_hook&#x27;: None, &#x27;_forward_hooks&#x27;: OrderedDict(), &#x27;_forward_pre_hooks&#x27;: OrderedDict(), &#x27;_state_dict_hooks&#x27;: OrderedDict(), &#x27;_load_state_dict_pre_hooks&#x27;: OrderedDict(), &#x27;_modules&#x27;: OrderedDict(), &#x27;num_features&#x27;: 16, &#x27;eps&#x27;: 1e-05, &#x27;momentum&#x27;: 0.1, &#x27;affine&#x27;: True, &#x27;track_running_stats&#x27;: True&#125; 因为Batchnorm层在训练模式和测试模式下的行为有所差别，所以在进行测试时，要使用eval把Batchnorm层的状态转换过来（从训练模式转换为测试模式）。因为test的时候只有一个样本，所以μ,σ2\\mu,\\sigma^2μ,σ2是无法被统计的。一般这个时候这两个值会被赋值为全局的值。 合理使用BatchNorm可以加快收敛速度并且提高准确度，而且使用了以后模型的鲁棒性会提升（模型更加稳定） 经典的卷积神经网络 其中几个转折点 AlexNet(2012) ZFNet(2013) VGG(2014) GoogLeNet(2014) ResNet※ LeNet-5 LeNet由Yann Lecun 提出，是一种经典的卷积神经网络，是现代卷积神经网络的起源之一。Yann将该网络用于邮局的邮政的邮政编码识别，有着良好的学习和识别能力。LeNet又称LeNet-5,具有一个输入层，两个卷积层，两个池化层，3个全连接层（其中最后一个全连接层为输出层）。 LeNet-5是一种经典的卷积神经网络结构，于1998年投入实际使用中。该网络最早应用于手写体字符识别应用中。普遍认为，卷积神经网络的出现开始于LeCun 等提出的LeNet 网络（LeCun et al., 1998），可以说LeCun 等是CNN 的缔造者，而LeNet-5 则是LeCun 等创造的CNN 经典之作 。 网络结构如下图所示，先是一个卷积层，然后是一个下采样，然后又是一个卷积层，然后又是一个下采样，最后三个就是三个全连接层。 AlexNet AlexNet是2012年ImageNet竞赛冠军获得者Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网络被提出，比如优秀的VGG，GoogLeNet。 这对于传统的机器学习分类算法而言，已经相当的出色。 AlexNet中包含了几个比较新的技术点，也首次在CNN中成功应用了ReLU、Dropout和LRN等Trick。同时AlexNet也使用了GPU进行运算加速。 AlexNet将LeNet的思想发扬光大，把CNN的基本原理应用到了很深很宽的网络中。AlexNet主要使用到的新技术点如下： （1）成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。 （2）训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。 （3）在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。 （4）提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 （5）使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。 （6）数据增强，随机地从256256的原始图像中截取224224大小的区域（以及水平翻转的镜像），相当于增加了2*(256-224)^2=2048倍的数据量。如果没有数据增强，仅靠原始的数据量，参数众多的CNN会陷入过拟合中，使用了数据增强后可以大大减轻过拟合，提升泛化能力。进行预测时，则是取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，对他们进行预测并对10次结果求均值。同时，AlexNet论文中提到了会对图像的RGB数据进行PCA处理，并对主成分做一个标准差为0.1的高斯扰动，增加一些噪声，这个Trick可以让错误率再下降1%。 模型特点 使用了ReLU激活函数 使用了最大池化层 标准化 Dropout VGG VGG模型是2014年ILSVRC竞赛的第二名，第一名是GoogLeNet。但是VGG模型在多个迁移学习任务中的表现要优于GoogLeNet。而且，从图像中提取CNN特征，VGG模型是首选算法。它的缺点在于，参数量有140M之多，需要更大的存储空间。但是这个模型很有研究价值。 模型的名称——“VGG”代表了牛津大学的Oxford Visual Geometry Group，该小组隶属于1985年成立的Robotics Research Group，该Group研究范围包括了机器学习到移动机器人。下面是一段来自网络对同年GoogLeNet和VGG的描述： “GoogLeNet和VGG的Classification模型从原理上并没有与传统的CNN模型有太大不同。大家所用的Pipeline也都是：训练时候：各种数据Augmentation（剪裁，不同大小，调亮度，饱和度，对比度，偏色），剪裁送入CNN模型，Softmax，Backprop。测试时候：尽量把测试数据又各种Augmenting（剪裁，不同大小），把测试数据各种Augmenting后在训练的不同模型上的结果再继续Averaging出最后的结果。” 需要注意的是，在VGGNet的6组实验中，后面的4个网络均使用了pre-trained model A的某些层来做参数初始化。虽然提出者没有提该方法带来的性能增益。先来看看VGG的特点： 小卷积核。作者将卷积核全部替换为3x3（极少用了1x1）； 小池化核。相比AlexNet的3x3的池化核，VGG全部为2x2的池化核； 层数更深特征图更宽。基于前两点外，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓； 全连接转卷积。网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高为的输入。 GoogLeNet GoogLeNet是2014年Christian Szegedy提出的一种全新的深度学习结构，在这之前的AlexNet、VGG等结构都是通过增大网络的深度（层数）来获得更好的训练效果，但层数的增加会带来很多负作用，比如overfit、梯度消失、梯度爆炸等。inception的提出则从另一种角度来提升训练结果：能更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果。 模型特点 使用不同大小的卷积核，从而感受不同范围的视野 ResNet（深度残差网络）※ 这里单独将ResNet新开一个章节，已经表明了他在这一领域的重要性。首先来看看ResNet在2015年的战绩： 从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，所以当模型更深时理论上可以取得更好的结果，但是更深的网络其性能一定会更好吗？实验发现深度网络出现了退化问题（Degradation problem）：网络深度增加时，网络准确度出现饱和，甚至出现下降。56层的网络比20层网络效果还要差。这不会是过拟合问题，因为56层网络的训练误差同样高。我们知道深层网络存在着梯度消失或者爆炸的问题，这使得深度学习模型很难训练。但是现在已经存在一些技术手段如BatchNorm来缓解这个问题。因此，出现深度网络的退化问题是非常令人诧异的。但是何恺明发明的ResNet有效的解决了这一问题。ResNet有效解决了深度CNN模型难训练的问题（网络太深了容易发生梯度弥散） ResNet残差网络主要是通过残差块组成的，在提出残差网络之前，网络结构无法很深，在VGG中，卷积网络达到了19层，在GoogLeNet中，网络达到了22层。随着网络层数的增加，网络发生了退化（degradation）的现象：随着网络层数的增多，训练集loss逐渐下降，然后趋于饱和，当你再增加网络深度的话，训练集loss反而会增大。而引入残差块后，网络可以达到很深，网络的效果也随之变好。 这里提供了一种想法：既然深层网络相比于浅层网络具有退化问题，那么是否可以保留深层网络的深度，又可以有浅层网络的优势去避免退化问题呢？如果将深层网络的后面若干层学习成恒等映射 h(x)=xh(x)=xh(x)=x ，那么模型就退化成浅层网络。但是直接去学习这个恒等映射是很困难的，那么就换一种方式，把网络设计成： H(x)=F(x)+x⇒F(x)=H(x)−xH(x)=F(x)+x \\Rightarrow F(x)=H(x)-x H(x)=F(x)+x⇒F(x)=H(x)−x 只要 F(x)=0F(x)=0F(x)=0 就构成了一个恒等映射 H(x)=xH(x)=xH(x)=x ，这里 F(x)F(x)F(x) 为残差。 相关资料链接：深度学习之16——残差网络(ResNet) PyTorch 残差块实现 12345678910111213141516171819class ResBlk(nn.Module): def __init__(self,ch_in,ch_out): self.conv1=nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1) self.bn1=nn.BatchNorm2d(ch_out) self.conv2=nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1) self.bn2=nn.BatchNorm2d(ch_out) self.extra=nn.Sequential() if ch_out!=ch_in: # 如果输入输出通道数量不一样的话 self.extra!=ch_in: self.extra=nn.Sequential( nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1), nn.BatchNorm2s(ch_out) ) def forward(self,x): out=F.relu(self.bn1(self.conv1(x))) out=self.bn2(self.conv2(out)) out=self.extra(x)+out return out DenseNet 其实相当于每一层都和前面所有层之间又一个shortcut nn.Module PyTorch中的nn.Module类为所有我们自定义网络层的一个父类！所以他非常的重要，以下是他的优点 提供了很多的操作 nn.Linear nn.BatchNorm2d nn.Conv2d etc. 提供容器nn.Sequential 对网络参数能够进行很好的管理 下面举一个例子，我们采用的是一个两层的网络，所以参数有4组，weight0，bias0,weight1,bias1 123456789101112131415161718192021In [3]: import torch.nn as nnIn [4]: net=nn.Sequential(nn.Linear(4,2),nn.Linear(2,2))In [5]: dict(net.named_parameters()).items()Out[5]: dict_items([(&#x27;0.weight&#x27;, Parameter containing:tensor([[-0.1411, 0.2698, 0.0457, -0.2939], [-0.3555, -0.3352, -0.3403, 0.4714]], requires_grad=True)), (&#x27;0.bias&#x27;, Parameter containing:tensor([ 0.1076, -0.1325], requires_grad=True)), (&#x27;1.weight&#x27;, Parameter containing:tensor([[ 0.0859, -0.6313], [-0.6823, -0.5285]], requires_grad=True)), (&#x27;1.bias&#x27;, Parameter containing:tensor([ 0.6793, -0.1800], requires_grad=True))])In [6]: list(net.parameters())[0]Out[6]: Parameter containing:tensor([[-0.1411, 0.2698, 0.0457, -0.2939], [-0.3555, -0.3352, -0.3403, 0.4714]], requires_grad=True)In [7]: list(net.parameters())[1]Out[7]: Parameter containing:tensor([ 0.1076, -0.1325], requires_grad=True)In [8]: optimizer=optim.SGD(net.parameters(),lr=1e-3) 可以清晰的查看网络结构 可以将网络方便的转移到GPU上进行加速 123device=torch,device(&#x27;cuda&#x27;)net=Net()net.to(device) 可以很方便的保存和加载网络的中间状态 这就方便我们进行early stop 123456789device=torch,device(&#x27;cuda&#x27;)net=Net()net.to(device)net.load_state_dict(torch.load(&#x27;ckpt.mdl&#x27;)) # 加载模型# train...torch.save(net.state_dict(),&#x27;ckpt.mdl&#x27;) # 保存模型 方便切换网络状态 对于网络中的一些层，比如BatchNorm层，他在训练状态下和测试状态下的行为是有一些差异的，如果我们对网络中的每一层都去执行切换状态的操作是非常麻烦的，但是nn.Module支持对自定义网络整体状态的切换，大大简化了操作。 12345678910111213device = torch.device(&#x27;cuda&#x27;)net=Net()net.to(device)# trainnet.train()# ...# ...# ...# testnet.eval()# ... 方便定义自己的类 比如说PyTorch现目前暂时不提供将tensor拍平的操作（层之间，作用是将卷积层转化为全连接层），因此这个层需要我们自己去实现 12345class Flatten(nn.Module): def __init__(self): super(Flatten,self).__init__() def forward(self,input): return input.view(input.size(0),-1) 以上这个类使用的非常的广泛 我们也可以尝试自己写一个Linear类 123456789101112class MyLinear(nn.Module): def __init__(self,inp,outp): super(MyLinear,self).__init__() # requiers_grad = True self.w = nn.Parameter(torch.randn(outp,inp)) self.b = nn.Parameter(torch.randn(outp)) def forward(self, x): x = x @ self.w.t() + self.b return x 数据增强 其实非常好理解，数据增强让有限的数据产生更多的数据，增加训练样本的数量以及多样性（噪声数据），提升模型鲁棒性，一般用于训练集。神经网络需要大量的参数，许许多多的神经网路的参数都是数以百万计，而使得这些参数可以正确工作则需要大量的数据进行训练，但在很多实际的项目中，我们难以找到充足的数据来完成任务。随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。 常用的数举增强方法有 翻转 旋转 随机移动和裁剪 加噪 GAN 翻转（Flip） PyTorch 实现 12345678910train_loader=torch.utils.dataDataLoader( datasets.MNIST(&#x27;../data&#x27;,train=True,download=True, transform=transforms.Compose([ transforms.RandomHorizontalFlip(),# 因为前面是random，所以该操作有可能做也可能不做 transforms.RandomVerticalFlip(), transforms.ToTensor(), # transforms.Normalize((0.1307,),(0.3081,)) ])), batch_size=batch_size,shuffle=True) transform是torchvision中提供的操作 旋转（Rotate） PyTorch实现 123456789101112train_loader=torch.utils.dataDataLoader( datasets.MNIST(&#x27;../data&#x27;,train=True,download=True, transform=transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(15),# 这里是对于每张照片随机旋转一个角度，角度的范围是从-15°到15° transforms.(RandomRotation([90,180,270]))# 这里是对于每张照片随机旋转一个角度，角度是 90°或180°或270° transforms.ToTensor(), # transforms.Normalize((0.1307,),(0.3081,)) ])), batch_size=batch_size,shuffle=True) 缩放（scale） PyTorch 实现 12345678910111213train_loader=torch.utils.dataDataLoader( datasets.MNIST(&#x27;../data&#x27;,train=True,download=True, transform=transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(15), transforms.(RandomRotation([90,180,270])) transforms.Resize([32,32]) # 将大小变为32×32 transforms.ToTensor(), # transforms.Normalize((0.1307,),(0.3081,)) ])), batch_size=batch_size,shuffle=True) 随机移动和裁剪（Crop part） PyTorch 实现 1234567891011121314train_loader=torch.utils.dataDataLoader( datasets.MNIST(&#x27;../data&#x27;,train=True,download=True, transform=transforms.Compose([ transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(15), transforms.(RandomRotation([90,180,270])) transforms.Resize([32,32]) # 将大小变为32×32 transforms.ToTensor(), transforms.RandomCrop([28,28]) # transforms.Normalize((0.1307,),(0.3081,)) ])), batch_size=batch_size,shuffle=True) 一般进行数据增强是RandomRotation和RandomCrop结合起来一起使用的 加噪（Noise） 总结 虽然数据增强确实可以提高模型的表现，但是他不会帮助太多。"},{"title":"PyTorch RNN&LSTM","path":"/wiki/PyTorch/PyTorch RNN&LSTM.html","content":"序列表示（Sequence representation） 方法一 对于一个句子，我们采用如下的表示方式： [seq_len,feature_len][seq\\_len,feature\\_len] [seq_len,feature_len] seq_lenseq \\_ lenseq_len表示的是这个句子所含的单词数量，feature_lenfeature \\_ lenfeature_len表示的是表示一个单词所需向量的长度（一般采用独热编码）。 缺点：独热码使得矩阵变得稀疏，占用了大量存储空间的同时，表达的信息海非常少，这不是我们想看到的结果。 方法二 为了让特征向量变得不那么稀疏，我们不采用独热编码方式。我们还是可以将要使用的单词使用向量编码，只不过这次向量编码有效的利用了语义相关性，语义相关性高的两个向量，他们的夹角更小，反之，夹角会非常大。现目前已经有两种技术我们可以使用来进行这种方式的编码。 Word2vec GloVe 因为不会深入nlp部分，所以我们暂时不详细介绍这两种方式，大家有兴趣可以自行百度。 方法二也是现目前使用最广泛的一种方法。 PyTorch实现 使用PyTorch 建立索引特征向量表并根据索引查询对应单词的特征向量 12345678910import torchfrom torch import nnword_to_ix=&#123;&quot;hello&quot;: 0,&quot;world&quot; :1&#125;lookup_tensor = torch.tensor([word_to_ix[&quot;hello&quot;]],dtype=torch.long)embeds=nn.Embedding(2,5)hello_embed=embeds(lookup_tensor)print(hello_embed) 运行结果 12tensor([[-0.4940, -0.9166, 1.2154, 0.4011, -0.6101]], grad_fn=&lt;EmbeddingBackward0&gt;) PyTorch 导入GloVe GloVe是一种nlp中常用的单词的向量编码。 在PyTorch中使用GloVe编码的代码如下： 1234from torchnlp.word_to_vector import GloVevectors=GloVe()print(vectors[&#x27;hello&#x27;]) # 然后就可以得到向量表示的结果 RNN原理 所具备的能力 能处理长句子，普通的全连接神经网络不行，因为这样w,b参数太多了。解决方式是采用了权值共享 能够连接上下文，具有上下文语境信息（语境贯穿） 具体一般情况下的表达式如下： ht=fw(ht−1,xt)h_t=f_w(h_{t-1},x_t) ht​=fw​(ht−1​,xt​) ht=tanh(Whhht−1+Wxhxt)(there are two bias!)h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)\\qquad(there\\ are \\ two \\ bias!) ht​=tanh(Whh​ht−1​+Wxh​xt​)(there are two bias!) yt=Whyhty_t=W_{hy}h_t yt​=Why​ht​ 反向传播更新梯度的公式 ∂Et∂Whh=∑i=0t∂Et∂yt∂yt∂ht∂ht∂hi∂hi∂Whh\\frac{\\partial E_t}{\\partial W_{hh}}=\\sum_{i=0}^t \\frac{\\partial E_t}{\\partial y_t}\\frac{\\partial y_t}{\\partial h_t}\\frac{\\partial h_t}{\\partial h_i}\\frac{\\partial h_i}{\\partial W_{hh}} ∂Whh​∂Et​​=i=0∑t​∂yt​∂Et​​∂ht​∂yt​​∂hi​∂ht​​∂Whh​∂hi​​ 如果我们采用[seq_len,batch_sz,feature_len][seq\\_len,batch\\_sz,feature\\_len][seq_len,batch_sz,feature_len]来表示数据，则对于数据大小为[5,3,100]的语义数据来说，我们输入RNN中xix_ixi​的维度应该是[3,100]也就是[batch_sz,feature_len][batch\\_sz,feature\\_len][batch_sz,feature_len] RNN Layer使用 请牢记 xt@wxh+ht@whhx_t@w_{xh}+h_t@w_{hh} xt​@wxh​+ht​@whh​ nn.RNN 首先举一个简单的例子让我们来看一看该API中的参数信息 123456789In [3]: from torch import nnIn [4]: rnn=nn.RNN(100,20)In [5]: rnn._parameters.keys()Out[5]: odict_keys([&#x27;weight_ih_l0&#x27;, &#x27;weight_hh_l0&#x27;, &#x27;bias_ih_l0&#x27;, &#x27;bias_hh_l0&#x27;])In [6]: rnn.weight_hh_l0.shape,rnn.weight_ih_l0.shapeOut[6]: (torch.Size([20, 20]), torch.Size([20, 100]))In [7]: rnn.bias_hh_l0.shape,rnn.bias_ih_l0.shapeOut[7]: (torch.Size([20]), torch.Size([20])) 一般来说该API填入参数是`nn.RNN(input_size,hidden_size,byn_layers)`input_size:代表的是表示一个单词所需的向量的长度。 hidden_size:代表的是输出的时候向量的长度。 byn_layers:代表的是输入到输出中间层的个数，默认层数为一层 因为上面代码没写`byn_layers`参数，所以默认一层，因此上面的网络参数结尾数字都是0。 forward 1out,ht=forward(x,h0) x：网络输入，tensor大小为[seq_len,batchsz,word_vec][seq\\_len,batchsz,word\\_vec][seq_len,batchsz,word_vec] h0：RNN的初始输入，tensor大小为[num_layers,batchsz,h_dim][num\\_layers,batchsz,h\\_dim][num_layers,batchsz,h_dim]其实可以不写，forword对于一个长度为seq_len的句子，会将循环神经网络全部执行一遍（每个单词按顺序一次扔进去更新模型参数）。 ht：tensor大小为[num_layers,batchsz,h_dim][num\\_layers,batchsz,h\\_dim][num_layers,batchsz,h_dim],表示的是最后一个单词送入后，最后一个MLP的每一层的输出 out：tensor大小为[seq_len,batchsz,h_dim][seq\\_len,batchsz,h\\_dim][seq_len,batchsz,h_dim],表示的是每送入一个单词后，每一个MLP的最后一层的输出 h_dim就是上面的hidden_size 下面举一个例子更加具体的来说明这个API的使用方法。 这是一个单层的RNN 123456789In [3]: from torch import nnIn [4]: rnn=nn.RNN(input_size=100, hidden_size=20,num_layers=1)In [5]: print(rnn)RNN(100, 20)In [6]: x=torch.randn(10,3,100)In [7]: out,ht=rnn(x,torch.zeros(1,3,20))In [8]: print(out.shape)torch.Size([10, 3, 20]) 仔细阅读代码会发现和前面说的一样，很好的验证了前面我们说的正确性。 多层RNN 原理一样，这里放个代码来观察一下PyTorch中RNN模块的参数定义和多层RNN设计规则。 123456789In [3]: from torch import nnIn [4]: rnn=nn.RNN(100,10,num_layers=2)In [5]: rnn._parameters.keys()Out[5]: odict_keys([&#x27;weight_ih_l0&#x27;, &#x27;weight_hh_l0&#x27;, &#x27;bias_ih_l0&#x27;, &#x27;bias_hh_l0&#x27;, &#x27;weight_ih_l1&#x27;, &#x27;weight_hh_l1&#x27;, &#x27;bias_ih_l1&#x27;, &#x27;bias_hh_l1&#x27;])In [6]: rnn.weight_hh_l0.shape, rnn.weight_ih_l0.shapeOut[6]: (torch.Size([10, 10]), torch.Size([10, 100]))In [7]: rnn.weight_hh_l1.shape, rnn.weight_ih_l1.shapeOut[7]: (torch.Size([10, 10]), torch.Size([10, 10])) 注意观察rnn.weight_ih_l0.shape和rnn.weight_ih_l1.shape的维度区别 同理我们也给出和上面单层比较像的4层RNN代码，帮助大家理解。 123456789In [3]: from torch import nnIn [4]: rnn=nn.RNN(input_size=100,hidden_size=20,num_layers=4)In [5]: print(rnn)RNN(100, 20, num_layers=4)In [6]: x=torch.randn(10,3,100)In [7]: out,h=rnn(x)In [8]: print(out.shape,h.shape)torch.Size([10, 3, 20]) torch.Size([4, 3, 20]) nn.RNNCell 不同于nn.RNN,该函数需要我们多次input才能迭代出最后的结果。打比方就是比如说我有一句话，其中有10个单词。对于nn.RNN我们只需要一起input进去，nn.RNN迭代一次就自动更新了所有，而nn.RNNCell我们要按顺序一个单词一个单词扔进去训练。相比之下nn.RNNCell虽然更加麻烦，但是比nn.RNN更加灵活。 初始化 1rnncell=nn.RNNCell(100,20) 两个参数表示输入和输出的维度，感觉和nn.Linear有点像 使用方法 1ht=rnncell(xt,ht_1) xt：当前网络输入，tensor大小为[batchsz,word_vec][batchsz,word\\_vec][batchsz,word_vec] ht_1：上一次RNN输出，tensor大小为[num_layers,batchsz,h_dim][num\\_layers,batchsz,h\\_dim][num_layers,batchsz,h_dim]，和之前那一样 ht：本次RNN输出，tensor大小为[num_layers,batchsz,h_dim][num\\_layers,batchsz,h\\_dim][num_layers,batchsz,h_dim] 迭代操作 单层RNN 1234567In [3]: from torch import nnIn [4]: cell1=nn.RNNCell(100,20)In [5]: h1=torch.zeros(3,20)In [6]: for xt in x: ...: h1=cell1(xt,h1)In [7]: print(h1.shape)torch.Size([3,20]) 双层RNN 12345678910In [3]: from torch import nnIn [4]: cell1=nn.RNNCell(100,30)In [5]: h1=torch.zeros(3,30)In [6]: cell2=nn.RNNCell(30,20)In [7]: h2=torch.zeros(3,20)In [8]: for xt in x: ...: h1=cell1(xt,h1) ...: h2=cell2(h1,h2)In [9]: print(h2.shape)torch.Size([3,20]) 实战RNN波形预测 这里想实现的一个目的就是根据正弦波形的前段部分，预测后面的正弦曲线走势。 我们这里实现的功能是根据前1个值预测后1个值是多少。 训练是每次扔入连续的50个值，然后预测向后预测仅往后移动一个单位的50个值（有49个值是重复的） 网络设计 12345678910111213141516171819202122232425class Net(nn.Module): def __init__(self, ): super(Net, self).__init__() self.rnn = nn.RNN( input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True, #input:[b,seq_len,word_vec] ) # 初始化参数 for p in self.rnn.parameters(): nn.init.normal_(p, mean=0.0, std=0.001) self.linear = nn.Linear(hidden_size, output_size) def forward(self, x, hidden_prev): out, hidden_prev = self.rnn(x, hidden_prev) # [b, seq, h] out = out.view(-1, hidden_size) out = self.linear(out) out = out.unsqueeze(dim=0) return out, hidden_prev 训练部分 123456789101112131415161718192021for iter in range(6000): start = np.random.randint(3, size=1)[0] time_steps = np.linspace(start, start + 10, num_time_steps) data = np.sin(time_steps) data = data.reshape(num_time_steps, 1) x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1) y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1) output, hidden_prev = model(x, hidden_prev) hidden_prev = hidden_prev.detach() loss = criterion(output, y) model.zero_grad() loss.backward() # for p in model.parameters(): # print(p.grad.norm()) # torch.nn.utils.clip_grad_norm_(p, 10) optimizer.step() if iter % 100 == 0: print(&quot;Iteration: &#123;&#125; loss &#123;&#125;&quot;.format(iter, loss.item())) 代码中的detach是我们之前从未见过的函数。它返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，**不同之处只是requirse_grad为false.**得到的这个tensir永远不需要计算器梯度，不具有grad. 测试部分 12345678910111213141516171819202122start = np.random.randint(3, size=1)[0]time_steps = np.linspace(start, start + 10, num_time_steps)data = np.sin(time_steps)data = data.reshape(num_time_steps, 1)x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)predictions = []input = x[:, 0, :]for _ in range(x.shape[1]): input = input.view(1, 1, 1) (pred, hidden_prev) = model(input, hidden_prev) input = pred predictions.append(pred.detach().numpy().ravel()[0])x = x.data.numpy().ravel()y = y.data.numpy()plt.scatter(time_steps[:-1], x.ravel(), s=90)plt.plot(time_steps[:-1], x.ravel())plt.scatter(time_steps[1:], predictions)plt.show() 代码汇总 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom matplotlib import pyplot as pltnum_time_steps = 50input_size = 1hidden_size = 16output_size = 1lr=0.01class Net(nn.Module): def __init__(self, ): super(Net, self).__init__() self.rnn = nn.RNN( input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True, ) for p in self.rnn.parameters(): nn.init.normal_(p, mean=0.0, std=0.001) self.linear = nn.Linear(hidden_size, output_size) def forward(self, x, hidden_prev): out, hidden_prev = self.rnn(x, hidden_prev) # [b, seq, h] out = out.view(-1, hidden_size) out = self.linear(out) out = out.unsqueeze(dim=0) return out, hidden_prevmodel = Net()criterion = nn.MSELoss()optimizer = optim.Adam(model.parameters(), lr)hidden_prev = torch.zeros(1, 1, hidden_size)for iter in range(6000): start = np.random.randint(3, size=1)[0] time_steps = np.linspace(start, start + 10, num_time_steps) data = np.sin(time_steps) data = data.reshape(num_time_steps, 1) x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1) y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1) output, hidden_prev = model(x, hidden_prev) hidden_prev = hidden_prev.detach() loss = criterion(output, y) model.zero_grad() loss.backward() # for p in model.parameters(): # print(p.grad.norm()) # torch.nn.utils.clip_grad_norm_(p, 10) optimizer.step() if iter % 100 == 0: print(&quot;Iteration: &#123;&#125; loss &#123;&#125;&quot;.format(iter, loss.item()))start = np.random.randint(3, size=1)[0]time_steps = np.linspace(start, start + 10, num_time_steps)data = np.sin(time_steps)data = data.reshape(num_time_steps, 1)x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)predictions = []input = x[:, 0, :]for _ in range(x.shape[1]): input = input.view(1, 1, 1) (pred, hidden_prev) = model(input, hidden_prev) input = pred predictions.append(pred.detach().numpy().ravel()[0])x = x.data.numpy().ravel()y = y.data.numpy()plt.scatter(time_steps[:-1], x.ravel(), s=90)plt.plot(time_steps[:-1], x.ravel())plt.scatter(time_steps[1:], predictions)plt.show() ravel()在numpy中是将numpy数组拍平，变成一维的操作，类似前面讲的Flatten 结果 RNN训练问题 梯度爆炸 梯度弥散 梯度爆炸 解决方法 其实并不是很难，我们采用clipping的方法，就是我们每次使用backward()更新了梯度信息后，对所得到的梯度大小进行判断，如果模长大于了阈值，则我们对该梯度(grad⃗\\vec{grad}grad​)进行如下操作: grad⃗=threshold×grad⃗∣∣grad∣∣\\vec{grad}=threshold\\times \\frac{\\vec{grad}}{||grad||} grad​=threshold×∣∣grad∣∣grad​​ 虽然没有从根本解决梯度爆炸的问题，但是我们有效遏制了它带来的糟糕的情况。 注意是对参数的梯度进行clipping不是对网络中的参数本体！！！ PyTorch实现 所使用的函数是torch.nn.utils.clip_grad_norm_ 1234567loss = criterion(output,y)model.zero_grad()loss.backward()for p in model.parameters():\tprint(p.grad.norm())\ttorch.nn.utils.clip_grad_norm_(p,10) # &lt;10optimizer.step() 梯度在10左右比较合适 梯度弥散 RNN梯度弥散的解决方法，请见下节课所讲的LSTM LSTM 大体结构和RNN相同只是多了三个σ\\sigmaσ(sigmoid)函数来作为门，这三道门分别叫做Forget gate,Input gate,Output gate 不同于RNN，LSTM的CtC_tCt​的作用才是memory（RNN中是hth_tht​） 三个Sigmoid Forget gate ft=σ(Wf⋅[ht−1,xt]+bf)f_t=\\sigma (W_f \\cdot [h_{t-1},x_t] + b_f) ft​=σ(Wf​⋅[ht−1​,xt​]+bf​) Input gate it=σ(Wi⋅[ht−1,xt]+bi)i_t=\\sigma(W_i\\cdot [h_{t-1},x_t]+b_i) it​=σ(Wi​⋅[ht−1​,xt​]+bi​) C~t=tanh(WC⋅[ht−1,xt]+bC)\\tilde{C}_t=tanh(W_C\\cdot [h_{t-1},x_t]+b_C) C~t​=tanh(WC​⋅[ht−1​,xt​]+bC​) 更新C Ct=ft∗Ct−1+it∗C~tC_t=f_t*C_{t-1}+i_t*\\tilde{C}_t Ct​=ft​∗Ct−1​+it​∗C~t​ Output gate ot=σ(Wo[ht−1,xt]+bo)o_t=\\sigma (W_o[h_{t-1},x_t]+b_o) ot​=σ(Wo​[ht−1​,xt​]+bo​) ht=ot∗tanh(Ct)h_t=o_t*tanh(C_t) ht​=ot​∗tanh(Ct​) LSTM行为 input gate forget gate behavior 0 1 记住过去的值 1 1 将现在的值加入记忆 0 0 抹除所有的记忆 1 0 忘掉过去的，加入现在的 Forget门叫Remember更加合理一些 LSTM解决梯度弥散 因为数学原理稍微复杂，所以这里只放出一张图供大家参考，有兴趣可以下来自行百度。简单直观理解就是，LSTM因为加入了input gate forget gate等结构，原理上类似ResNet，在某些情况下，一个RNN单元可能退化成直连（类似于ResNet中退化成只有shortcut）。因此一定程度上解决了梯度弥散问题。 LSTM Layer nn.LSTM __init()__ 和nn.RNN初始化操作一样: nn.LSTM(input_size,hidden_size,num_layers) 注意：nn.LSTM中的CCC和hhh的维度大小是一样的，都用hidden_size进行表示 LSTM.forward() 1out,(ht,ct)=lstm(x,[ht_1,ct_1]) x：tensor大小为[seq,b,vec][seq,b,vec][seq,b,vec] h/c：tensor大小为[numlayer,b,h][num_layer,b,h][numl​ayer,b,h] out：tensor大小为[seq,b,h][seq,b,h][seq,b,h] 注意：out输出的是h不是c 下面还是简单给出一个例子，帮助理解： 123456789In [3]: from torch import nnIn [4]: lstm=nn.LSTM(input_size=100, hidden_size=20, num_layers=4)In [5]: lstmOut[5]: LSTM(100, 20, num_layers=4)In [6]: x=torch.randn(10,3,100)In [7]: out,(h,c)=lstm(x)In [8]: out.shape,h.shape,c.shapeOut[8]: (torch.Size([10, 3, 20]), torch.Size([4, 3, 20]), torch.Size([4, 3, 20])) 其实和原来的RNN使用差别不是非常大 nn.LSTMCell __init()__ 和nn.LSTM初始化操作一样: nn.LSTM(input_size,hidden_size,num_layers) LSTMCell.forward() 1ht,ct=lstmcell(xt,[ht_1,ct_1]) 对于一个输入是[10,3,100]的数据，使用上述方法需要送入10次，每次送入大小是[3,100] 同理，相比LSTM，LSTMCell更加的灵活，我们更推荐这样的方式 同上，下面还是简单给出一个例子，帮助理解： 123456789101112131415In [3]: from torch import nnIn [4]: xt=torch.randn(10,3,100)In [5]: cell1=nn.LSTMCell(input_size=100,hidden_size=30)In [6]: cell2=nn.LSTMCell(input_size=30,hidden_size=20)In [7]: h1=torch.zeros(3,30)In [8]: c1=torch.zeros(3,30)In [9]: h2=torch.zeros(3,20)In [10]: c2=torch.zeros(3,20)In [11]: for x in xt: ...: h1,c1=cell1(x,[h1,c1]) ...: h2,c2=cell2(h1,[h2,c2]) In [12]: h2.shape,c2.shapeOut[12]: (torch.Size([3, 20]), torch.Size([3, 20])) LSTM情感分类实战 安利：Google CoLab 免费12H训练 免费K80GPU 界面类似Jupyter，我们只需要把我们的代码扔上去就可以跑啦。 数据导入 12345TEXT = data.Field(tokenize=&#x27;spacy&#x27;)LABEL = data.LabelField(dtype=torch.float)train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)print(&#x27;len of train data:&#x27;, len(train_data))print(&#x27;len of test data:&#x27;, len(test_data)) 运行结果 12len of train data: 25000len of test data: 25000 打出数据中的一个example进行观察 12print(train_data.examples[15].text)print(train_data.examples[15].label) 12[&#x27;Well&#x27;, &#x27;when&#x27;, &#x27;watching&#x27;, &#x27;this&#x27;, &#x27;film&#x27;, &#x27;late&#x27;, &#x27;one&#x27;, &#x27;night&#x27;, &#x27;I&#x27;, &#x27;was&#x27;, &#x27;simple&#x27;, &#x27;amazed&#x27;, &#x27;by&#x27;, &#x27;it&#x27;, &quot;&#x27;s&quot;, &#x27;greatness&#x27;, &#x27;.&#x27;, &#x27;Fantastic&#x27;, &#x27;script&#x27;, &#x27;,&#x27;, &#x27;great&#x27;, &#x27;acting&#x27;, &#x27;,&#x27;, &#x27;costumes&#x27;, &#x27;and&#x27;, &#x27;special&#x27;, &#x27;effects&#x27;, &#x27;,&#x27;, &#x27;and&#x27;, &#x27;the&#x27;, &#x27;plot&#x27;, &#x27;twists&#x27;, &#x27;,&#x27;, &#x27;wow&#x27;, &#x27;!&#x27;, &#x27;!&#x27;, &#x27;In&#x27;, &#x27;fact&#x27;, &#x27;if&#x27;, &#x27;you&#x27;, &#x27;can&#x27;, &#x27;see&#x27;, &#x27;the&#x27;, &#x27;ending&#x27;, &#x27;coming&#x27;, &#x27;you&#x27;, &#x27;should&#x27;, &#x27;become&#x27;, &#x27;a&#x27;, &#x27;writer&#x27;, &#x27;yourself.&lt;br&#x27;, &#x27;/&gt;&lt;br&#x27;, &#x27;/&gt;Great&#x27;, &#x27;,&#x27;, &#x27;I&#x27;, &#x27;would&#x27;, &#x27;recommend&#x27;, &#x27;this&#x27;, &#x27;film&#x27;, &#x27;to&#x27;, &#x27;anyone&#x27;, &#x27;,&#x27;, &#x27;especially&#x27;, &#x27;if&#x27;, &#x27;I&#x27;, &#x27;don;t&#x27;, &#x27;like&#x27;, &#x27;them&#x27;, &#x27;much.&lt;br&#x27;, &#x27;/&gt;&lt;br&#x27;, &#x27;/&gt;Terrific&#x27;]pos 使用GloVe对数据编码 123456789101112# word2vec, gloveTEXT.build_vocab(train_data, max_size=10000, vectors=&#x27;glove.6B.100d&#x27;)LABEL.build_vocab(train_data)batchsz = 30device = torch.device(&#x27;cuda&#x27;)train_iterator, test_iterator = data.BucketIterator.splits( (train_data, test_data), batch_size = batchsz, device=device) 网络设计※ 12345678910111213141516171819202122232425262728293031323334353637class RNN(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): &quot;&quot;&quot; &quot;&quot;&quot; super(RNN, self).__init__() # [0-10001] =&gt; [100] self.embedding = nn.Embedding(vocab_size, embedding_dim) # [100] =&gt; [256] self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.5) # [256*2] =&gt; [1] self.fc = nn.Linear(hidden_dim*2, 1) self.dropout = nn.Dropout(0.5) def forward(self, x): &quot;&quot;&quot; x: [seq_len, b] vs [b, 3, 28, 28] &quot;&quot;&quot; # [seq, b, 1] =&gt; [seq, b, 100] embedding = self.dropout(self.embedding(x)) # output: [seq, b, hid_dim*2] # hidden/h: [num_layers*2, b, hid_dim] # cell/c: [num_layers*2, b, hid_di] output, (hidden, cell) = self.rnn(embedding) # [num_layers*2, b, hid_dim] =&gt; 2 of [b, hid_dim] =&gt; [b, hid_dim*2] hidden = torch.cat([hidden[-2], hidden[-1]], dim=1) # [b, hid_dim*2] =&gt; [b, 1] hidden = self.dropout(hidden) out = self.fc(hidden) return out 因为nn.LSTM采用的是双向bidirectional=True,因此后面全连接层的hidden_size要乘2。最后LSTM算出来的hidden部分，最后两层就是实际网络的最后一层，只不过是两个方向，所以维度上占了两层。 `vocab_size`表示的是有多少个单词，`embedding_dim`表示的是表示一个单词需要多长维度的向量。最后一个dropout的引入是为了加强网络的鲁棒性而引入的。 训练部分 123456789101112131415161718192021222324252627282930313233343536optimizer = optim.Adam(rnn.parameters(), lr=1e-3)criteon = nn.BCEWithLogitsLoss().to(device)rnn.to(device)def binary_acc(preds, y): &quot;&quot;&quot; get accuracy &quot;&quot;&quot; preds = torch.round(torch.sigmoid(preds)) correct = torch.eq(preds, y).float() acc = correct.sum() / len(correct) return accdef train(rnn, iterator, optimizer, criteon): avg_acc = [] rnn.train() for i, batch in enumerate(iterator): # [seq, b] =&gt; [b, 1] =&gt; [b] pred = rnn(batch.text).squeeze(1) # 将第一个维度的信息压缩掉 # loss = criteon(pred, batch.label) acc = binary_acc(pred, batch.label).item() avg_acc.append(acc) optimizer.zero_grad() loss.backward() optimizer.step() if i%10 == 0: print(i, acc) # 一轮训练结束后打出训练过程中的平均准确率 avg_acc = np.array(avg_acc).mean() print(&#x27;avg acc:&#x27;, avg_acc) eval部分 123456789101112131415161718192021def eval(rnn, iterator, criteon): avg_acc = [] rnn.eval() # 切换状态 with torch.no_grad(): for batch in iterator: # [b, 1] =&gt; [b] pred = rnn(batch.text).squeeze(1) # loss = criteon(pred, batch.label) acc = binary_acc(pred, batch.label).item() avg_acc.append(acc) avg_acc = np.array(avg_acc).mean() print(&#x27;&gt;&gt;test:&#x27;, avg_acc) main部分 1234for epoch in range(10): eval(rnn, test_iterator, criteon) train(rnn, train_iterator, optimizer, criteon) 运行结果： 123456789101112131415161718192021222324252627282930&gt;&gt;test: 0.49976021360507690 0.4666666984558105510 0.4000000357627868720 0.530 0.540 0.433333367109298750 0.533333361148834260 0.600000023841857970 0.566666722297668580 0.4000000357627868790 0.36666667461395264100 0.5333333611488342110 0.6666666865348816120 0.7333333492279053130 0.4333333671092987140 0.6000000238418579150 0.5333333611488342160 0.5170 0.46666669845581055180 0.6333333849906921190 0.6666666865348816200 0.40000003576278687210 0.46666669845581055220 0.5666667222976685230 0.5...810 0.9666666984558105820 0.9666666984558105830 0.9666666984558105avg acc: 0.9673461499545786 汇总 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157# -*- coding: utf-8 -*-&quot;&quot;&quot;lstmAutomatically generated by Colaboratory.Original file is located at https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu&quot;&quot;&quot;!pip install torch!pip install torchtext!python -m spacy download en# K80 gpu for 12 hoursimport torchfrom torch import nn, optimfrom torchtext import data, datasetsprint(&#x27;GPU:&#x27;, torch.cuda.is_available())torch.manual_seed(123)TEXT = data.Field(tokenize=&#x27;spacy&#x27;)LABEL = data.LabelField(dtype=torch.float)train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)print(&#x27;len of train data:&#x27;, len(train_data))print(&#x27;len of test data:&#x27;, len(test_data))print(train_data.examples[15].text)print(train_data.examples[15].label)# word2vec, gloveTEXT.build_vocab(train_data, max_size=10000, vectors=&#x27;glove.6B.100d&#x27;)LABEL.build_vocab(train_data)batchsz = 30device = torch.device(&#x27;cuda&#x27;)train_iterator, test_iterator = data.BucketIterator.splits( (train_data, test_data), batch_size = batchsz, device=device)class RNN(nn.Module): def __init__(self, vocab_size, embedding_dim, hidden_dim): &quot;&quot;&quot; &quot;&quot;&quot; super(RNN, self).__init__() # [0-10001] =&gt; [100] self.embedding = nn.Embedding(vocab_size, embedding_dim) # [100] =&gt; [256] self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=0.5) # [256*2] =&gt; [1] self.fc = nn.Linear(hidden_dim*2, 1) self.dropout = nn.Dropout(0.5) def forward(self, x): &quot;&quot;&quot; x: [seq_len, b] vs [b, 3, 28, 28] &quot;&quot;&quot; # [seq, b, 1] =&gt; [seq, b, 100] embedding = self.dropout(self.embedding(x)) # output: [seq, b, hid_dim*2] # hidden/h: [num_layers*2, b, hid_dim] # cell/c: [num_layers*2, b, hid_di] output, (hidden, cell) = self.rnn(embedding) # [num_layers*2, b, hid_dim] =&gt; 2 of [b, hid_dim] =&gt; [b, hid_dim*2] hidden = torch.cat([hidden[-2], hidden[-1]], dim=1) # [b, hid_dim*2] =&gt; [b, 1] hidden = self.dropout(hidden) out = self.fc(hidden) return outrnn = RNN(len(TEXT.vocab), 100, 256)pretrained_embedding = TEXT.vocab.vectorsprint(&#x27;pretrained_embedding:&#x27;, pretrained_embedding.shape)rnn.embedding.weight.data.copy_(pretrained_embedding)print(&#x27;embedding layer inited.&#x27;)optimizer = optim.Adam(rnn.parameters(), lr=1e-3)criteon = nn.BCEWithLogitsLoss().to(device)rnn.to(device)import numpy as npdef binary_acc(preds, y): &quot;&quot;&quot; get accuracy &quot;&quot;&quot; preds = torch.round(torch.sigmoid(preds)) correct = torch.eq(preds, y).float() acc = correct.sum() / len(correct) return accdef train(rnn, iterator, optimizer, criteon): avg_acc = [] rnn.train() for i, batch in enumerate(iterator): # [seq, b] =&gt; [b, 1] =&gt; [b] pred = rnn(batch.text).squeeze(1) # loss = criteon(pred, batch.label) acc = binary_acc(pred, batch.label).item() avg_acc.append(acc) optimizer.zero_grad() loss.backward() optimizer.step() if i%10 == 0: print(i, acc) avg_acc = np.array(avg_acc).mean() print(&#x27;avg acc:&#x27;, avg_acc) def eval(rnn, iterator, criteon): avg_acc = [] rnn.eval() with torch.no_grad(): for batch in iterator: # [b, 1] =&gt; [b] pred = rnn(batch.text).squeeze(1) # loss = criteon(pred, batch.label) acc = binary_acc(pred, batch.label).item() avg_acc.append(acc) avg_acc = np.array(avg_acc).mean() print(&#x27;&gt;&gt;test:&#x27;, avg_acc)for epoch in range(10): eval(rnn, test_iterator, criteon) train(rnn, train_iterator, optimizer, criteon)"},{"title":"PyTorch 自定义数据集实战","path":"/wiki/PyTorch/PyTorch 自定义数据集实战.html","content":"在前面的学习中，我们已经较为熟练的掌握了不同种类神经网络的原理和基本PyTorch实现，但是之前我们所使用的数据集多为PyTorch中现成的，并没有自己提取数据的这一过程，所以本篇博客旨在采用自定义数据集，帮助大家体验一次包含数据提取，数据训练，数据测试的完整的神经网络数据处理过程。 Pokemon 数据集 数据集基本信息 本次实战，采用Pokemon自定义数据集，数据集中包含5种类型的精灵 数据集中所包含的图片的数量分别是： 皮卡丘 ：234 超梦： 239 杰尼龟： 223 小火龙： 238 妙蛙种子： 234 数据集划分 实战步骤 数据集加载（Load data）※ 创建模型（Build model） 训练和测试（Train and Test） 迁移学习（Transfer Learning）※ 数据集加载 这一部分的工作主要就是要继承父类（torch.utils.data.Dataset）并且实现该类下的两个函数 __len__:返回样本长度 __getitem__:取出样本 举个例子： 12345678910class NumberDataset(Dataset): def __init__(self,training=True): if training: self.samples = list(range(1,1001)) else: self.samples = list(range(1001,1501)) def __len__(self): return len(self.samples) def __getitem__(self, idx): return self.samples[idx] 了解了数据集PyTorch中的加载方法，下面来介绍一下本次项目需要完成的数据预处理相关操作。 数据预处理步骤（Preprocessing） Image Resize 224*224 for ResNet 18 Data Argumentation Rotate Crop Normalize Mean,std ToTensor 数据集存放文件结构 123456pokeman├─bulbasaur├─charmander├─mewtwo├─pikachu└─squirtle 可以发现是5种精灵是分了5个文件夹，我们打开pikachu文件夹，可以看到里面的图片数据是这样的。 这样存放的好处是，使用PyTorch可以直接一行代码导入所有的数据。后面我们会进行讲解。 PyTorch实现 首先是名字类别的映射 name2label 12345678910111213def __init__(self, root, resize, mode): super(Pokemon, self).__init__() # 调用父类初始化函数 self.root = root self.resize = resize # 创建类别和标签映射表 self.name2label=&#123;&#125; for name in sorted(os.listdir((os.path.join(root)))): if not os.path.isdir(os.path.join(root,name)): continue self.name2label[name] = len(self.name2label.keys()) # 调试代码 print(self.name2label) 这里因为listdir返回的顺序不稳定，所以返回后增加一个sorted函数对名字排序，这样就保证了返回的顺序稳定的问题。 结果如下： 1&#123;&#x27;bulbasaur&#x27;: 0, &#x27;charmander&#x27;: 1, &#x27;mewtwo&#x27;: 2, &#x27;pikachu&#x27;: 3, &#x27;squirtle&#x27;: 4&#125; 注意__getitem__重写的时候要将图片里面的信息提取出来，而不是图片的路径！！！ load_csv加载文件信息 这个函数也是写在我们自己的Class中的，目的是根据一个数据集加载一个数据集信息的csv文件(如果没有就先创建一个然后再加载)，文件中存储每一个数据的存放位置以及标签信息。 123456789101112131415161718192021222324252627282930313233343536373839def load_csv(self, filename):\t# 没有csv数据集信息文件，生成一个 if not os.path.exists(os.path.join(self.root, filename)): images = [] for name in self.name2label.keys(): # &#x27;pokemon\\\\mewtwo\\\\00001.png images += glob.glob(os.path.join(self.root, name, &#x27;*.png&#x27;)) images += glob.glob(os.path.join(self.root, name, &#x27;*.jpg&#x27;)) images += glob.glob(os.path.join(self.root, name, &#x27;*.jpeg&#x27;)) # 1167, &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; print(len(images), images) random.shuffle(images) with open(os.path.join(self.root, filename), mode=&#x27;w&#x27;, newline=&#x27;&#x27;) as f: writer = csv.writer(f) for img in images: # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; name = img.split(os.sep)[-2] label = self.name2label[name] # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27;, 0 writer.writerow([img, label]) print(&#x27;writen into csv file:&#x27;, filename) # 如果存在csv数据集信息文件，则读取 # read from csv file images, labels = [], [] with open(os.path.join(self.root, filename)) as f: reader = csv.reader(f) for row in reader: # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27;, 0 img, label = row label = int(label) images.append(img) labels.append(label) assert len(images) == len(labels) return images, labels glob是python自己带的一个文件操作相关模块，用它可以查找符合自己目的的文件.该方法返回所有匹配的文件路径列表（list）；该方法需要一个参数用来指定匹配的路径字符串（字符串可以为绝对路径也可以为相对路径），其返回的文件名只包括当前目录里的文件名，不包括子文件夹里的文件。 划分数据集 在__init__中load_csv后，我们就可以进行数据集裁剪了。 123456789101112# image, labelself.images, self.labels = self.load_csv(&#x27;images.csv&#x27;)if mode==&#x27;train&#x27;: # 60% self.images = self.images[:int(0.6*len(self.images))] self.labels = self.labels[:int(0.6*len(self.labels))]elif mode==&#x27;val&#x27;: # 20% = 60%-&gt;80% self.images = self.images[int(0.6*len(self.images)):int(0.8*len(self.images))] self.labels = self.labels[int(0.6*len(self.labels)):int(0.8*len(self.labels))]else: # 20% = 80%-&gt;100% self.images = self.images[int(0.8*len(self.images)):] self.labels = self.labels[int(0.8*len(self.labels)):] 其实我更推荐的方法是先将数据集全部读出来，然后再另写一个函数，使用PyTorch提供的subset方法对数据集进行划分。详情可以见PyTorch技巧中Kfold这一个模块。 数据增强 我们是在取数据的时候对数据进行数据增强操作 12345678910111213141516171819202122def __getitem__(self, idx): # idx~[0~len(images)] # self.images, self.labels # img: &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; # label: 0 img, label = self.images[idx], self.labels[idx] tf = transforms.Compose([ lambda x:Image.open(x).convert(&#x27;RGB&#x27;), # string path= &gt; image data transforms.Resize((int(self.resize*1.25), int(self.resize*1.25))), transforms.RandomRotation(15), transforms.CenterCrop(self.resize), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) img = tf(img) label = torch.tensor(label) return img, label Class部分代码汇总 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119class Pokemon(Dataset): def __init__(self, root, resize, mode): super(Pokemon, self).__init__() self.root = root self.resize = resize self.name2label = &#123;&#125; # &quot;sq...&quot;:0 for name in sorted(os.listdir(os.path.join(root))): if not os.path.isdir(os.path.join(root, name)): continue self.name2label[name] = len(self.name2label.keys()) # print(self.name2label) # image, label self.images, self.labels = self.load_csv(&#x27;images.csv&#x27;) if mode==&#x27;train&#x27;: # 60% self.images = self.images[:int(0.6*len(self.images))] self.labels = self.labels[:int(0.6*len(self.labels))] elif mode==&#x27;val&#x27;: # 20% = 60%-&gt;80% self.images = self.images[int(0.6*len(self.images)):int(0.8*len(self.images))] self.labels = self.labels[int(0.6*len(self.labels)):int(0.8*len(self.labels))] else: # 20% = 80%-&gt;100% self.images = self.images[int(0.8*len(self.images)):] self.labels = self.labels[int(0.8*len(self.labels)):] def load_csv(self, filename): if not os.path.exists(os.path.join(self.root, filename)): images = [] for name in self.name2label.keys(): # &#x27;pokemon\\\\mewtwo\\\\00001.png images += glob.glob(os.path.join(self.root, name, &#x27;*.png&#x27;)) images += glob.glob(os.path.join(self.root, name, &#x27;*.jpg&#x27;)) images += glob.glob(os.path.join(self.root, name, &#x27;*.jpeg&#x27;)) # 1167, &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; print(len(images), images) random.shuffle(images) with open(os.path.join(self.root, filename), mode=&#x27;w&#x27;, newline=&#x27;&#x27;) as f: writer = csv.writer(f) for img in images: # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; name = img.split(os.sep)[-2] label = self.name2label[name] # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27;, 0 writer.writerow([img, label]) print(&#x27;writen into csv file:&#x27;, filename) # read from csv file images, labels = [], [] with open(os.path.join(self.root, filename)) as f: reader = csv.reader(f) for row in reader: # &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27;, 0 img, label = row label = int(label) images.append(img) labels.append(label) assert len(images) == len(labels) return images, labels def __len__(self): return len(self.images) def denormalize(self, x_hat): mean = [0.485, 0.456, 0.406] std = [0.229, 0.224, 0.225] # x_hat = (x-mean)/std # x = x_hat*std = mean # x: [c, h, w] # mean: [3] =&gt; [3, 1, 1] mean = torch.tensor(mean).unsqueeze(1).unsqueeze(1) std = torch.tensor(std).unsqueeze(1).unsqueeze(1) # print(mean.shape, std.shape) x = x_hat * std + mean return x def __getitem__(self, idx): # idx~[0~len(images)] # self.images, self.labels # img: &#x27;pokemon\\\\bulbasaur\\\\00000000.png&#x27; # label: 0 img, label = self.images[idx], self.labels[idx] tf = transforms.Compose([ lambda x:Image.open(x).convert(&#x27;RGB&#x27;), # string path= &gt; image data transforms.Resize((int(self.resize*1.25), int(self.resize*1.25))), transforms.RandomRotation(15), transforms.CenterCrop(self.resize), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) img = tf(img) label = torch.tensor(label) return img, label 其中denormalize函数做的工作是反正则化 Visdom验证自定义数据集加载正确性 我们在主函数中写下如下的代码： 12345678910111213141516171819import visdomimport timeimport torchvisionviz = visdom.Visdom()db = Pokemon(&#x27;pokemon&#x27;, 64, &#x27;train&#x27;)x,y = next(iter(db))print(&#x27;sample:&#x27;, x.shape, y.shape, y)viz.image(db.denormalize(x), win=&#x27;sample_x&#x27;, opts=dict(title=&#x27;sample_x&#x27;))loader = DataLoader(db, batch_size=32, shuffle=True, num_workers=8)for x,y in loader: viz.images(db.denormalize(x), nrow=8, win=&#x27;batch&#x27;, opts=dict(title=&#x27;batch&#x27;)) viz.text(str(y.numpy()), win=&#x27;label&#x27;, opts=dict(title=&#x27;batch-y&#x27;))time.sleep(10) 使用Visdom之前一定要在命令行启动Visdom本地服务器，输入以下命令 1python -m visdom.server 运行效果是每过10sload32张图出来，一排8个，一共4排。 自定义数据集的部分就完成了，下面就进入到建立模型的阶段了。 建立模型 这一阶段在之前的PyTorch CNN实战部分已经有过详细的讲解了，这里就不赘述了。 本实验使用的ResNet18模型代码： 1234567891011121314151617181920212223242526272829303132333435363738394041class ResNet18(nn.Module): def __init__(self, num_class): super(ResNet18, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3, stride=3, padding=0), nn.BatchNorm2d(16) ) # followed 4 blocks # [b, 16, h, w] =&gt; [b, 32, h ,w] self.blk1 = ResBlk(16, 32, stride=3) # [b, 32, h, w] =&gt; [b, 64, h, w] self.blk2 = ResBlk(32, 64, stride=3) # # [b, 64, h, w] =&gt; [b, 128, h, w] self.blk3 = ResBlk(64, 128, stride=2) # # [b, 128, h, w] =&gt; [b, 256, h, w] self.blk4 = ResBlk(128, 256, stride=2) # [b, 256, 7, 7] self.outlayer = nn.Linear(256*3*3, num_class) def forward(self, x): &quot;&quot;&quot; :param x: :return: &quot;&quot;&quot; x = F.relu(self.conv1(x)) # [b, 64, h, w] =&gt; [b, 1024, h, w] x = self.blk1(x) x = self.blk2(x) x = self.blk3(x) x = self.blk4(x) # print(x.shape) x = x.view(x.size(0), -1) x = self.outlayer(x) return x 训练，验证，测试 严格Train，Val，Test模板※ 12345678910111213for epoch in range(epochs): train(train_db) if epoch%10==0: # Val val_acc = evaluate(val_db) if val_acc is the best: save_ckpt() # 保存当前网络参数 if out_of_patience(): breakload_ckpt # 加载网络参数test_acc = evaluate(test_db) # test 以后训练按照以上模板进行书写即可。 本实验中所使用的代码如下所示： evaluate部分 1234567891011121314def evalute(model, loader): model.eval() correct = 0 total = len(loader.dataset) for x,y in loader: x,y = x.to(device), y.to(device) with torch.no_grad(): logits = model(x) pred = logits.argmax(dim=1) correct += torch.eq(pred, y).sum().float().item() return correct / total main部分 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def main(): model = ResNet18(5).to(device) optimizer = optim.Adam(model.parameters(), lr=lr) criteon = nn.CrossEntropyLoss() best_acc, best_epoch = 0, 0 # 保存最优模型参数 global_step = 0 viz.line([0], [-1], win=&#x27;loss&#x27;, opts=dict(title=&#x27;loss&#x27;)) viz.line([0], [-1], win=&#x27;val_acc&#x27;, opts=dict(title=&#x27;val_acc&#x27;)) for epoch in range(epochs): for step, (x,y) in enumerate(train_loader): # x: [b, 3, 224, 224], y: [b] x, y = x.to(device), y.to(device) model.train() logits = model(x) loss = criteon(logits, y) optimizer.zero_grad() loss.backward() optimizer.step() viz.line([loss.item()], [global_step], win=&#x27;loss&#x27;, update=&#x27;append&#x27;) global_step += 1 if epoch % 1 == 0: val_acc = evalute(model, val_loader) if val_acc&gt; best_acc: best_epoch = epoch best_acc = val_acc torch.save(model.state_dict(), &#x27;best.mdl&#x27;) # 保存最优模型参数 viz.line([val_acc], [global_step], win=&#x27;val_acc&#x27;, update=&#x27;append&#x27;)\t# 结束train后 print(&#x27;best acc:&#x27;, best_acc, &#x27;best epoch:&#x27;, best_epoch)\t# 加载最优模型 model.load_state_dict(torch.load(&#x27;best.mdl&#x27;)) print(&#x27;loaded from ckpt!&#x27;)\t# 进行test test_acc = evalute(model, test_loader) print(&#x27;test acc:&#x27;, test_acc) 如果想要实时检测训练情况，那么我们可以使用visdom工具，而不是最后进行matplotlib，实时监控可以在模型出现问题时及时停下来，进行调整。(如上面的代码所示) 目前结果 按照以上步骤下来，我们的模型train下来的loss是非常小的，但是测试集，准确率并没有达到理想的准确率，这说明我们的模型发生了过拟合,发生这样的事情非常的正常，因为我们数据集的规模非常的小，而且种类也不多，对于ResNet18这种较为复杂的神经网络是不够的，很容易出现这样的问题，因此，这个时候就有必要使用迁移学习解决过拟合的问题了。 迁移学习（Transfer Learning） 简而言之，迁移学习是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程中。 在我们这里，就是在训练好ImageNet数据集的神经网络的基础上，提取其中训练好的网络参数，加载到要训练宝可梦数据集的网络中，然后再对宝可梦数据集进行训练。 这里我们直接使用的是torchvision中提供好的resnet18模型 1from torchvision.models import resnet18 此ResNet18是已经训练好的模型！ 我们要做的就是将其前17层拆下来，然后最后一层接一层我们自己的全连接层进行分类。 使用children()方法拆下网络的前17层然后传入到我们自己的model中去，然后再接Flatten操作，然后接全连接层。 代码如下所示：（只需要在初始化模型那里改变一点即可，这里就不贴代码汇总了） 12345trained_model = resnet18(pretrained=True)model = nn.Sequential(*list(trained_model.children())[:-1], #[b, 512, 1, 1] Flatten(), # [b, 512, 1, 1] =&gt; [b, 512] nn.Linear(512, 5) ).to(device) Flatten层代码： 12345678class Flatten(nn.Module): def __init__(self): super(Flatten, self).__init__() def forward(self, x): shape = torch.prod(torch.tensor(x.shape[1:])).item() return x.view(-1, shape) 结果 最终，train loss在同一水平下，验证集的准确率提高了10%左右，使用迁移学习的效果提升还是非常明显的。"},{"title":"PyTorch 自编码器","path":"/wiki/PyTorch/PyTorch 自编码器.html","content":"我们之前所学习的知识都是监督学习，监督学习很多是基于人的经验来对分类回归问题进行判断，现在我们来研究一些关于非监督学习的内容。 首先便是一个问题，为什么我们需要研究非监督学习。 非监督学习的意义 数据维度减少（Dimension reduction） 预处理（Preprocessing） 可视化（Visualization） 更好的利用看上去不重要的数据(Taking advantage of unsupervised data) 压缩，降噪，超分辨率(Compression,denoising,super-resolution) 一个可以可视化数据的网站：https://projector.tensorflow.org 总而言之就是更好的利用数据，并从数据中发现一些有用的东西。 Auto-Encoders 自编码器（autoencoder, AE）是一类在半监督学习和非监督学习中使用的神经网络（Artificial Neural Networks, ANNs），其功能是通过将输入信息作为学习目标，对输入信息进行表征学习（representation learning） 自编码器具有一般意义上表征学习算法的功能，被应用于**降维（dimensionality reduction）**和异常值检测（anomaly detection） 输入输出一样，重建输入数据 中间那个很少神经元的层叫做neck（一般是对数据进行降维） Auto Encoder就是一个非常普通的神经网络 如何训练 Loss Function Lose function for real-valued inputs l(f(x))=12∑k(x^k−xk)2l(f(x))=\\frac{1}{2}\\sum_k(\\hat{x}_k-x_k)^2 l(f(x))=21​k∑​(x^k​−xk​)2 x^k\\hat{x}_kx^k​是重建后数据k位置的值 xkx_kxk​是输入数据k位置的值 Loss function for binary inputs l(f(x))=−∑k(xk log(x^k)+(1−xk)log(1−x^k))l(f(x))=-\\sum_k(x_k\\ log(\\hat{x}_k)+(1-x_k)log(1-\\hat{x}_k)) l(f(x))=−k∑​(xk​ log(x^k​)+(1−xk​)log(1−x^k​)) Cross-entropy error function 一般用于分类问题，类似one-hot编码 其实还是和普通神经网络差不多 AE vs PCA 使用MNIST数据集作为输入，然后分别使用PCA或AE对数据进行降维，然后再重构，效果图如下图所示。 人脸数据集第二行是AE，第三行是PCA，综上，可以看出AE的效果还是要好一些的。 Denoising Auto-Encoders 就是对输入数据加噪声，以此让模型不要记住某些特征，而是理解某些特征，让模型发掘一些数据高层次的特征。这种类型的AE可以对图片进行降噪操作。 Dropout Auto-Encoders 正如其名，和我们前面讲的Dropout是一个意思。 Adversarial Auto-Encoders Discriminator这一部分知识涉及到GAN，会在GAN部分讲解清楚 我们目前只需要知道，随着训练次数的增加，中间的隐藏层h所得的数据会呈现出一种分布的状态。 Variational Auto-Encoders（VAE）※ VAE非常的重要，具体原理可以看这个视频或这篇博客，这里就不再赘述了。（因为本人表达能力问题和比较菜，可能说不太清楚） 链接：李宏毅老师VAE原理 直观理解 注意：还是和AE不太一样，因为中间有一个采样的操作，采样的值作为提取出来的特征。因此这两层的连接也和其他层之间的连接也不一样，并不是全连接！！！ 还要注意：这里我们的Loss函数不仅要最小AE中的均方差，还要最小化KLdiv（上图标有Minimize的黄框），所以要将他们两个加起来作为最终的loss进行反向传播！！！（这个东西可以在上面的链接中了解到这是什么） VAE不同于AE，学习的不是单个向量，而是分布！！！因此具有一定的推理能力，直观理解见下图的例子 综上，VAE虽然效果比AE好，但是根据他的原理，他本质上只是记住了现有数据的特征，并没有学习如何生成数据（只是再一个确定分布内随机），这是VAE最大的一个问题，解决这个问题需要使用我们后面马上会讲解的GAN。 Pytoch实战 Auto-Encoder实验 目标 本次实验的目标是构建一个Auto-Encoder重构MNIST数据集。 代码实现 其实代码都和最简单的全连接神经网络非常的相似，这里不再赘述，直接贴代码。 首先是网络结构的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041class AE(nn.Module): def __init__(self) -&gt; None: super(AE,self).__init__() # Encoder # [b,784] =&gt; [b,20] self.encoder = nn.Sequential( nn.Linear(784,256), nn.ReLU(), nn.Linear(256,64), nn.ReLU(), nn.Linear(64, 20), nn.ReLU() ) # Decoder # [b,20] =&gt; [b,784] self.decoder = nn.Sequential( nn.Linear(20,64), nn.ReLU(), nn.Linear(64,256), nn.ReLU(), nn.Linear(256,784), nn.Sigmoid() ) def forward(self, x): batchsz = x.size(0) # flatten x = x.view(batchsz,784) # encoder x = self.encoder(x) # decoder x = self.decoder(x) # reshape x = x.view(batchsz,1,28,28) return x 注意这里我们网络结构是拆成了encoder和decoder的，这样使得网络结构更加的清晰！ 训练测试部分主代码（没有区别， 只是多做了一个可视化 ）： 123456789101112131415161718192021222324252627282930313233343536373839404142def main(): mnist_train = datasets.MNIST(&#x27;mnist&#x27;, True, transform=transforms.Compose([ transforms.ToTensor() ]), download=True) mnist_train = DataLoader(mnist_train,batch_size=32,shuffle=True) mnist_test = datasets.MNIST(&#x27;mnist&#x27;, False, transform=transforms.Compose([ transforms.ToTensor() ]), download=True) mnist_test = DataLoader(mnist_test,batch_size=32,shuffle=True) x,_ = iter(mnist_train).next() print(&#x27;x:&#x27;, x.shape) device = torch.device(&#x27;cuda&#x27;) model = AE().to(device) print(model) criteon = nn.MSELoss() optimizer = optim.Adam(model.parameters(),lr=1e-3) viz =visdom.Visdom() for epoch in range(1000): for batchidx, (x,_) in enumerate(mnist_train): x = x.to(device) x_hat = model(x) loss = criteon(x_hat,x) # backprop optimizer.zero_grad() loss.backward() optimizer.step() print(epoch , &quot;loss:&quot;, loss.item()) x,_ = iter(mnist_test).next() x = x.to(device) with torch.no_grad(): x_hat = model(x) viz.images(x,nrow=8,win=&#x27;x&#x27;,opts=dict(title=&#x27;x&#x27;)) viz.images(x_hat,nrow=8,win=&#x27;x_hat&#x27;,opts=dict(title=&#x27;x_hat&#x27;)) 结果 重建结果如下图所示，尽管这个数据集非常的简单，但是我们还是可以看出，还是有一些数据重建的并不是非常的理想。 VAE实验 目标 本次实验的目标是构建一个VAE重构MNIST数据集。 因为VAE中间有一个操作是采样，而采样这个操作是不可导的，因此在实际代码实现中这里有一个小trick。 代码实现 网络结构代码和AE代码是一样的（还是有一点不一样 decoder起始神经元数量变为10，注意改一下！ ），不一样的部分体现在forward部分（在encoder和decoder中间增加了采样），这里就使用到了我们的trick，利用pytorch自带的分布生成器，可以生成可导的采样操作。注意forward最后返回了kld哦！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class VAE(nn.Module): def __init__(self): super(VAE, self).__init__() # [b, 784] =&gt; [b, 20] # u: [b, 10] # sigma: [b, 10] self.encoder = nn.Sequential( nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 20), nn.ReLU() ) # [b, 20] =&gt; [b, 784] self.decoder = nn.Sequential( nn.Linear(10, 64), nn.ReLU(), nn.Linear(64, 256), nn.ReLU(), nn.Linear(256, 784), nn.Sigmoid() ) self.criteon = nn.MSELoss() def forward(self, x): &quot;&quot;&quot; :param x: [b, 1, 28, 28] :return: &quot;&quot;&quot; batchsz = x.size(0) # flatten x = x.view(batchsz, 784) # encoder # [b, 20], including mean and sigma h_ = self.encoder(x) # [b, 20] =&gt; [b, 10] and [b, 10] mu, sigma = h_.chunk(2, dim=1) # reparametrize trick, epison~N(0, 1) h = mu + sigma * torch.randn_like(sigma) # decoder x_hat = self.decoder(h) # reshape x_hat = x_hat.view(batchsz, 1, 28, 28) kld = 0.5 * torch.sum( torch.pow(mu, 2) + torch.pow(sigma, 2) - torch.log(1e-8 + torch.pow(sigma, 2)) - 1 ) / (batchsz*28*28) return x_hat, kld 然后就是训练部分的代码了，这里也就只有loss函数改了一下。 12345x_hat, kld = model(x)loss = criteon(x_hat, x)if kld is not None: elbo = - loss - 1.0 * kld loss = - elbo 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def main(): mnist_train = datasets.MNIST(&#x27;mnist&#x27;, True, transform=transforms.Compose([ transforms.ToTensor() ]), download=True) mnist_train = DataLoader(mnist_train, batch_size=32, shuffle=True) mnist_test = datasets.MNIST(&#x27;mnist&#x27;, False, transform=transforms.Compose([ transforms.ToTensor() ]), download=True) mnist_test = DataLoader(mnist_test, batch_size=32, shuffle=True) x, _ = iter(mnist_train).next() print(&#x27;x:&#x27;, x.shape) device = torch.device(&#x27;cuda&#x27;) # model = AE().to(device) model = VAE().to(device) criteon = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=1e-3) print(model) viz = visdom.Visdom() for epoch in range(1000): for batchidx, (x, _) in enumerate(mnist_train): # [b, 1, 28, 28] x = x.to(device) x_hat, kld = model(x) loss = criteon(x_hat, x) if kld is not None: elbo = - loss - 1.0 * kld loss = - elbo # backprop optimizer.zero_grad() loss.backward() optimizer.step() print(epoch, &#x27;loss:&#x27;, loss.item(), &#x27;kld:&#x27;, kld.item()) x, _ = iter(mnist_test).next() x = x.to(device) with torch.no_grad(): x_hat, kld = model(x) viz.images(x, nrow=8, win=&#x27;x&#x27;, opts=dict(title=&#x27;x&#x27;)) viz.images(x_hat, nrow=8, win=&#x27;x_hat&#x27;, opts=dict(title=&#x27;x_hat&#x27;)) 结果 重建结果如下图所示，VAE重建的所有数字基本上是可以看清楚的，但是因为此数据集比较简单的缘故，VAE相比于AE的优势并没有很好的体现出来。"},{"title":"PyTorch入门-简单BP全连接神经网络","path":"/wiki/PyTorch/PyTorch入门-简单BP神经网络.html","content":"简介 因为课程需要学习PyTorch，所以这里就先简单的入个门，使用PyTorch实现一个简单的3层BP全连接神经网络实验。实验使用的数据集是MNIST手写数字识别数据集。 我们设计的网络结构是4层全连接神经网络，因为一个数字所存储的像素信息是28×28×128\\times 28\\times 128×28×1因此，网络的第一层有28×28×1=78428\\times 28 \\times 1=78428×28×1=784个神经元，第二层设计的有 256个神经元，第三层有64个神经元，第四层有10个神经元。其中第一层和第二层，第二层和第三层，第三层和第四层之间，都有一个relu激活函数。最后第四层的1×101\\times 101×10输出向量相当于是对10个数字的识别相似度，最大的数的index代表识别出来相似度最高的数字。 网络构建部分的代码实现如下： 1234567891011121314151617181920class Net(nn.Module): def __init__(self): super(Net, self).__init__() # xw+b self.fc1 = nn.Linear(28*28, 256) self.fc2 = nn.Linear(256, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): # x: [b, 1, 28, 28] # h1 = relu(xw1+b1) x = F.relu(self.fc1(x)) # h2 = relu(h1w2+b2) x = F.relu(self.fc2(x)) # h3 = h2w3+b3 x = self.fc3(x) return x 定义损失函数是： cost=∑(pred−Y)2cost=\\sum(pred-Y)^2 cost=∑(pred−Y)2 代码实现与讲解 数据载入 123456789101112131415161718192021222324252627282930313233343536import torchfrom torch import nnfrom torch.nn import functional as Ffrom torch import optimimport torchvisionfrom matplotlib import pyplot as pltfrom utils import plot_image, plot_curve, one_hotbatch_size = 512 #批# step1. load datasettrain_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(&#x27;mnist_data&#x27;, train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(&#x27;mnist_data/&#x27;, train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=False)x, y = next(iter(train_loader))print(x.shape, y.shape, x.min(), x.max())plot_image(x, y, &#x27;image sample&#x27;) 这里就是载入了训练集train_loader和test_loader 然后倒数第三行的next返回的是训练集中下一个元素（下一个batch），因为是batch大小是512，所以print出来的x和y的维度如下所示。 1torch.Size([512, 1, 28, 28]) torch.Size([512]) tensor(-0.4242) tensor(2.8215) 建立网络 上面已经进行过讲解了，这里不再赘述 1234567891011121314151617181920class Net(nn.Module): def __init__(self): super(Net, self).__init__() # xw+b self.fc1 = nn.Linear(28*28, 256) self.fc2 = nn.Linear(256, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): # x: [b, 1, 28, 28] # h1 = relu(xw1+b1) x = F.relu(self.fc1(x)) # h2 = relu(h1w2+b2) x = F.relu(self.fc2(x)) # h3 = h2w3+b3 x = self.fc3(x) return x 初始化参数 123net = Net() # 实例化对象，这里就是我们建立的网络# [w1, b1, w2, b2, w3, b3]optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9) # 初始化参数（设置超参数和随机初始化参数） 开始训练 1234567891011121314151617181920212223for epoch in range(3): for batch_idx, (x, y) in enumerate(train_loader): # x: [b, 1, 28, 28], y: [512] # [b, 1, 28, 28] =&gt; [b, 784] x = x.view(x.size(0), 28*28) # =&gt; [b, 10] out = net(x) # [b, 10] y_onehot = one_hot(y) # loss = mse(out, y_onehot) loss = F.mse_loss(out, y_onehot) optimizer.zero_grad() loss.backward() # w&#x27; = w - lr*grad optimizer.step() train_loss.append(loss.item()) if batch_idx % 10==0: print(epoch, batch_idx, loss.item()) epoch代表训练的轮数，这里一共训练三轮。然后每一轮中每次训练取出同一batch下的所有训练数据（一共512组），注释中的b=512 然后先进行reshape。代码中的view实现的功能类似numpy中reshape。这里就将X从一个四维数组512×1×28×28512\\times 1 \\times 28\\times 28512×1×28×28转化为了512×784512 \\times 784512×784的二维数组。然后扔入网络进行训练，将y标签先转化为独热编码和网络扔出的值结合计算loss。计算结束后就进行以下三步： 1234optimizer.zero_grad() # 将模型的参数梯度初始化为0 loss.backward() # 反向传播计算梯度 # w&#x27; = w - lr*grad optimizer.step() # 更新所有参数 总结一下，训练过程的代码一般就以下几块： 12345678910optimizer.zero_grad() # 将模型的参数梯度初始化为0outputs=model（inputs） # 前向传播计算预测值loss = cost(outputs, y_train) # 计算当前损失loss.backward() # 反向传播计算梯度optimizer.step() # 更新所有参数 然后训练过程就结束了，下面就是测试环节了。 进行测试 1234567891011121314151617total_correct = 0for x,y in test_loader: x = x.view(x.size(0), 28*28) out = net(x) # out: [b, 10] =&gt; pred: [b] pred = out.argmax(dim=1) correct = pred.eq(y).sum().float().item() total_correct += correcttotal_num = len(test_loader.dataset)acc = total_correct / total_numprint(&#x27;test acc:&#x27;, acc)x, y = next(iter(test_loader))out = net(x.view(x.size(0), 28*28))pred = out.argmax(dim=1)plot_image(x, pred, &#x27;test&#x27;) 同理，和训练过程很像，也是先使用view，reshape一下后，扔入网络，得到out后，寻找数值最大的下标记录在pred中，pred就是测试预测的数值，然后就可以计算正确率等评价指标了。 总结 不同于tensorflow的place_holder,pytorch网络定义更加简单便捷，只需要使用类似nn.Linear(a,b)的函数定义连接方式，然后最后实例化网络的时候传入一个parameter()就可以了，相比于tensorflow确实更加简单。 最终代码 mnist_train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114import torchfrom torch import nnfrom torch.nn import functional as Ffrom torch import optimimport torchvisionfrom matplotlib import pyplot as pltfrom utils import plot_image, plot_curve, one_hotbatch_size = 512 #批# step1. load datasettrain_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(&#x27;mnist_data&#x27;, train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST(&#x27;mnist_data/&#x27;, train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=False)x, y = next(iter(train_loader))print(x.shape, y.shape, x.min(), x.max())plot_image(x, y, &#x27;image sample&#x27;)class Net(nn.Module): def __init__(self): super(Net, self).__init__() # xw+b self.fc1 = nn.Linear(28*28, 256) self.fc2 = nn.Linear(256, 64) self.fc3 = nn.Linear(64, 10) def forward(self, x): # x: [b, 1, 28, 28] # h1 = relu(xw1+b1) x = F.relu(self.fc1(x)) # h2 = relu(h1w2+b2) x = F.relu(self.fc2(x)) # h3 = h2w3+b3 x = self.fc3(x) return xnet = Net()# [w1, b1, w2, b2, w3, b3]optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)train_loss = []for epoch in range(3): for batch_idx, (x, y) in enumerate(train_loader): # x: [b, 1, 28, 28], y: [512] # [b, 1, 28, 28] =&gt; [b, 784] x = x.view(x.size(0), 28*28) # =&gt; [b, 10] out = net(x) # [b, 10] y_onehot = one_hot(y) # loss = mse(out, y_onehot) loss = F.mse_loss(out, y_onehot) optimizer.zero_grad() loss.backward() # w&#x27; = w - lr*grad optimizer.step() train_loss.append(loss.item()) if batch_idx % 10==0: print(epoch, batch_idx, loss.item())plot_curve(train_loss) # 画出损失函数# we get optimal [w1, b1, w2, b2, w3, b3]total_correct = 0for x,y in test_loader: x = x.view(x.size(0), 28*28) out = net(x) # out: [b, 10] =&gt; pred: [b] pred = out.argmax(dim=1) correct = pred.eq(y).sum().float().item() total_correct += correcttotal_num = len(test_loader.dataset)acc = total_correct / total_numprint(&#x27;test acc:&#x27;, acc)x, y = next(iter(test_loader))out = net(x.view(x.size(0), 28*28))pred = out.argmax(dim=1)plot_image(x, pred, &#x27;test&#x27;) utils.py 1234567891011121314151617181920212223242526272829303132import torchfrom matplotlib import pyplot as pltdef plot_curve(data): fig = plt.figure() plt.plot(range(len(data)), data, color=&#x27;blue&#x27;) plt.legend([&#x27;value&#x27;], loc=&#x27;upper right&#x27;) plt.xlabel(&#x27;step&#x27;) plt.ylabel(&#x27;value&#x27;) plt.show()def plot_image(img, label, name): fig = plt.figure() for i in range(6): plt.subplot(2, 3, i + 1) plt.tight_layout() plt.imshow(img[i][0]*0.3081+0.1307, cmap=&#x27;gray&#x27;, interpolation=&#x27;none&#x27;) plt.title(&quot;&#123;&#125;: &#123;&#125;&quot;.format(name, label[i].item())) plt.xticks([]) plt.yticks([]) plt.show()def one_hot(label, depth=10): out = torch.zeros(label.size(0), depth) idx = torch.LongTensor(label).view(-1, 1) out.scatter_(dim=1, index=idx, value=1) return out 结果 最终可以发现打印出来的均预测正确。acc为0.8794，是可以接受的正确率。"},{"title":"PyTorch基础","path":"/wiki/PyTorch/PyTorch基础.html","content":"Tensor数据类型 对比 Python PyTorch Int IntTensor of size() float FloatTensor of size() Int array IntTensor of size [d1,d2,…] Float array FloatTensor of size[d1,d2,…] string – 可以发现PyTorch并不支持string表示，但是我们可以使用如下两种方法在PyTorch 中表示string。 One-hot Embedding Word2vec glove 这里不对以上两种方法展开进行讲解，大家感兴趣可以自行百度。 数据类型 最常用的一般是以下几种 FloatTensor DoubleTensor IntTensor LongTensor ByteTensor 数据类型推断 123456In [3]: a=torch.randn(2,3)In [4]: a.type()Out[4]: &#x27;torch.FloatTensor&#x27;In [5]: type(a)Out[5]: torch.Tensor type()：打出当前tensor的数据类型 isinstance()：检验当前数据是否为此类型 type(xxx)：Python自带的类型检测，只能检测最基本数据类型 以下代码测试说明了，同一数据部署在CPU和GPU上数据类型是不一样的 12345In [7]: isinstance(a,torch.cuda.FloatTensor)Out[7]: FalseIn [8]: a=a.cuda()In [9]: isinstance(a,torch.cuda.FloatTensor)Out[9]: True 标量的表示: Dim0 1234In [10]: torch.tensor(1.0)Out[10]: tensor(1.)In [11]: torch.tensor(1.3)Out[11]: tensor(1.3000) 以下代码展示了shape,size(),dim()的使用方法。 其中输入19和21是常见的确定tensor维度的方法 123456789In [17]: a=torch.tensor(1.3)In [18]: a.shapeOut[18]: torch.Size([])In [19]: len(a.shape)Out[19]: 0In [20]: a.size()Out[20]: torch.Size([])In [21]: a.dim()Out[21]: 0 向量Vector：Dim1 一个元素的一维向量，其实就是多加了一个中括号，详情见下面的代码。 FloatTensor接收的参数是shape，然后随机初始化向量。 然后In [9]以后介绍了一种numpy转tensor的方法 123456789101112131415161718In [3]: torch.tensor([1.1])Out[3]: tensor([1.1000])In [4]: a=torch.tensor([1.1])In [5]: a.dim()Out[5]: 1In [6]: torch.tensor([1.1,2.2])Out[6]: tensor([1.1000, 2.2000])In [7]: torch.FloatTensor(1)Out[7]: tensor([0.])In [8]: torch.FloatTensor(2)Out[8]: tensor([0., 0.])In [9]: import numpy as npIn [10]: data=np.ones(2)In [11]: dataOut[11]: array([1., 1.])In [12]: torch.from_numpy(data)Out[12]: tensor([1., 1.], dtype=torch.float64) 矩阵Matrix：Dim2 1234567891011121314In [3]: a=torch.randn(2,3)In [4]: aOut[4]: tensor([[-0.6969, 1.8345, -0.4894], [-0.0066, -0.0398, -0.6140]])In [5]: a.shapeOut[5]: torch.Size([2, 3])In [6]: a.size(0)Out[6]: 2In [7]: a.size(1)Out[7]: 3In [8]: a.shape[1]Out[8]: 3 在PyTorch中，randn是随机正态分布，而rand是随机均匀分布。 Dim3 12345678910111213141516171819In [3]: a=torch.rand(1,2,3)In [4]: aOut[4]: tensor([[[0.7603, 0.4443, 0.4788], [0.0602, 0.6325, 0.8426]]])In [5]: a.shapeOut[5]: torch.Size([1, 2, 3])In [6]: a[0]Out[6]: tensor([[0.7603, 0.4443, 0.4788], [0.0602, 0.6325, 0.8426]])In [7]: a[0,1]Out[7]: tensor([0.0602, 0.6325, 0.8426])In [8]: a[0,1,0]Out[8]: tensor(0.0602)In [9]: list(a.shape)Out[9]: [1, 2, 3] 三维的tensor一般用于RNN中 Dim4 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253In [3]: a=torch.rand(2,3,28,28)In [4]: a.dim()Out[4]: 4In [5]: aOut[5]: tensor([[[[0.5255, 0.9019, 0.9471, ..., 0.7050, 0.4455, 0.8681], [0.7229, 0.9215, 0.4863, ..., 0.4863, 0.5061, 0.3511], [0.0015, 0.3703, 0.0695, ..., 0.7927, 0.0028, 0.5953], ..., [0.3896, 0.5274, 0.7822, ..., 0.7864, 0.5558, 0.2190], [0.8072, 0.7983, 0.8963, ..., 0.8586, 0.2881, 0.1836], [0.7002, 0.4452, 0.8168, ..., 0.6467, 0.2885, 0.4928]], [[0.5885, 0.6984, 0.6732, ..., 0.3300, 0.3039, 0.1747], [0.2004, 0.1730, 0.4867, ..., 0.5207, 0.3391, 0.6586], [0.0279, 0.1897, 0.5924, ..., 0.7152, 0.6622, 0.1943], ..., [0.6697, 0.9150, 0.1786, ..., 0.6098, 0.8226, 0.2826], [0.2074, 0.6030, 0.0912, ..., 0.9088, 0.1924, 0.6403], [0.0557, 0.0836, 0.2382, ..., 0.4082, 0.0600, 0.5102]], [[0.3708, 0.5273, 0.0810, ..., 0.8926, 0.5854, 0.7139], [0.6355, 0.3067, 0.9933, ..., 0.7173, 0.0793, 0.3067], [0.9277, 0.6752, 0.7401, ..., 0.6875, 0.4112, 0.9575], ..., [0.2762, 0.6146, 0.9833, ..., 0.2625, 0.9879, 0.1417], [0.1789, 0.9695, 0.6872, ..., 0.2485, 0.0840, 0.4170], [0.5604, 0.1260, 0.8523, ..., 0.6334, 0.1975, 0.5056]]], [[[0.9585, 0.5642, 0.2652, ..., 0.7171, 0.5738, 0.7511], [0.0737, 0.4149, 0.1017, ..., 0.4772, 0.4125, 0.4467], [0.6502, 0.9494, 0.1355, ..., 0.0834, 0.4446, 0.5004], ..., [0.2966, 0.6502, 0.6373, ..., 0.6158, 0.9067, 0.0245], [0.1544, 0.6203, 0.7105, ..., 0.5898, 0.9481, 0.7846], [0.0048, 0.8663, 0.5166, ..., 0.7284, 0.1925, 0.2311]], [[0.0852, 0.0139, 0.8834, ..., 0.6738, 0.1022, 0.8247], [0.0161, 0.2121, 0.9729, ..., 0.2736, 0.3918, 0.5949], [0.1229, 0.8208, 0.6334, ..., 0.0238, 0.5333, 0.0843], ..., [0.9250, 0.3495, 0.2272, ..., 0.2272, 0.7861, 0.1189], [0.7124, 0.8442, 0.6705, ..., 0.7530, 0.0809, 0.4165], [0.5223, 0.0803, 0.3681, ..., 0.3973, 0.7163, 0.1920]], [[0.1807, 0.7834, 0.5945, ..., 0.5394, 0.7165, 0.4785], [0.8470, 0.0618, 0.2278, ..., 0.5901, 0.4242, 0.0087], [0.9244, 0.1387, 0.8042, ..., 0.4879, 0.6511, 0.4556], ..., [0.2453, 0.7729, 0.5773, ..., 0.7876, 0.7998, 0.5125], [0.3097, 0.0141, 0.5490, ..., 0.2160, 0.4402, 0.9370], [0.5274, 0.9141, 0.0378, ..., 0.9517, 0.8165, 0.6193]]]])In [6]: a.shapeOut[6]: torch.Size([2, 3, 28, 28])In [7]: a.numel()Out[7]: 4704 numel函数的含义是numberof element,返回对应tensor中元素的个数 四维的tensor一般用于CNN中 创建Tensor 从numpy中导入数据 123456789In [4]: a=np.array([2,3.3])In [5]: torch.from_numpy(a)Out[5]: tensor([2.0000, 3.3000], dtype=torch.float64)In [6]: a=np.ones([2,3])In [7]: torch.from_numpy(a)Out[7]: tensor([[1., 1., 1.], [1., 1., 1.]], dtype=torch.float64) 从List列表中导入数据 1234567891011121314151617In [3]: torch.tensor([2.,3.2])Out[3]: tensor([2.0000, 3.2000])In [4]: torch.FloatTensor([2.,3.2])Out[4]: tensor([2.0000, 3.2000])In [5]: torch.FloatTensor(2,3)Out[5]: tensor([[0., 0., 0.], [0., 0., 0.]])In [6]: torch.tensor([[2.,3.2],[1.,22.3]])Out[6]: tensor([[ 2.0000, 3.2000], [ 1.0000, 22.3000]])In [7]: torch.Tensor(2,3)Out[7]: tensor([[0., 0., 0.], [0., 0., 0.]]) 注意区分大写Tensor和小写tensor，见In[3]和In [7]的区别，然后In [4]和In [5]也要进行区分。 生成未经过初始化的数据uninitalized torch.empty() torch.FloatTensor() torch.IntTensor(d1,d2,d3l) 1234567891011121314151617181920212223In [3]: torch.empty(1)Out[3]: tensor([0.])In [4]: torch.empty([2,3])Out[4]: tensor([[0., 0., 0.], [0., 0., 0.]])In [5]: torch.empty(2,3)Out[5]: tensor([[0., 0., 0.], [0., 0., 0.]])In [6]: torch.Tensor(2,3)Out[6]: tensor([[0., 0., 0.], [0., 0., 0.]])In [7]: torch.IntTensor(2,3)Out[7]: tensor([[0, 0, 0], [0, 0, 0]], dtype=torch.int32)In [8]: torch.FloatTensor(2,3)Out[8]: tensor([[0., 0., 0.], [0., 0., 0.]]) 非常不建议直接将未初始化的Tensor带入计算，有极大概率出现奇怪的问题。这是因为未经初始化的Tensor，随机生成的数据有可能会相差较大（比如极大或者极小），上面的代码并没有出现这样的问题，可能是因为运气比较好。 empty()加不加括号都可以 设置默认数据类型 PyTorch默认的Tensor类型是FloatTensor，因此在这种情况下，使用Tensor或tensor生成的tensor的数据类型都是FloatTensor类型的。 设置Tensor默认的数据类型采用的函数是set_default_tensor_type 123456In [3]: torch.tensor([1.2,3]).type()Out[3]: &#x27;torch.FloatTensor&#x27;In [4]: torch.set_default_tensor_type(torch.DoubleTensor)In [5]: torch.tensor([1.2,3]).type()Out[5]: &#x27;torch.DoubleTensor&#x27; 随机初始化 rand():随机生成(0,1)之间的数（均匀分布） rand_like：传入参数是一个Tensor，就rand一个和传入tensor形状一样的tensor randint：指定最小值和最大值以及Tensor的维度信息，然后进行随机。 randint_like：同randlike,这里不再赘述，详情见上面的rand_like函数 randn：随机生成(0,1)之间的数（正态分布） full：给定维度信息和对应的数，使用该数初始化给定维度的tensor。 1234567891011121314151617181920212223242526272829303132333435363738In [3]: torch.rand(3,3)Out[3]: tensor([[0.2722, 0.3876, 0.2607], [0.2612, 0.5041, 0.4084], [0.2007, 0.0119, 0.1834]])In [4]: a=torch.rand(3,3)In [5]: torch.rand_like(a)Out[5]: tensor([[0.1392, 0.9533, 0.5443], [0.9043, 0.4646, 0.0362], [0.7572, 0.5375, 0.8975]])In [6]: b=torch.randint(1,10,[2,3])In [7]: bOut[7]: tensor([[5, 6, 9], [5, 1, 1]])In [8]: c=torch.randint_like(b,1,100)In [9]: cOut[9]: tensor([[42, 20, 59], [15, 51, 1]])In [10]: torch.randn(3,3)Out[10]: tensor([[ 0.6524, -1.4728, 0.0623], [-0.6681, 0.7447, 0.9798], [ 0.0814, 1.3355, -0.7414]])In [11]: torch.full([5,2],0.1213)Out[11]: tensor([[0.1213, 0.1213], [0.1213, 0.1213], [0.1213, 0.1213], [0.1213, 0.1213], [0.1213, 0.1213]])In [12]: torch.full([],7)#生成标量Out[12]: tensor(7)In [13]: torch.full([1],7)#生成一维向量Out[13]: tensor([7]) arange 传入两个或三个参数，分别表示起始和终止位置（左闭右开），然后第三个参数是步长，不写就默认是1。 12345In [3]: torch.arange(0,10)Out[3]: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [4]: torch.arange(0,10,2)Out[4]: tensor([0, 2, 4, 6, 8]) linspace/logspace linspace和arange非常的像，但是第三个参数不表示步长了，而是表示的是数量，还有一个非常重要的不同是，这里的终止点是包含在了分割的序列中的。（见下面的例子） logspace先是同linspace一样，将起始点和终止点之间的数据划分成给定数量的等差数列，然后将这些值作为指数，10作为底数（当然底数也可以自己通过改变base参数进行设置），计算出来的序列作为最后呈现的答案。 1234567891011121314151617In [3]: torch.linspace(0,10,steps=5)Out[3]: tensor([ 0.0000, 2.5000, 5.0000, 7.5000, 10.0000])In [4]: torch.linspace(0,10,steps=10)Out[4]: tensor([ 0.0000, 1.1111, 2.2222, 3.3333, 4.4444, 5.5556, 6.6667, 7.7778, 8.8889, 10.0000])In [5]: torch.linspace(0,10,steps=11)Out[5]: tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.])In [6]: torch.logspace(0,-1,steps=10)Out[6]: tensor([1.0000, 0.7743, 0.5995, 0.4642, 0.3594, 0.2783, 0.2154, 0.1668, 0.1292, 0.1000])In [7]: torch.logspace(0,10,steps=11,base=2)Out[7]: tensor([1.0000e+00, 2.0000e+00, 4.0000e+00, 8.0000e+00, 1.6000e+01, 3.2000e+01, 6.4000e+01, 1.2800e+02, 2.5600e+02, 5.1200e+02, 1.0240e+03]) ones/zeros/eye 这个非常简单，大家直接看下面的例子就可以明白，这里就不再赘述。 12345678910111213141516171819202122232425In [3]: torch.ones(3,3)Out[3]: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])In [4]: torch.zeros(3,3)Out[4]: tensor([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])In [5]: torch.eye(3,3)Out[5]: tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])In [6]: torch.eye(3,5)Out[6]: tensor([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.]])In [7]: torch.eye(3)Out[7]: tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) like方法在这里也可以使用哦。比如ones_like randperm 随机打乱函数，一般用于打乱索引编号。 123In [3]: idx=torch.randperm(10)In [4]: idxOut[4]: tensor([8, 6, 9, 5, 1, 2, 4, 7, 3, 0]) Tensor 切片 简单切片 123456789In [3]: a=torch.rand(4,3,28,28)In [4]: a[0].shapeOut[4]: torch.Size([3, 28, 28])In [5]: a[0,1].shapeOut[5]: torch.Size([28, 28])In [6]: a[0,1,2,3]Out[6]: tensor(0.4744)In [7]: a[:2].shapeOut[7]: torch.Size([2, 3, 28, 28]) 选区间 123456789In [9]: a[:2,:1,:,:].shapeOut[9]: torch.Size([2, 1, 28, 28])In [10]: a[:2,1:,:,:].shapeOut[10]: torch.Size([2, 2, 28, 28])In [11]: a[:2,-1,:,:].shapeOut[11]: torch.Size([2, 28, 28])In [12]: a[:2,-1:,:,:].shapeOut[12]: torch.Size([2, 1, 28, 28]) 有步长的切片 1234In [14]: a[:,:,0:28:2,0:28:2].shapeOut[14]: torch.Size([4, 3, 14, 14])In [15]: a[:,:,::2,::2].shapeOut[15]: torch.Size([4, 3, 14, 14]) 特定索引切片 使用index_select这个函数 传入两个参数，第一个参数是一个标量表示对第几个维度进行索引操作，第二个参数是一个tensor数组，表示我们想取出来的索引号。 123456789101112131415In [21]: a=torch.eye(3)In [22]: aOut[22]: tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])In [23]: a.index_select(0,torch.tensor([0,2]))Out[23]: tensor([[1., 0., 0.], [0., 0., 1.]])In [24]: a.index_select(0,torch.arange(2))Out[24]: tensor([[1., 0., 0.], [0., 1., 0.]]) … 12345678910111213In [3]: a=torch.rand(4,3,28,28)In [4]: a.shapeOut[4]: torch.Size([4, 3, 28, 28])In [5]: a[...].shapeOut[5]: torch.Size([4, 3, 28, 28])In [6]: a[0,...].shapeOut[6]: torch.Size([3, 28, 28])In [7]: a[:,1,...].shapeOut[7]: torch.Size([4, 28, 28])In [8]: a[...,:2].shapeOut[8]: torch.Size([4, 3, 28, 2]) 通过遮罩取数 使用到的函数是masked_selected，这个函数并不是非常的常用。 123456789101112131415In [3]: z=torch.randn(3,4)In [4]: zOut[4]: tensor([[-0.9371, 1.0520, 0.2948, 0.3691], [ 0.9820, 0.9555, -0.9335, -1.5135], [-0.2594, -0.7860, -0.5336, 0.2682]])In [5]: mask=z.ge(0.5)In [6]: maskOut[6]: tensor([[False, True, False, False], [ True, True, False, False], [False, False, False, False]])In [7]: torch.masked_select(z,mask)Out[7]: tensor([1.0520, 0.9820, 0.9555]) 通过take来进行取数 take取数会将tensor打平然后再根据新的索引进行取数操作，详情见下面的例子。 123In [3]: src=torch.tensor([[4,3,5],[6,7,8]])In [4]: torch.take(src,torch.tensor([0,2,5]))Out[4]: tensor([4, 5, 8]) Tensor 维度变换 view/reshape squeeze/unsqueeze transpose/t/permute expand/repeat reshape/view reshape和view是完全等价的，详情见下面的例子。 12345678In [3]: a=torch.rand(4,1,28,28)In [4]: a.shapeOut[4]: torch.Size([4, 1, 28, 28])In [5]: a.view(4,28*28).shapeOut[5]: torch.Size([4, 784])In [6]: a.reshape(4,28*28).shapeOut[6]: torch.Size([4, 784]) squeeze/unsqueeze unsqueeze 对于一个维度为n的tensor，想要扩展维度所能够传入参数的范围是**$[-n-1,n+1)$**如果输入的是正的索引（或0）则是在之前插入，负的索引则是在之后插入 123456789101112131415In [3]: a=torch.randn(4,1,28,28)In [4]: a.shapeOut[4]: torch.Size([4, 1, 28, 28])In [5]: a.unsqueeze(0).shapeOut[5]: torch.Size([1, 4, 1, 28, 28])In [6]: a.unsqueeze(-1).shapeOut[6]: torch.Size([4, 1, 28, 28, 1])In [7]: a.unsqueeze(4).shapeOut[7]: torch.Size([4, 1, 28, 28, 1])In [8]: a.unsqueeze(-4).shapeOut[8]: torch.Size([4, 1, 1, 28, 28])In [9]: a.unsqueeze(-5).shapeOut[9]: torch.Size([1, 4, 1, 28, 28])In [10]: a.unsqueeze(5).shapeOut[10]: RuntimeError: Dimension out of range squeeze 1234567891011121314In [3]: b=torch.rand(1,32,1,1)In [4]: b.shapeOut[4]: torch.Size([1, 32, 1, 1])In [5]: b.squeeze().shapeOut[5]: torch.Size([32])In [6]: b.squeeze(0).shapeOut[6]: torch.Size([32, 1, 1])In [7]: b.squeeze(-1).shapeOut[7]: torch.Size([1, 32, 1])In [8]: b.squeeze(1).shape # can&#x27;t squeeze this dimensionOut[8]: torch.Size([1, 32, 1, 1])In [9]: b.squeeze(-4).shapeOut[9]: torch.Size([32, 1, 1]) expand/repeat expand 这是维度扩展函数和维度增加函数还是有较大不同的，注意区分！维度增加是增加了一个新的维度，而维度扩展是将当前已有维度改变其shape的大小。 两个API所实现的功能是完全一样的，但是我们更加推荐使用第一个API，因为第一个API是在数据需要使用时才进行复制，因此expand的执行速度更快，并且更加节约内存。 只有原来维度为1，才能够被扩展！！！ 如果expand中填写的参数为-1，表示的是维度大小保持不变。 123456789101112131415161718In [3]: a=torch.rand(1,3)In [4]: aOut[4]: tensor([[0.2768, 0.6714, 0.6259]])In [5]: a.shapeOut[5]: torch.Size([1, 3])In [6]: a.expand(4,3)Out[6]: tensor([[0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259]])In [7]: a.expand(4,-1)Out[7]: tensor([[0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259], [0.2768, 0.6714, 0.6259]]) repeat 传入的参数和expand函数有一些不同，这里所传入的参数代表拷贝的次数，比如如果原来维度大小是32，下面对应参数填写为2，最后生成的大小就是64。 12345678910In [3]: a=torch.rand(1,32,1,1)In [4]: a.repeat(4,32,1,1).shapeOut[4]: torch.Size([4, 1024, 1, 1])In [5]: a.repeat(4,1,1,1).shapeOut[5]: torch.Size([4, 32, 1, 1])In [6]: a.repeat(4,1,32,32).shapeOut[6]: torch.Size([4, 32, 32, 32])In [7]: a.repeat(4,1,32,0).shapeOut[7]: torch.Size([4, 32, 32, 0]) transpose/t/permute t 矩阵转置操作 这个函数只适用于二维tensor，其他维度的tensor使用会报错 12345678910111213In [3]: a=torch.rand(3,4)In [4]: aOut[4]: tensor([[0.0723, 0.9169, 0.2574, 0.9654], [0.5009, 0.9152, 0.6344, 0.7596], [0.5432, 0.3906, 0.9426, 0.9421]])In [5]: a.t()Out[5]: tensor([[0.0723, 0.5009, 0.5432], [0.9169, 0.9152, 0.3906], [0.2574, 0.6344, 0.9426], [0.9654, 0.7596, 0.9421]]) transpose 矩阵维度交换函数，直接输入两个需要进行交换的维度，就可以直接将这两个维度进行交换，同时这两个维度存储的信息也会进行交换。 12345678910In [3]: a=torch.rand(4,3,32,32)In [4]: a1=a.transpose(1,3).contiguous().view(4,3*32*32).view(4,3,32,32)In [5]: a2=a.transpose(1,3).contiguous().view(4,3*32*32).view(4,32,32,3).transpose(1,3)In [6]: a1.shape,a2.shapeOut[6]: (torch.Size([4, 3, 32, 32]), torch.Size([4, 3, 32, 32]))In [7]: torch.all(torch.eq(a,a1))Out[7]: tensor(False)In [8]: torch.all(torch.eq(a,a2))Out[8]: tensor(True) contiguous函数是使内存顺序变得连续，不然会如果有以下写法会出现报错。（因为view会忽视维度信息） permute 和transpose比较类似，但是很好的解决了transpose每次只能交换两个维度的问题，我们输入的参数，是维度的排列顺序，这样就可以同时交换多个维度。 12345In [3]: a=torch.rand(1,2,3,4)In [4]: a.shapeOut[4]: torch.Size([1, 2, 3, 4])In [5]: a.permute(3,0,2,1).shapeOut[5]: torch.Size([4, 1, 3, 2])"},{"title":"PyTorch梯度","path":"/wiki/PyTorch/PyTorch梯度.html","content":"梯度 梯度的方向代表的是从小到大的方向 θt+1=θt−αt∇f(θt)\\theta_{t+1}=\\theta_t-\\alpha_t abla f(\\theta_t) θt+1​=θt​−αt​∇f(θt​) An overview of gradient descent optimization algorithms (ruder.io) 搜不到全局最小值的原因 局部最小值 鞍点 大部分情况下，鞍点比局部最小值带来的影响更为严重 常见函数的梯度 激活函数和梯度 sigmoid sigmoid(x)=11+e−xsigmoid(x)=\\frac{1}{1+e^{-x}} sigmoid(x)=1+e−x1​ 优点 连续 光滑 范围01，适合于一些输出需要控制在01的场景（eg:概率，rgb值） 缺点 当范围趋于+∞+\\infty+∞或−∞-\\infty−∞时，因为导数较小，所以参数更新会非常的缓慢，收敛速度慢 PyTorch实现 1234567891011121314151617In [3]: a=torch.linspace(-100,100,10)In [4]: aOut[4]: tensor([-100.0000, -77.7778, -55.5556, -33.3333, -11.1111, 11.1111, 33.3333, 55.5556, 77.7778, 100.0000])In [5]: torch.sigmoid(a)Out[5]: tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00])In [6]: from torch.nn import functional as FIn [7]: F.sigmoid(a)D:\\App\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch n\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;)Out[7]: tensor([0.0000e+00, 1.6655e-34, 7.4564e-25, 3.3382e-15, 1.4945e-05, 9.9999e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00]) tanh tanh(x)=ex−e−xex+e−x=2sigmoid(2x)−1tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1 tanh(x)=ex+e−xex−e−x​=2sigmoid(2x)−1 ddxtanh(x)=1−tanh2(x)\\frac{d}{dx}tanh(x)=1-tanh^2(x) dxd​tanh(x)=1−tanh2(x) 在RNN中使用较多。 PyTorch实现 12345In [3]: a=torch.linspace(-1,1,10)In [4]: torch.tanh(a)Out[4]: tensor([-0.7616, -0.6514, -0.5047, -0.3215, -0.1107, 0.1107, 0.3215, 0.5047, 0.6514, 0.7616]) Rectified Linear Unit(ReLU) f(x)={0for x&lt;0xfor x≥0f(x)=\\begin{cases} 0\\quad for\\ x&lt;0\\\\ x\\quad for\\ x\\ge0 \\end{cases} f(x)={0for x&lt;0xfor x≥0​ 经过大量实验验证，ReLU函数被证明非常适用于深度学习。 Pytoch实现 12345678910In [3]: a=torch.linspace(-1,1,10)In [4]: torch.relu(a)Out[4]: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778, 1.0000])In [5]: aOut[5]: tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111, 0.1111, 0.3333, 0.5556, 0.7778, 1.0000]) Loss函数和梯度 常见的Loss函数 均方差Loss函数（Mean Square Error） Cross Entropy Loss 二分类（binary） 多分类（multi-class） softmax MSE loss=∑[y−(xw+b)2]loss=\\sum[y-(xw+b)^2] loss=∑[y−(xw+b)2] 注意和2范数进行区分（L2_norm） L2_norm=∣∣y−(xw+b)∣∣2L2\\_norm=||y-(xw+b)||_2 L2_norm=∣∣y−(xw+b)∣∣2​ loss=norm(y−(xw+b))2loss=norm(y-(xw+b))^2 loss=norm(y−(xw+b))2 PyTorch中进行表示结果如下： 1torch.norm(y-pred,2).pow(2) 对mse函数求导表达式如下： loss=∑[y−fθ(x)]2loss=\\sum[y-f_\\theta(x)]^2 loss=∑[y−fθ​(x)]2 ∇loss∇θ=2∑[y−fθ(x)]∗∇fθ(x)∇θ\\frac{ abla loss}{ abla \\theta}=2\\sum[y-f_\\theta(x)]*\\frac{ abla f_\\theta(x)}{ abla\\theta} ∇θ∇loss​=2∑[y−fθ​(x)]∗∇θ∇fθ​(x)​ 补充：在PyTorch中实现自动求导※ autograd.grad 123456789In [3]: x=torch.ones(1)In [4]: w=torch.tensor([2.],requires_grad=True)# importantIn [5]: from torch.nn import functional as FIn [6]: mse=F.mse_loss(torch.ones(1),x*w)#label,predIn [7]: mseOut[7]: tensor(1., grad_fn=&lt;MseLossBackward0&gt;)In [8]: torch.autograd.grad(mse,[w])Out[8]: (tensor([2.]),) loss.backward※ 接上面 1234In [9]: mse=F.mse_loss(torch.ones(1),x*w)In [10]: mse.backward()In [11]: w.gradOut[11]: tensor([2.]) 一般使用下面这种 补充：softmax soft version of max 我们假设，左边没有进入softmax层的特征向量为aaa右边经过了softmax层的特征向量为ppp,那么有。 ∂pi∂aj={pi(1−pj)if i=j−pjpiif i≠j\\frac{\\partial p_i}{\\partial a_j}=\\begin{cases} p_i(1-p_j)\\quad if \\ i=j\\\\ -p_jp_i\\quad if\\ i e j \\end{cases} ∂aj​∂pi​​={pi​(1−pj​)if i=j−pj​pi​if i=j​ pytorch 代码验证如下： 12345678910In [3]: a=torch.rand(3)In [4]: a.requires_grad_()Out[4]: tensor([0.1714, 0.4650, 0.7201], requires_grad=True)In [5]: from torch.nn import functional as FIn [6]: p=F.softmax(a,dim=0)In [7]: p.sum().backward()# 如果这里不求和，就要在backward中 指定一个和p一样大的tensor，详情见：下面的红色框中的blogIn [8]: a.gradOut[8]: tensor([0., 0., 0.])In [9]: p.gradOut[9]: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#x27;t be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:417.) [gradcan be implicitly created only for scalar outputs_](https://blog.csdn.net/qq_39208832/article/details/117415229#:~:text=1.1 grad can be implicitly created only for,是一个 标量 (即它包含一个元素的数据），则不需要为 backward () 指定任何参数，但是如果它有更多的元素，则需要指定一个 gradient 参数，该参数是形状匹配的张量。) Ps：这里将p求和，和不将p求和对a求导的最终结果是一样的，因为根据链式法则，最后求和每个p的部分前面的系数都是1，所以乘上1不影响最终的结果。 可以发现最后我们是**无法查看**最终的p.sum()对中间变量p的求导参数的(**不保存中间变量的梯度信息**)。详情见下面这篇博客： 通俗讲解PyTorch梯度的相关问题：计算图、torch.no_grad、zero_grad、detach和backward；Variable、Parameter和torch.tensor 这其中也有讲重复backward报错的问题！要解决这个问题，每次backward将retain_graph设为True就好了。 backward(retain_graph=True) 记得再次进行backward前，要根据自己需求确定是否使用a.grad.zero_()将梯度清零。不然梯度会在上一次求出来的基础上进行叠加。 Cross Entropy Loss（交叉熵） 熵（Entropy） 不确定性 用于衡量惊喜程度 熵值越高：越高的不确定性 Entropy=−∑iP(i)logP(i)Entropy=-\\sum_iP(i)logP(i) Entropy=−i∑​P(i)logP(i) 交叉熵（Cross Entropy） 数学定义 H(p,q)=−∑p(x)log q(x)H(p,q)=-\\sum p(x)log \\ q(x) H(p,q)=−∑p(x)log q(x) H(p,q)=H(p)+DKL(p∣q)H(p,q)=H(p)+D_{KL}(p|q) H(p,q)=H(p)+DKL​(p∣q) DKLD_{KL}DKL​是散度，用于衡量两个分布的接近程度的，散度越小，分布越接近。 这里p是我们将p定为网络学习出来的分布，q为实际数据的分布，所以我们直观理解优化目标就是，网络学习出来的分布要有较高的确定性（不能觉得同时归属于几个种类的概率是一样的）。也要和实际分布接近（散度小）。 因此对于二分类，我们的 Cross Ebtropy Loss函数可以写为： H(P,Q)=−(ylog(p)+(1−y)log(1−p))H(P,Q)=-(ylog(p)+(1-y)log(1-p)) H(P,Q)=−(ylog(p)+(1−y)log(1−p)) 为什么使用交叉熵 对于分类问题 基于sigmoid的mse容易发生梯度消失的问题 收敛缓慢 PyTorch实例 注意:cross_entropy函数包含了求softmax，log这些步骤。 PyTorch单层单输出感知机实战 上标代表第几层，下标代表特征向量的第几个元素。 求导过程如下： 这样得知了∂E∂wj0\\frac{\\partial E}{\\partial w_{j0}}∂wj0​∂E​后，我们便可以更新w了。 下面使用PyTorch简单的实现上述单层单输出感知机。 123456789101112131415In [3]: x=torch.randn(1,10)In [4]: w=torch.randn(1,10,requires_grad=True)In [5]: o=torch.sigmoid(x@w.t())In [6]: o.shapeOut[6]: torch.Size([1, 1])In [7]: from torch.nn import functional as FIn [8]: loss=F.mse_loss(torch.ones(1,1),o)In [9]: loss.shapeOut[9]: torch.Size([])In [10]: loss.backward()In [11]: w.gradOut[11]: tensor([[-0.1147, -0.2456, -0.2645, 0.1144, -0.0162, 0.1094, -0.3674, -0.0048, -0.1127, -0.1605]]) 然后，我们便可以使用w.grad对w进行更新啦！ PyTorch多输出感知机实战 求导推导过程如下： 下面使用PyTorch简单的实现上述单层多输出感知机。 1234567891011121314151617In [3]: x=torch.randn(1,10)In [4]: w=torch.randn(2,10,requires_grad=True)In [5]: o=torch.sigmoid(x@w.t())In [6]: o.shapeOut[6]: torch.Size([1, 2])In [7]: from torch.nn import functional as FIn [8]: loss=F.mse_loss(torch.ones(1,2),o)In [9]: lossOut[9]: tensor(0.6030, grad_fn=&lt;MseLossBackward0&gt;)In [10]: loss.backward(retain_graph=True)In [11]: w.gradOut[11]: tensor([[-0.1720, -0.1305, -0.0129, 0.0506, -0.0449, 0.1076, 0.0133, 0.0291, 0.0757, 0.0186], [-0.0033, -0.0025, -0.0002, 0.0010, -0.0009, 0.0021, 0.0003, 0.0006, 0.0015, 0.0004]]) 链式法则※ 通过使用链式法则，我们可以把最后一层的误差，一层一层的输出到中间层的权值上面去，从而得到中间层的梯度信息，进而很好的更新权值，达到反向传播优化模型的效果 PyTorch实验 12345678910111213141516In [3]: from torch import autogradIn [4]: x=torch.tensor(1.)In [5]: w1=torch.tensor(2.,requires_grad=True)In [6]: b1=torch.tensor(1.)In [7]: w2=torch.tensor(2.,requires_grad=True)In [8]: b2=torch.tensor(1.)In [9]: y1=x*w1+b1In [10]: y2=y1*w2+b2In [11]: dy2_dy1=autograd.grad(y2,[y1],retain_graph=True)[0]In [12]: dy1_dw1=autograd.grad(y1,[w1],retain_graph=True)[0]In [13]: dy2_dw1=autograd.grad(y2,[w1],retain_graph=True)[0]In [14]: dy2_dy1*dy1_dw1Out[14]: tensor(2.)In [15]: dy2_dw1Out[15]: tensor(2.) 由上证明了链式法则的正确性！ MLP反向传播 多层感知机 其实原理非常的简单，就是正向传播完成后，倒着一层一层计算导数，每一层的倒数计算同上面的单层单输出感知机和单层多输出感知机。所以整个反向传播的过程相当于是很多个单层n输出感知机接在一起。（感知机的推到步骤见上面） 2D函数优化实例 Himmelblau function 这里我们采用的函数表达式如下： f(x,y)=(x2+y−11)2+(x+y2−7)2f(x,y)=(x^2+y-11)^2+(x+y^2-7)^2 f(x,y)=(x2+y−11)2+(x+y2−7)2 如下图所示 该函数在以下四点取得全局最小值： f(3.0,2.0)=0.0f(3.0,2.0)=0.0f(3.0,2.0)=0.0 f(−2.805118,3.131312)=0.0f(-2.805118,3.131312)=0.0f(−2.805118,3.131312)=0.0 f(−3.779310,−3.283186)=0.0f(-3.779310,-3.283186)=0.0f(−3.779310,−3.283186)=0.0 f(3.584428,−1.848126)=0.0f(3.584428,-1.848126)=0.0f(3.584428,−1.848126)=0.0 首先是画图代码： 1234567891011121314151617181920import numpy as npimport matplotlib.pyplot as pltimport torchdef himmelblau(x): return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2x=np.arange(-6,6,0.1)y=np.arange(-6,6,0.1)print(&#x27;x,y range:&#x27;,x.shape,y.shape)X,Y=np.meshgrid(x,y)print(&#x27;X,Y maps:&#x27;,X.shape,Y.shape)Z=himmelblau([X,Y])fig=plt.figure(&#x27;himmelblau&#x27;)ax=fig.gca(projection=&#x27;3d&#x27;)ax.plot_surface(X,Y,Z)ax.view_init(60,-30)ax.set_xlabel(&#x27;x&#x27;)ax.set_ylabel(&#x27;y&#x27;)plt.show() 画图结果如下： 然后就是求导找出最优解了，代码如下： 具体细节见代码中的注释 12345678910111213141516# 下面使用随机梯度下降的方式进行求解全局最小值x=torch.tensor([0.,0.],requires_grad=True)# 使用优化器，优化器会自动根据梯度信息和学习率来更新目标（这里是x）的值optimizer=torch.optim.Adam([x],lr=1e-3)# 初始化优化器for step in range(20000): pred=himmelblau(x) optimizer.zero_grad()# 清零梯度（不然会出现梯度累加） pred.backward() # 自动求导获取梯度信息 optimizer.step()# 优化器执行一次更新 if step%2000==0: print(&#x27;step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;&#x27; .format(step,x.tolist(),pred.item())) 最终运行结果如下所示，可以发现最终是找到了一个全局最优解： 了解了PyTorch最基本的梯度知识后，下面我们将继续学习如何使用PyTorch构造简单的神经网络了。"},{"title":"PyTorch神经网络","path":"/wiki/PyTorch/PyTorch神经网络.html","content":"Logistic Regression 该概念现目前已经完全被Classification替换掉 就是通过输出加sigmoid函数，让输出接近0或1，达到分类的效果。 目标：最小化dist(pred,y) Logistic Regression 一般使用交叉熵作为Loss函数，在Pytoch梯度中的交叉熵一节有对交叉熵loss函数详细的讲解，这里就不再赘述。 多分类问题实战——函数API实现 前面对这一部分的介绍已经非常详细了，这里就不再赘述。 网络结构 123456789101112131415161718192021w1, b1 = torch.randn(200, 784, requires_grad=True),\\ torch.zeros(200, requires_grad=True)w2, b2 = torch.randn(200, 200, requires_grad=True),\\ torch.zeros(200, requires_grad=True)w3, b3 = torch.randn(10, 200, requires_grad=True),\\ torch.zeros(10, requires_grad=True)# 以下三行是kaiming初始化torch.nn.init.kaiming_normal_(w1)torch.nn.init.kaiming_normal_(w2)torch.nn.init.kaiming_normal_(w3)def forward(x): x = x@w1.t() + b1 x = F.relu(x) x = x@w2.t() + b2 x = F.relu(x) x = x@w3.t() + b3 x = F.relu(x) # logits return x 注意：初始化中tensor第一个维度是out（下一层向量长度），第二个维度是in（这一层向量长度） 训练过程 训练过程的原理和博客PyTorch梯度最后讲的一样，这里不再赘述 123456789101112131415161718192021optimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)criteon = nn.CrossEntropyLoss()for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) logits = forward(data) loss = criteon(logits, target) # 包含了softmax和log求导部分 optimizer.zero_grad() loss.backward() # print(w1.grad.norm(), w2.grad.norm()) optimizer.step() if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) 汇总 采用的是MINIST数据集,代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformsbatch_size=200learning_rate=0.01epochs=10train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)w1, b1 = torch.randn(200, 784, requires_grad=True),\\ torch.zeros(200, requires_grad=True)w2, b2 = torch.randn(200, 200, requires_grad=True),\\ torch.zeros(200, requires_grad=True)w3, b3 = torch.randn(10, 200, requires_grad=True),\\ torch.zeros(10, requires_grad=True)# 以下三行是kaiming初始化torch.nn.init.kaiming_normal_(w1)torch.nn.init.kaiming_normal_(w2)torch.nn.init.kaiming_normal_(w3)def forward(x): x = x@w1.t() + b1 x = F.relu(x) x = x@w2.t() + b2 x = F.relu(x) x = x@w3.t() + b3 x = F.relu(x) return xoptimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)criteon = nn.CrossEntropyLoss()for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) logits = forward(data) loss = criteon(logits, target) # 包含了softmax和log求导部分 optimizer.zero_grad() loss.backward() # print(w1.grad.norm(), w2.grad.norm()) optimizer.step() if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) test_loss = 0 correct = 0 for data, target in test_loader: data = data.view(-1, 28 * 28) logits = forward(data) test_loss += criteon(logits, target).item() pred = logits.data.max(1)[1] correct += pred.eq(target.data).sum() test_loss /= len(test_loader.dataset) print(&#x27; Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) &#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 全连接层——类API实现 网络结构 在上一次的实践中，我们使用分散的w，b tensor实现了一个非常简单的三层全连接层（函数API实现）。但是这样虽然简单，但是不够直观，变量稍微有点多，这次我们介绍使用PyTorch自带的API实现一个和上面一样的三层全连接层。 1234567891011121314151617In [3]: x=torch.randn(1,784)In [4]: x.shapeOut[4]: torch.Size([1, 784])In [5]: from torch import nnIn [6]: layer1=nn.Linear(784,200)In [7]: layer2=nn.Linear(200,200)In [8]: layer3=nn.Linear(200,10)In [9]: x=layer1(x)In [10]: x.shapeOut[10]: torch.Size([1, 200])In [11]: x=layer2(x)In [12]: x.shapeOut[12]: torch.Size([1, 200])In [13]: x=layer3(x)In [14]: x.shapeOut[14]: torch.Size([1, 10]) 这个只是比较像，但是还是有些区别，因为我们还没有加层之间的激活函数。 下面我们在层之间添加激活函数： 12345678910111213141516171819In [3]: from torch import nnIn [4]: import torch.nn.functional as FIn [5]: x=torch.randn(1,784)In [6]: layer1=nn.Linear(784,200)In [7]: layer2=nn.Linear(200,200)In [8]: layer3=nn.Linear(200,10)In [9]: x=layer1(x)In [10]: x=F.relu(x,inplace=True)In [11]: x.shapeOut[11]: torch.Size([1, 200])In [12]: x=layer2(x)In [13]: x=F.relu(x,inplace=True)In [14]: x.shapeOut[14]: torch.Size([1, 200])In [15]: x=layer3(x)In [16]: x=F.relu(x,inplace=True)In [17]: x.shapeOut[17]: torch.Size([1, 10]) relu中的inplace参数如果设置为true的话，节省了内存，相当于输出和输入用同一份内存。 学会了如上API定义网络，因为一般情况下的工程，网络都会定义成为一个类，所以这里，我们学习如何将网络定义为一个类。 12345678910111213141516class MPL(nn.Module): def __init__(self): super(MLP,self).__init__() self.model=nn.Sequential( #线性容器，可以容纳所有nn.Module类 nn.Linear(784,200), nn.ReLU(inplace=True), nn.Linear(200,200), nn.ReLU(inplace=True), nn.Linear(200,10), nn.ReLU(inplace=True), ) def forward(self,x): x=self.model(x) return x 注意区分F.relu(x,inplace=True)和上面nn.ReLU(inplace=True)这两种类型的API，前者是函数类型API，其中的tensor支持自己管理，后者是类-类型API，tensor为类内部变量不能随意访问，使用也必须将类实例化后才可以使用。 训练过程 代码如下，详情见注释： 123456789101112131415net=MLP()# 实例化网络optimizer=optim.SGD(net.parameters(),lr=learning_rate) # 这里使用parameters()自动加载目标变量criteon = nn.CrossEntropyLoss()for epoch in range(epochs): for batch_idx, (data,target) in enumerate(train_loader): data=data.view(-1,28*28) logits=net(data) # 重载forward后，直接传入参数默认forward loss=criteon(logits,target) optimizer.zero_grad() loss.backward() optimizer.step() 汇总 采用类API书写，使用的是MINIST数据集,代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformsbatch_size=200learning_rate=0.01epochs=10train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.model = nn.Sequential( nn.Linear(784, 200), nn.ReLU(inplace=True), nn.Linear(200, 200), nn.ReLU(inplace=True), nn.Linear(200, 10), nn.ReLU(inplace=True), ) def forward(self, x): x = self.model(x) return xnet = MLP()optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss()for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) logits = net(data) loss = criteon(logits, target) optimizer.zero_grad() loss.backward() # print(w1.grad.norm(), w2.grad.norm()) optimizer.step() if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) test_loss = 0 correct = 0 for data, target in test_loader: data = data.view(-1, 28 * 28) logits = net(data) test_loss += criteon(logits, target).item() pred = logits.data.max(1)[1] correct += pred.eq(target.data).sum() test_loss /= len(test_loader.dataset) print(&#x27; Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) &#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 以上为目前的主流版本，值得学习与参考！ 激活函数与GPU加速 激活函数 tanh——RNN sigmoid——probability ReLU——DL LeakyReLU——DL SELU——优化了ReLU在0点导数不连续的情况 softplus——同SELU，光滑了ReLU在0点处的连接 GPU加速 现目前较高版本的PyTorch已经可以使用to方法指定使用特定设备进行运算，而不必像原来使用不同设备进行相同的运算需要调用不同的API。 使用GPU cuda加速运算的代码如下图所示： 12345678910device = torch.device(&#x27;cuda:0&#x27;)net = MLP().to(device)optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss().to(device)for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) data, target = data.to(device), target.cuda()# 建议统一用to，这里只是想说明用.cuda()也是可以的 上述代码相当于是将网络，loss函数和所有的数据都搬运到了GPU上去。 汇总 采用的是MINIST数据集, 优化激活函数变为LeakyReLU 使用了GPU cuda加速 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformsbatch_size=200learning_rate=0.01epochs=10train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.model = nn.Sequential( nn.Linear(784, 200), nn.LeakyReLU(inplace=True), nn.Linear(200, 200), nn.LeakyReLU(inplace=True), nn.Linear(200, 10), nn.LeakyReLU(inplace=True), ) def forward(self, x): x = self.model(x) return xdevice = torch.device(&#x27;cuda:0&#x27;)net = MLP().to(device)optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss().to(device)for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) data, target = data.to(device), target.cuda() logits = net(data) loss = criteon(logits, target) optimizer.zero_grad() loss.backward() # print(w1.grad.norm(), w2.grad.norm()) optimizer.step() if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) test_loss = 0 correct = 0 for data, target in test_loader: data = data.view(-1, 28 * 28) data, target = data.to(device), target.cuda() logits = net(data) test_loss += criteon(logits, target).item() pred = logits.data.max(1)[1] correct += pred.eq(target.data).sum() test_loss /= len(test_loader.dataset) print(&#x27; Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) &#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 测试环节 可以发现前面所讲的内容只是覆盖了网络结构的初始化以及训练过程，并没有讲解测试过程，下面我们就来学习一下当PyTorch训练完成后，如何进行测试。 对于MINIST数据集其实就是将最后的算loss和loss.backward()去掉，直接将logits接一个softmax层（其实也可以不加）然后找到最大值的index即可（使用argmax函数）。 计算准确率就是用预测正确的数量除以总数量。 代码实现 下面是一个简单的测试环节的模拟代码，其中计算准确率这一部分还是有一些技术性的。 1234567891011121314151617In [3]: logits=torch.rand(4,10)In [4]: import torch.nn.functional as FIn [5]: pred=F.softmax(logits,dim=1)In [6]: pred.shapeOut[6]: torch.Size([4, 10])In [7]: pred_label=pred.argmax(dim=1)In [8]: pred_labelOut[8]: tensor([8, 0, 0, 0])In [9]: logits.argmax(dim=1)Out[9]: tensor([8, 0, 0, 0])In [10]: label=torch.tensor([8,0,1,2])In [11]: correct=torch.eq(pred_label,label)In [12]: correctOut[12]: tensor([ True, True, False, False])In [13]: correct.sum().float().item()/4Out[13]: 0.5 还有一些其他的评价参数，比如precision或recall，这些后面会单独写一篇博客进行讲解。 什么时候测试？ 在运行完几个Batch后进行一次test 运行完一个epoch后进行一次测试 汇总 代码汇总见上面的汇总模块 Visdom可视化 step1：安装visdom 1pip install visdom step2：开启visdom Web服务器 命令行中输入： 1python -m visdom.server step3：然后就可以将数据丢入visdom进行可视化查看了 1234from visdom import Visdomviz=Visdom()viz.line([0.],[0.],win=&#x27;train_loss&#x27;,opts=dict(title=&#x27;train_loss_title&#x27;))# 创建一条直线，前两个参数第一个是y，第二个是xviz.line([loss.item()],[global_step],win=&#x27;train_loss&#x27;,update=&#x27;append&#x27;)# 传入仍是numpy数据（image可以接收tensor） `win`：小窗口IDenv:大窗口ID，大窗口中可以有很多个小窗口，默认是main大窗口 update:若为append表示添加在当前直线的后面，若不指定会被覆盖掉 多条曲线一个窗口 上面的代码实现的是一条曲线一个窗口，下面我们来讲一下如何实现多条曲线画在一个窗口。 12345from matplotlib.pyplot import legendfrom visdom import Visdomviz=Visdom()viz.line([0.,0.],[0.],win=&#x27;test&#x27;,opts=dict(title=&#x27;train_loss&amp;acc&#x27;,legend=[&#x27;loss&#x27;,&#x27;acc&#x27;]))viz.line([test_loss,correct/len(test_loader.dataset)],[global_step],win=&#x27;train_loss&#x27;,update=&#x27;append&#x27;) 其实就是将y参数的list增加了一个长度，就可以一个小窗口画两条曲线。 visual x 这是visdom提供的一个可视化的功能 123456from matplotlib.pyplot import legendfrom visdom import Visdomviz=Visdom()#MINST为例viz.images(data.view(-1,1,28,28)，win=&#x27;x&#x27;) # 对于图片，这里可以直接接收tensor！！！viz.text(str(pred.detach().cpu().numpy()),win=&#x27;pred&#x27;,opts=dict(title=&#x27;pred&#x27;)) # 对于String类型还是要先转到cpu然年转numpy然后转string"},{"title":"PyTorch终章：开GAN！","path":"/wiki/PyTorch/PyTorch终章：开GAN！.html","content":"GAN简介 GAN的终极目的就是学习p(x)p(x)p(x),p(x)p(x)p(x)是一个分布，比如它可以是二次元头像图片特征的分布，可以是一种类型的画作的特征集合，我们学会了p(x)p(x)p(x)后，我们便可以在其中进行sample然后就可以进行创作了。这便是GAN的原理解释。 GAN 结构 生成器（Painter or Generator） 鉴别器（Critic or Discriminator） 大致结构如上，生成器根据随机生成的信号，产生一幅“画”。鉴别器使用很多真的和假的“画”进行训练，来分辨画的真假，从而产生一个打分值。分值越高表明画越真实（鉴别器看来）。鉴别器的目标是尽可能的分辨出真“画”和假“画”。生成器的目标是尽可能的最大化鉴别器的打分（相当于尽可能的欺骗鉴别器） GAN的出现让神经网络具有了创造性，当我们需要使用神经网络完成一些具有创造力的任务时，GAN是一个非常不错的选择。 这里推荐一个非常不错的关于GAN在线训练的网页链接 [GANplayground: Experiment with Generative Adversarial Networks in your browser ](https://reiinakano.com/gan-playground/) 下图是GAN网络的形象解释，绿线是我们要学习的物体的特征（比如二次元头像的特征）的分布，黑线是我们学习到的特征的分布，蓝线是鉴别器的输出，一开始生成器和鉴别器都没有进行训练，所以生成器生成的分布非常的烂，鉴别器也无法很好的鉴别图片是否是生成器生成的（如图(a)所示），紧接着我们训练鉴别器，然后可以发现在训练一段时间后鉴别器已经可以很好的鉴别图片的真伪了（图b）。接着我们训练生成器，生成器的目标是尽量让鉴别器认为图片是真的，从而给出高分，随着训练次数的增加，生成器生成的分布会越来越接近真实的分布（如图©所示），在最后的时候连鉴别器也无法识别生成器生成图片的真伪时，训练结束。（图d） GAN原理 GAN的训练分为两步。 固定生成器（G）训练鉴别器（D）使其收敛 固定鉴别器（D）训练生成器（G）使其收敛 下面我们就分别来详细说明一下这两步中的数学原理。 首先我们要明白我们的最终目标 min⁡Gmax⁡DL(D,G)=Ex∼pr(x)[log⁡D(x)]+Ez∼pz(z)[log⁡(1−D(G(z)))]\\min\\limits_{G}\\max\\limits_{D} L(D,G)=\\mathbb{E}_{x\\sim p_r(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1- D(G(z)))] Gmin​Dmax​L(D,G)=Ex∼pr​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))] min⁡Gmax⁡DL(D,G)=Ex∼pr(x)[log⁡D(x)]+Ex∼pg(x)[log⁡(1−D(x))]\\min\\limits_{G}\\max\\limits_{D} L(D,G)=\\mathbb{E}_{x\\sim p_r(x)}[\\log D(x)] + \\mathbb{E}_{x\\sim p_g(x)}[\\log(1- D(x))] Gmin​Dmax​L(D,G)=Ex∼pr​(x)​[logD(x)]+Ex∼pg​(x)​[log(1−D(x))] 纳什均衡——D 对于固定的G，最好的D是： DG∗(x)=pdata(x)pdata(x)+pg(x)D^*_G(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_g(x)} DG∗​(x)=pdata​(x)+pg​(x)pdata​(x)​ 训练D的准则（criterion）是对于给定的G最大化V(G,D)V(G,D)V(G,D) V(G,D)=∫xpdata(x)log⁡(D(x))dx+∫zp(z)log⁡(1−D(g(z)))dz=∫xpdata(x)log⁡(D(x))+pg(x)log⁡(1−D(x))dx\\begin{aligned} V(G,D) &amp;= \\int_x p_{data}(x)\\log(D(x))dx + \\int_z p(z)\\log(1-D(g(z)))dz\\\\ &amp;= \\int_xp_{data}(x)\\log(D(x))+p_g(x)\\log(1-D(x))dx \\end{aligned} V(G,D)​=∫x​pdata​(x)log(D(x))dx+∫z​p(z)log(1−D(g(z)))dz=∫x​pdata​(x)log(D(x))+pg​(x)log(1−D(x))dx​ 这个其实就是求期望，只是把它写成了积分形式。 我们对上面这个式子求导，就可以知道最好的DDD是上面的D∗D^*D∗ 纳什均衡——G 当我们训练完D后，下面就轮到G进行训练了。 L(G,D∗)=2DJS(pr∣∣pg)−2log⁡2L(G,D^*)=2D_{JS}(p_r||p_g)-2\\log2 L(G,D∗)=2DJS​(pr​∣∣pg​)−2log2 这便是欺骗鉴别器这一直观理解的目标函数表达形式，我们的目的是最小化这个函数。 GAN的问题 训练稳定性差 导致这个情况主要是有两个原因： 数据本身的特征 因为在很多情况下PGP_GPG​和PdataP_{data}Pdata​是几乎不可能重合的，因为这两个分布的特征是在高维特征空间中就几乎是两条线（仅用于说明），他们重合的部分几乎可以忽略。 采样 就算PGP_GPG​和PdataP_{data}Pdata​还是有一部分重合的，但是如果我们采样没有采够的话，还是有可能出现下图的情况，让我们认为两个分布没有重合。（点为实际的采样点，两个椭圆表示的是实际的两个分布情况。） JS 经过数学推导，只要两个分布不重叠，JS Divergence的取值永远都是log⁡2\\log 2log2。这个就没有很好的量化分布之间距离的这种关系。而且根据数据的自然特性我们也知道这两个分布是不好重叠的，或者说大概率是不会重叠的，JS Divergence一直不变会给梯度下降带来很大的问题，导致模型一直不收敛。这便是JS Divergence在GAN中使用的一个非常严重的问题。 解决JS的问题 我们采用了Wasserstein Distance来代替JS。其根本思想就是衡量将一个分布变成另一个分布所需要的最小代价。（直观理解就是搬砖，把一个砖堆变成另外一种砖堆所需的最小代价）。 采用Wasserstein Distance来代替JS的GAN被称为WGAN 简要计算步骤如下图所示。 WGAN的提出从根本上解决了部分GAN无法收敛(训练不稳定)的问题。 实战GAN 网络结构 首先是建立网络结构，GAN的网络结构包含两个部分，一个是生成器（Generator），还有一个是鉴别器（Discriminator）。代码非常简答，这里就不再赘述了。 1234567891011121314151617181920212223242526272829303132333435363738394041h_dim = 400batchsz = 512class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.net = nn.Sequential( nn.Linear(2, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, 2), ) def forward(self, z): output = self.net(z) return output class Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.net = nn.Sequential( nn.Linear(2, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, 1), nn.Sigmoid() ) def forward(self, x): output = self.net(x) return output.view(-1) 生成数据集 我们本次实验采用的数据集是统计学中经常使用的混合高斯模型，如下图所示，二维平面上一共是有8个高斯分布组合而成的一个混合分布图形。 相信大家也明白了为什么输出是两个神经元——因为要在平面坐标系上进行可视化输出嘛😂 数据集生成代码如下： 12345678910111213141516171819202122232425262728def data_generator(): &#x27;&#x27;&#x27; 8-gaussian mixture models :return: &#x27;&#x27;&#x27; scale = 2. centers = [ (1, 0), (-1, 0), (0, 1), (0, -1), (1. / np.sqrt(2), 1. / np.sqrt(2)), (1. / np.sqrt(2), -1. / np.sqrt(2)), (-1. / np.sqrt(2), 1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2)) ] centers = [(scale * x, scale * y) for x, y in centers] while True: dataset = [] for i in range(batchsz): point = np.random.randn(2) * .02 center = random.choice(centers) point[0] += center[0] point[1] += center[1] dataset.append(point) dataset = np.array(dataset, dtype=&#x27;float32&#x27;) dataset /= 1.414 # stdev yield dataset yield 这里大家可能好奇yield是干什么的，下面我举两个例子帮助大家理解。 以下关于yield的部分讲解转载自:https://blog.csdn.net/mieleizhi0522/article/details/82142856 例一 123456789101112def dataset_generator(): i=0 while True: i=i+1 yield iif __name__ == &#x27;__main__&#x27;: g = dataset_generator() for ii in range (10): print(next(g)) print(&quot;*&quot;*20) 代码运行结果是： 12345678910111213141516171819201********************2********************3********************4********************5********************6********************7********************8********************9********************10******************** 例二 1234567891011def foo(): print(&quot;starting...&quot;) while True: res = yield 4 print(&quot;res:&quot;,res)if __name__ == &#x27;__main__&#x27;: g = foo() print(&#x27;ok&#x27;) print(next(g)) print(&quot;*&quot;*20) print(next(g)) 代码运行结果是： 123456okstarting...4********************res: None4 到这里你可能就明白yield和return的关系和区别了，带yield的函数是一个生成器，而不是一个函数了，这个生成器有一个函数就是next函数，next就相当于“下一步”生成哪个数，这一次的next开始的地方是接着上一次的next停止的地方执行的，所以调用next的时候，生成器并不会从foo函数的开始执行，只是接着上一步停止的地方开始，然后遇到yield后，return出要生成的数，此步就结束。 训练部分 GAN的训练和其他神经网络的训练还有一点不太一样，GAN的训练是D和G分开训练，D先训练几轮后，定住D训练G，然后依次往复。有点像左脚踩右脚，原地升天那种感觉。 训练D 训练鉴别器分为三个部分，训练真实数据，训练假数据（G生成的），反向传播，首先是我们先给鉴别器输入真实的数据（代码中是xr，使用刚才我们写的data_generator生成）,鉴别器的输出是predr。我们的目标是最大化这一部分的输出（相当于最小化predr的相反数）。因此lossr = - (predr.mean())。然后我们再给鉴别器输入G生成的数据（代码中是xf）,鉴别器的输出是predf。我们的目标是最小化这一部分的输出（相当于最小化predr的相反数）。因此lossf = (predr.mean())。 1234567891011121314151617181920212223242526272829# 1. train discriminator for k stepsfor _ in range(5): x = next(data_iter) xr = torch.from_numpy(x).cuda() # [b] predr = (D(xr)) # max log(lossr) lossr = - (predr.mean()) # [b, 2] z = torch.randn(batchsz, 2).cuda() # stop gradient on G # [b, 2] xf = G(z).detach() # [b] predf = (D(xf)) # min predf lossf = (predf.mean()) # gradient penalty gp = gradient_penalty(D, xr, xf) loss_D = lossr + lossf + gp optim_D.zero_grad() loss_D.backward() # for p in D.parameters(): # print(p.grad.norm()) optim_D.step() 注意：代码中有一句`xf= G(z).detach()。这句话的作用是断开G和D之间相连的反向传播链，这样就不会再我们更新D的时候同时更新前面D的参数。 详细解释： tensor.detach()返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，不同之处只是requirse_grad为false.得到的这个tensir永远不需要计算器梯度，不具有grad. 即使之后重新将它的requires_grad置为true,它也不会具有梯度grad.这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时到该调用detach()的tensor就会停止，不能再继续向前进行传播. 注意：是继续使用这个新的tensor`进行计算！ 训练G 然后接下来就是训练生成器G了，生成器的训练是根据随机正态分布中的采样来学习我们要学习的分布的特征。 12345678910# 2. train Generatoroptim_D.zero_grad()z = torch.randn(batchsz, 2).cuda()xf = G(z)predf = (D(xf))# max predfloss_G = - (predf.mean())optim_G.zero_grad()loss_G.backward()optim_G.step() 注意：D训练完了以后一定要清零，防止G更新的时候反向传播更新D的网络参数。 结果 运行上面的代码，我们大概率得到的GAN的训练结果如下图所示，我们会发现D的鉴别很准确，误差是0，而G因为生成的很烂所以是-1，但是又因为JS的特性，导致模型无法更新（分布不重合，JS恒定，梯度为0）。因此为了解决这种问题我们需要引入上文所说的WGAN。 实战WGAN 略微不同于GAN，主要是训练鉴别器部分有一些差别。训练鉴别器在WGAN中分为四个部分，训练真实数据，训练假数据（G生成的），梯度惩罚（gradient penalty），反向传播，首先是我们先给鉴别器输入真实的数据（代码中是xr，使用刚才我们写的data_generator生成）,鉴别器的输出是predr。我们的目标是最大化这一部分的输出（相当于最小化predr的相反数）。因此lossr = - (predr.mean())。然后我们再给鉴别器输入G生成的数据（代码中是xf）,鉴别器的输出是predf。我们的目标是最小化这一部分的输出（相当于最小化predr的相反数）。因此lossf = (predr.mean())。 然后接着是梯度惩罚操作，这一步非常重要后面我们单独讲，算出需要我们最小化的gp。然后我们就可以写出我们最后需要最小化的函数loss_D = lossr + lossf + gp。然后进行最后一步反向传播。 网络结构 和GAN基本一样，不再赘述。 相比于GAN只是多了一个**梯度惩罚(gradient_penalty)**部分。 梯度惩罚(gradient_penalty) 稍微理解一下即可，我们最后是要最小化该函数返回的gp的，具体代码如下所示： 123456789101112131415161718192021222324252627282930def gradient_penalty(D, xr, xf): &quot;&quot;&quot; :param D: :param xr: :param xf: :return: &quot;&quot;&quot; LAMBDA = 0.3 # only constrait for Discriminator xf = xf.detach() xr = xr.detach() # [b, 1] =&gt; [b, 2] alpha = torch.rand(batchsz, 1).cuda() alpha = alpha.expand_as(xr) interpolates = alpha * xr + ((1 - alpha) * xf) interpolates.requires_grad_() disc_interpolates = D(interpolates) gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones_like(disc_interpolates), create_graph=True, retain_graph=True, only_inputs=True)[0] # 对中间点鉴别结果关于中间点信息求导 gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA return gp 外面调用如下代码所示： 12# gradient penaltygp = gradient_penalty(D, xr, xf) 注意：这里的xf在前面是经过了detach()操作的。 代码汇总 最后在加上一些细节，最终的WGAN代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249import torch from torch import nn, optim, autogradimport numpy as npimport visdomfrom torch.nn import functional as Ffrom matplotlib import pyplot as pltimport randomh_dim = 400batchsz = 512viz = visdom.Visdom()class Generator(nn.Module): def __init__(self): super(Generator, self).__init__() self.net = nn.Sequential( nn.Linear(2, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, 2), ) def forward(self, z): output = self.net(z) return outputclass Discriminator(nn.Module): def __init__(self): super(Discriminator, self).__init__() self.net = nn.Sequential( nn.Linear(2, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, h_dim), nn.ReLU(True), nn.Linear(h_dim, 1), nn.Sigmoid() ) def forward(self, x): output = self.net(x) return output.view(-1)def data_generator(): scale = 2. centers = [ (1, 0), (-1, 0), (0, 1), (0, -1), (1. / np.sqrt(2), 1. / np.sqrt(2)), (1. / np.sqrt(2), -1. / np.sqrt(2)), (-1. / np.sqrt(2), 1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2)) ] centers = [(scale * x, scale * y) for x, y in centers] while True: dataset = [] for i in range(batchsz): point = np.random.randn(2) * .02 center = random.choice(centers) point[0] += center[0] point[1] += center[1] dataset.append(point) dataset = np.array(dataset, dtype=&#x27;float32&#x27;) dataset /= 1.414 # stdev yield dataset # for i in range(100000//25): # for x in range(-2, 3): # for y in range(-2, 3): # point = np.random.randn(2).astype(np.float32) * 0.05 # point[0] += 2 * x # point[1] += 2 * y # dataset.append(point) # # dataset = np.array(dataset) # print(&#x27;dataset:&#x27;, dataset.shape) # viz.scatter(dataset, win=&#x27;dataset&#x27;, opts=dict(title=&#x27;dataset&#x27;, webgl=True)) # # while True: # np.random.shuffle(dataset) # # for i in range(len(dataset)//batchsz): # yield dataset[i*batchsz : (i+1)*batchsz]def generate_image(D, G, xr, epoch): &quot;&quot;&quot; Generates and saves a plot of the true distribution, the generator, and the critic. &quot;&quot;&quot; N_POINTS = 128 RANGE = 3 plt.clf() points = np.zeros((N_POINTS, N_POINTS, 2), dtype=&#x27;float32&#x27;) points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None] points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :] points = points.reshape((-1, 2)) # (16384, 2) # print(&#x27;p:&#x27;, points.shape) # draw contour with torch.no_grad(): points = torch.Tensor(points).cuda() # [16384, 2] disc_map = D(points).cpu().numpy() # [16384] x = y = np.linspace(-RANGE, RANGE, N_POINTS) cs = plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose()) plt.clabel(cs, inline=1, fontsize=10) # plt.colorbar() # draw samples with torch.no_grad(): z = torch.randn(batchsz, 2).cuda() # [b, 2] samples = G(z).cpu().numpy() # [b, 2] plt.scatter(xr[:, 0], xr[:, 1], c=&#x27;orange&#x27;, marker=&#x27;.&#x27;) plt.scatter(samples[:, 0], samples[:, 1], c=&#x27;green&#x27;, marker=&#x27;+&#x27;) viz.matplot(plt, win=&#x27;contour&#x27;, opts=dict(title=&#x27;p(x):%d&#x27;%epoch))def weights_init(m): if isinstance(m, nn.Linear): # m.weight.data.normal_(0.0, 0.02) nn.init.kaiming_normal_(m.weight) m.bias.data.fill_(0)def gradient_penalty(D, xr, xf): &quot;&quot;&quot; :param D: :param xr: :param xf: :return: &quot;&quot;&quot; LAMBDA = 0.3 # only constrait for Discriminator xf = xf.detach() xr = xr.detach() # [b, 1] =&gt; [b, 2] alpha = torch.rand(batchsz, 1).cuda() alpha = alpha.expand_as(xr) interpolates = alpha * xr + ((1 - alpha) * xf) interpolates.requires_grad_() disc_interpolates = D(interpolates) gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones_like(disc_interpolates), create_graph=True, retain_graph=True, only_inputs=True)[0] gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA return gpdef main(): torch.manual_seed(23) np.random.seed(23) G = Generator().cuda() D = Discriminator().cuda() G.apply(weights_init) D.apply(weights_init) optim_G = optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.9)) optim_D = optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.9)) data_iter = data_generator() print(&#x27;batch:&#x27;, next(data_iter).shape) viz.line([[0,0]], [0], win=&#x27;loss&#x27;, opts=dict(title=&#x27;loss&#x27;, legend=[&#x27;D&#x27;, &#x27;G&#x27;])) for epoch in range(50000): # 1. train discriminator for k steps for _ in range(5): x = next(data_iter) xr = torch.from_numpy(x).cuda() # [b] predr = (D(xr)) # max log(lossr) lossr = - (predr.mean()) # [b, 2] z = torch.randn(batchsz, 2).cuda() # stop gradient on G # [b, 2] xf = G(z).detach() # [b] predf = (D(xf)) # min predf lossf = (predf.mean()) # gradient penalty gp = gradient_penalty(D, xr, xf) loss_D = lossr + lossf + gp optim_D.zero_grad() loss_D.backward() # for p in D.parameters(): # print(p.grad.norm()) optim_D.step() # 2. train Generator optim_D.zero_grad() z = torch.randn(batchsz, 2).cuda() xf = G(z) predf = (D(xf)) # max predf loss_G = - (predf.mean()) optim_G.zero_grad() loss_G.backward() optim_G.step() if epoch % 100 == 0: viz.line([[loss_D.item(), loss_G.item()]], [epoch], win=&#x27;loss&#x27;, update=&#x27;append&#x27;) generate_image(D, G, xr, epoch) print(loss_D.item(), loss_G.item())if __name__ == &#x27;__main__&#x27;: main()"},{"title":"PyTorch过拟合","path":"/wiki/PyTorch/PyTorch过拟合.html","content":"过拟合与欠拟合 欠拟合 训练时的准确率低（train acc. is bad） 测试时的准确率也很低（test acc. is bad as well） 过拟合 相比于欠拟合的状态，训练时的损失函数和准确率要好得多（train loss and acc. is much better） 测试时的准确率要低一些(test acc. is worse) 泛化能力较差（generalization performance is worse） Train-Val-Test划分 使用如Train-Val-Test划分来检测是否存在过拟合或者欠拟合的情况。 Train：用来训练网络Val：用来挑选模型参数，用于监视训练（几轮训练后跑一轮Val），发现过拟合时可以提前停止训练模型 Test：验证模型最终的性能（给客户看），一般实际情况下这一部分的数据客户不会提供 PyTorch划分Train-Val数据集代码如下： 1234567891011print(&#x27;train:&#x27;,len(train_db),&#x27;test:&#x27;,len(test_db))train_db,val_db=torch.utils.data.random_split(train_db,[50000,10000])print(&#x27;db1:&#x27;,len(train_db),&#x27;db2:&#x27;,len(val_db))train_loader=torch.utils.data.DataLoader( train_db, batch_size=batch_size,shuffle=True)val_loader=torch.utils.data.DataLoader( val_db, batch_size=batch_size,shuffle=True) k折交叉验证（k-fold cross validation） K-fold交叉验证是一种数据拆分技术，被定义为一种用于在未见过的数据上估计模型性能的方法。你可以使用k&gt;1折来实现用于不同目的的样本划分，也是一种用于超参数优化的技术，以便可以训练具有最优超参数值的模型。这是一种无需增添或者修改样本的重采样技术。这种方法的优点是，每个样本案例仅用于训练和验证（作为测试折的一部分）一次。与传统方法相比，这种方法可以很好降低模型性能的方差。 K-fold交叉验证的过程分为下面几步： 把数据集分为训练数据集和测试数据集。 然后将训练数据集拆分为K份；在K-folds样本中，（K-1）份用于训练，1份用于验证，把每次模型的性能记录下来。 重复第2步，直到每个k-fold 都用到了验证（这就是为什么它被称为k-fold交叉验证）。 通过获取步骤2中为所有K个模型计算的模型分数来计算模型性能的均值和标准差。 对不同的超参数值重复步骤2到步骤5。 最后选择产生最优分数均值和标准值的模型超参数。 在测试数据集上计算评估模型性能。 汇总 仍然是针对MINST数据集，基于前面优化的基础上但是划分成为了三个数据集的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformsbatch_size=200learning_rate=0.01epochs=10train_db = datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]))train_loader = torch.utils.data.DataLoader( train_db, batch_size=batch_size, shuffle=True)test_db = datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))test_loader = torch.utils.data.DataLoader(test_db, batch_size=batch_size, shuffle=True)print(&#x27;train:&#x27;, len(train_db), &#x27;test:&#x27;, len(test_db))train_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])print(&#x27;db1:&#x27;, len(train_db), &#x27;db2:&#x27;, len(val_db))train_loader = torch.utils.data.DataLoader( train_db, batch_size=batch_size, shuffle=True)val_loader = torch.utils.data.DataLoader( val_db, batch_size=batch_size, shuffle=True)class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.model = nn.Sequential( nn.Linear(784, 200), nn.LeakyReLU(inplace=True), nn.Linear(200, 200), nn.LeakyReLU(inplace=True), nn.Linear(200, 10), nn.LeakyReLU(inplace=True), ) def forward(self, x): x = self.model(x) return xdevice = torch.device(&#x27;cuda:0&#x27;)net = MLP().to(device)optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss().to(device)# Train部分for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) data, target = data.to(device), target.cuda() logits = net(data) loss = criteon(logits, target) optimizer.zero_grad() loss.backward() # print(w1.grad.norm(), w2.grad.norm()) optimizer.step() if batch_idx % 100 == 0: print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\t# Val验证部分 test_loss = 0 correct = 0 for data, target in val_loader: data = data.view(-1, 28 * 28) data, target = data.to(device), target.cuda() logits = net(data) test_loss += criteon(logits, target).item() pred = logits.data.max(1)[1] correct += pred.eq(target.data).sum() test_loss /= len(val_loader.dataset) print(&#x27; VAL set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) &#x27;.format( test_loss, correct, len(val_loader.dataset), 100. * correct / len(val_loader.dataset)))# test测试部分test_loss = 0correct = 0for data, target in test_loader: data = data.view(-1, 28 * 28) data, target = data.to(device), target.cuda() logits = net(data) test_loss += criteon(logits, target).item() pred = logits.data.max(1)[1] correct += pred.eq(target.data).sum()test_loss /= len(test_loader.dataset)print(&#x27; Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%) &#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) 减少过拟合的方法 更多的数据（More Data） 减少模型的复杂程度（Constraint model complexity） 更浅的网络 正则化 Dropout 数据增强（Data aargumentation） Early stopping（前面进行讲解过） 正则化（Regularization） 你可能熟悉奥卡姆剃刀原则：给出两个解释，最可能正确的解释是更简单的一个 – 假设较少的解释。 这个原则也适用于神经网络的模型： 简单的模型比复杂的泛化能力好。 正则化，即在成本函数中加入一个正则化项(惩罚项)，惩罚模型的复杂度，防止网络过拟合。 以Logistic Regression的交叉熵损失函数为例。 J(θ)=−1m∑i=1m[yilny^i+(1−yi)ln(1−y^i)]J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y_i ln\\hat{y}_i+(1-y_i)ln(1-\\hat{y}_i)] J(θ)=−m1​i=1∑m​[yi​lny^​i​+(1−yi​)ln(1−y^​i​)] 我们加入对参数的惩罚项 J(θ)=−1m∑i=1m[yilny^i+(1−yi)ln(1−y^i)]+λ∑i=1n∣θi∣J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y_i ln\\hat{y}_i+(1-y_i)ln(1-\\hat{y}_i)]+\\lambda\\sum_{i=1}^n|\\theta_i| J(θ)=−m1​i=1∑m​[yi​lny^​i​+(1−yi​)ln(1−y^​i​)]+λi=1∑n​∣θi​∣ 当然也可以加L2范数的惩罚项(在PyTorch中最常用) J(θ)=−1m∑i=1m[yilny^i+(1−yi)ln(1−y^i)]+12λ∑i=1n∣∣θi∣∣2J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y_i ln\\hat{y}_i+(1-y_i)ln(1-\\hat{y}_i)]+\\frac{1}{2}\\lambda\\sum_{i=1}^n||\\theta_i||^2 J(θ)=−m1​i=1∑m​[yi​lny^​i​+(1−yi​)ln(1−y^​i​)]+21​λi=1∑n​∣∣θi​∣∣2 这样通过调整合适的λ\\lambdaλ参数就可以有效的抑制模型高阶参数。 正则化在PyTorch中又叫做`WeightDecay` 在PyTorch中做L2-regularization 1234device=torch.device(&#x27;cuda:0&#x27;)net=MLP().to(device)optimizer=optim.SGD(net.parameters(),lr=learning_rate,weight_decay=0.01)#这里加一个weight_decay参数就好了criteon=nn.CrossEntropyLoss().to(device) 在PyTorch中做L1-regularization 因为PyTorch不提供相应的API，所以L1-regularization需要自己实现. 12345678910regularization_loss=0for param in model.parameters(): regularization_loss+=torch.sum(torch.abs(param))classify_loss=criteon(logits,target)loss=classify_loss+0.01*regularization_lossoptimizer.zero_grad()loss.backward()optimizer.step() 一般是出现了overfitting的情况后才会设置weight_decay 动量与学习率衰减 动量（momentum） 学习率衰减（learningrate decay） 动量（momentum） 原来的更新函数 wk+1=wk−α∇f(wk)w^{k+1}=w^k-\\alpha abla f(w^k) wk+1=wk−α∇f(wk) 加了动量以后的更新函数 wk+1=wk−αzk+1w^{k+1}=w^k-\\alpha z^{k+1} wk+1=wk−αzk+1 zk+1=βzk+∇f(wk)z^{k+1}=\\beta z^k+ abla f(w^k) zk+1=βzk+∇f(wk) zkz^kzk在这里代表上一次梯度的方向，所以每次的更新不仅取决于这一次梯度的方向还要取决于上一次梯度的方向。可以发现动量其实就是指数加权平均。 可以发现相比未添加动量，有动量的训练更新的时候方向变化没有那么尖锐和剧烈了，未添加动量的训练最终无法收敛到最优解，但是添加了动量的训练，最终可以凭借惯性得到全局最优解。 PyTorch对momentum的支持 123456789optimizer=torch.optim.SGD(model.parameters(),args.lr, momentum=args.momentum,# 这里添加一个动量参数就可以了 weight_decay=args.weight_decay)scheduler=ReduceLROnPlateau(optimizer,&#x27;min&#x27;)for epoch in xrange(args.start_epoch,args.epochs): train(train_loader,model,criterion,optimizer,epoch) reult_avg,loss_val=validate(val_loader,model,criterion,epoch) scheduler.step(loss_val) 学习率衰减（learningrate decay） 学习率衰减一般有两种衰减策略 一种是当损失函数碰到平原的时候进行学习率衰减，使用PyTorch中的函数是ReduceLROnPlateau 123456789optimizer=torch.optim.SGD(model.parameters(),args.lr, momentum=args.momentum,# 这里添加一个动量参数就可以了 weight_decay=args.weight_decay)scheduler=ReduceLROnPlateau(optimizer,&#x27;min&#x27;)for epoch in xrange(args.start_epoch,args.epochs): train(train_loader,model,criterion,optimizer,epoch) reult_avg,loss_val=validate(val_loader,model,criterion,epoch) scheduler.step(loss_val) 还有一种比较简单粗暴，就是每过多少步然后把learning_rate进行衰减。PyTorch中使用的函数是StepLR 123456789# Assuming optimizer uses lr = 0.05 for all groups# lr = 0.05 if epoch &lt; 30# lr = 0.005 if 30 &lt;= epoch &lt; 60# lr = 0.0005 if 60 &lt;= epoch &lt; 90schedular =StepLR(optimizer,steo_size=30,gamma=0.1)for epoch in range(100): scheduler.step() train(...) validate(...) StepLR中的参数表示的就是每步进30步，lr减少为原来的0.1，详情可以见上面的代码注释。 Early Stop&amp;Dropout Early Stop 因为有时训练轮数过多会出现过拟合的情况从而让模型性能变坏，所以在必要的时候我们要先停止训练模型（val取最大值时），保存参数，避免继续训练出现过拟合的情况。这就是我们所说的Early Stop 步骤 Val数据集用来选择参数（超参数） 观察验证集的表现 在Val验证集Acc最高（或Loss最低）处停止训练（根据经验，连续下滑一段时间后我们就认为前面的最高点就是最好的） Dropout 思想 学的更少来学的更好 每个连接都有p的概率被断开 代码实现 PyTorch中添加Dropout还是比较方便的 123456789net_dropped=torch.nn.Sequential( torch.nn.Linear(784,200), torch.nn.Dropout(0.5), torch.nn.ReLU(), torch.nn.Linear(200,200), torch.nn.Dropout(0.5), torch.nn.ReLU(), torch.nn.Linear(200,10),) 层之间是直连的，这里的意思是在层之间有50%的概率出现连接断掉。和上面画的示意图有一点不一样。 注意：dropout只在训练的时候才有，测试的时候是不会dropout的。 数据增强 见PyTorch CNN,因为讲解了卷积神经网络在图像识别方面的应用后，可能会对这一方面印象更加深刻一些。 SGD SGD全称Stochastic Gradient Descent，中文全称叫随机梯度下降。为了解决数据集过大无法将所有样本放入进行梯度下降，我们将数据集化成若干个Batch，一个Batch中包含若干个样本，每次使用一个Batch进行一次梯度下降（注意是一个Batch而不是每次取一个样本就下降一次） 其实以上这个应该叫做小批量梯度下降，而并非SGD，SGD是Batchsize=1的小批量梯度下降。 这里推荐一篇讲解不同梯度下降的博客：机器学习：面对海量数据如何进行机器学习"},{"title":"bitset 基础","path":"/wiki/C++/bitset/bitset 基础.html","content":"bitset 是 C++ 标准库中的一个模板类，用于表示和操作一组位（bit）。它提供了一种高效的方式来管理和操作固定大小的位序列。 基本用法 bitset 是一个模板类，它需要一个整数参数来指定位的数量。例如： 12345678#include &lt;bitset&gt;#include &lt;iostream&gt;int main() &#123; std::bitset&lt;8&gt; bs; // 定义一个包含8个位的bitset，初始化为00000000 std::cout &lt;&lt; bs &lt;&lt; std::endl; return 0;&#125; 在这个例子中，std::bitset&lt;8&gt; 定义了一个包含8个位的位集，默认情况下所有位都初始化为0。 特点和注意事项 固定大小：bitset 的大小在编译时确定，不能动态改变。这意味着一旦创建，位的数量是固定的。 位操作：bitset 提供了丰富的位操作函数，如按位与（&amp;）、按位或（|）、按位异或（^）、取反（~）等。 高效：由于bitset是基于位操作的，因此它在内存和速度方面都非常高效，尤其适用于需要大量布尔操作的场景。 位访问：可以通过下标操作符[]来访问和修改特定位。 初始化：可以通过整数或字符串来初始化bitset，但要注意字符串的长度和格式。 示例代码 以下是一些关于bitset的示例代码，以展示其常见用法和操作： 1234567891011121314151617181920212223242526272829#include &lt;bitset&gt;#include &lt;iostream&gt;int main() &#123; std::bitset&lt;8&gt; bs1; // 默认初始化为00000000 std::bitset&lt;8&gt; bs2(42); // 用整数初始化，42的二进制表示是00101010 std::bitset&lt;8&gt; bs3(&quot;1100&quot;); // 用字符串初始化，前面四位会被补零，得到00001100 // 输出bitset std::cout &lt;&lt; &quot;bs1: &quot; &lt;&lt; bs1 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;bs2: &quot; &lt;&lt; bs2 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;bs3: &quot; &lt;&lt; bs3 &lt;&lt; std::endl; // 位操作 bs1.set(1); // 将第1位置为1，结果是00000010 bs2.reset(1); // 将第1位清零，结果是00101000 bs3.flip(0); // 翻转第0位，结果是00001101 std::cout &lt;&lt; &quot;After operations:&quot; &lt;&lt; std::endl; std::cout &lt;&lt; &quot;bs1: &quot; &lt;&lt; bs1 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;bs2: &quot; &lt;&lt; bs2 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;bs3: &quot; &lt;&lt; bs3 &lt;&lt; std::endl; // 访问位 std::cout &lt;&lt; &quot;bs2[3]: &quot; &lt;&lt; bs2[3] &lt;&lt; std::endl; // 输出1 std::cout &lt;&lt; &quot;bs3.count() : &quot; &lt;&lt; bs3.count() &lt;&lt; std::endl; // 计算1的数量，输出3 return 0;&#125; 额外资源 C++ Reference: bitset cppreference.com: std::bitset"},{"title":"test","path":"/wiki/C++/bitset/test.html","content":"bitset 中的 test 函数用于检查 bitset 中某个位是否被设置为1。它有两种用法： 检查指定位置的位是否为1。 检查bitset是否有任何位被设置为1。 基本用法 1. 检查指定位置的位 test 函数的参数是一个位的位置索引，返回值是一个布尔值，如果该位置的位为1，则返回true，否则返回false。 123456789101112#include &lt;bitset&gt;#include &lt;iostream&gt;int main() &#123; std::bitset&lt;8&gt; bs(&quot;11001010&quot;); // 用字符串初始化，得到 11001010 // 检查第0位和第1位 std::cout &lt;&lt; &quot;bs[0] is &quot; &lt;&lt; bs.test(0) &lt;&lt; std::endl; // 输出 0，因为第0位是0 std::cout &lt;&lt; &quot;bs[1] is &quot; &lt;&lt; bs.test(1) &lt;&lt; std::endl; // 输出 1，因为第1位是1 return 0;&#125; 2. 检查bitset是否有任何位被设置为1 无参数的 test 函数检查 bitset 中是否有任何位被设置为1。 123456789101112#include &lt;bitset&gt;#include &lt;iostream&gt;int main() &#123; std::bitset&lt;8&gt; bs1(&quot;00000000&quot;); // 全部位都是0 std::bitset&lt;8&gt; bs2(&quot;11001010&quot;); // 有一些位是1 std::cout &lt;&lt; &quot;bs1 has any bit set to 1: &quot; &lt;&lt; bs1.any() &lt;&lt; std::endl; // 输出 0（false） std::cout &lt;&lt; &quot;bs2 has any bit set to 1: &quot; &lt;&lt; bs2.any() &lt;&lt; std::endl; // 输出 1（true） return 0;&#125; 特点和注意事项 越界检查：test 函数会检查给定的位置索引是否有效，如果索引超出范围，会抛出 std::out_of_range 异常。 布尔返回值：test 返回布尔值，可以直接用于条件语句中。 示例代码 以下是一个完整的示例代码，演示如何使用 test 函数来检查 bitset 中某个位的状态以及是否有任何位被设置为1： 123456789101112131415161718#include &lt;bitset&gt;#include &lt;iostream&gt;int main() &#123; std::bitset&lt;8&gt; bs(&quot;11001010&quot;); // 用字符串初始化，得到 11001010 // 检查第3位和第5位 std::cout &lt;&lt; &quot;bs[3] is &quot; &lt;&lt; (bs.test(3) ? &quot;set&quot; : &quot;not set&quot;) &lt;&lt; std::endl; // 输出 &quot;set&quot; std::cout &lt;&lt; &quot;bs[5] is &quot; &lt;&lt; (bs.test(5) ? &quot;set&quot; : &quot;not set&quot;) &lt;&lt; std::endl; // 输出 &quot;not set&quot; // 检查整个bitset是否有任何位被设置为1 std::cout &lt;&lt; &quot;bs has any bit set to 1: &quot; &lt;&lt; (bs.any() ? &quot;true&quot; : &quot;false&quot;) &lt;&lt; std::endl; // 输出 &quot;true&quot; // 检查整个bitset是否全部位都是0 std::cout &lt;&lt; &quot;bs has all bits set to 0: &quot; &lt;&lt; (bs.none() ? &quot;true&quot; : &quot;false&quot;) &lt;&lt; std::endl; // 输出 &quot;false&quot; return 0;&#125;"},{"title":"atoi","path":"/wiki/C++/cstdlib/atoi.html","content":"atoi 是 C 标准库中的一个函数，用于将字符串转换为整数。尽管 atoi 函数主要属于 C 语言，但在 C++ 中也广泛使用。下面将详细解释 atoi 的基本用法、特点和注意事项，以及示例代码。 基本用法 atoi 函数用于将以数字形式表示的字符串转换为 int 类型的整数。 函数原型： 1int atoi(const char *str); 参数： str：指向以 null 结尾的字符串，该字符串表示一个整数。 返回值： 成功时：返回转换后的整数值。 失败时：返回 0，如果字符串不包含有效的数字。 特点和注意事项 输入字符串要求： atoi 只能处理纯数字字符串，可以包含可选的正负号。 输入字符串中不能包含非数字字符，否则结果可能不准确。 如果字符串为空或不包含有效的整数，atoi 会返回 0。 异常处理： atoi 不会检测转换过程中的溢出情况。例如，如果字符串表示的数字超出了 int 类型的范围，atoi 不会抛出错误或警告，只会返回未定义的行为。 安全替代： 由于 atoi 缺乏错误处理机制，建议使用更安全的替代函数，如 strtol 或 C++11 引入的 std::stoi，它们提供了更好的错误检测和处理能力。 示例代码 下面是一些示例代码，展示了如何使用 atoi 函数将字符串转换为整数： 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;cstdlib&gt; // 包含 atoi 函数的头文件int main() &#123; const char *numStr1 = &quot;12345&quot;; const char *numStr2 = &quot;-6789&quot;; const char *numStr3 = &quot;42 is the answer&quot;; const char *numStr4 = &quot;abc123&quot;; // 使用 atoi 将字符串转换为整数 int num1 = atoi(numStr1); int num2 = atoi(numStr2); int num3 = atoi(numStr3); int num4 = atoi(numStr4); // 输出转换结果 std::cout &lt;&lt; &quot;String: &quot; &lt;&lt; numStr1 &lt;&lt; &quot; -&gt; Integer: &quot; &lt;&lt; num1 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;String: &quot; &lt;&lt; numStr2 &lt;&lt; &quot; -&gt; Integer: &quot; &lt;&lt; num2 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;String: &quot; &lt;&lt; numStr3 &lt;&lt; &quot; -&gt; Integer: &quot; &lt;&lt; num3 &lt;&lt; std::endl; // 只转换前面的数字部分 std::cout &lt;&lt; &quot;String: &quot; &lt;&lt; numStr4 &lt;&lt; &quot; -&gt; Integer: &quot; &lt;&lt; num4 &lt;&lt; std::endl; // 非数字开头，返回 0 return 0;&#125; 输出： 1234String: 12345 -&gt; Integer: 12345String: -6789 -&gt; Integer: -6789String: 42 is the answer -&gt; Integer: 42String: abc123 -&gt; Integer: 0 总结 atoi 是一个简单但不够健壮的字符串到整数转换函数，适用于处理纯数字字符串。由于缺乏错误处理机制和安全性问题，建议在实际开发中使用更安全的替代函数，如 strtol 或 std::stoi。"},{"title":"multiset 基础","path":"/wiki/C++/set/multiset 基础.html","content":"基本用法 multiset 是 C++ 标准模板库（STL）中的一个关联容器，它允许存储多个相同值的元素，并且自动按顺序进行排序。multiset 的主要特点是可以高效地进行插入、删除和查找操作。 要使用 multiset，首先需要包含头文件： 1#include &lt;set&gt; 然后，可以定义一个 multiset： 1std::multiset&lt;int&gt; myMultiset; 特点和注意事项 自动排序：multiset 中的元素总是按升序排列（默认情况下），也可以通过自定义比较函数进行排序。 允许重复：与 set 不同，multiset 允许存储多个相同的值。 底层实现：multiset 通常使用红黑树（Red-Black Tree）来实现，这使得插入、删除和查找操作的时间复杂度为 O(log n)。 查找操作： count(key)：返回指定键的数量。 find(key)：返回一个指向键的迭代器。 equal_range(key)：返回一个范围，表示键的所有等价元素。 插入和删除操作： insert(value)：插入值。 erase(key)：删除所有等于键的元素。 erase(iterator)：删除指定迭代器处的元素。 示例代码 以下是一个使用 multiset 的简单示例，展示了基本的插入、查找和删除操作： 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;set&gt;int main() &#123; // 定义一个 multiset std::multiset&lt;int&gt; myMultiset; // 插入元素 myMultiset.insert(10); myMultiset.insert(20); myMultiset.insert(10); myMultiset.insert(30); // 输出 multiset 中的元素 std::cout &lt;&lt; &quot;Multiset elements: &quot;; for (const int&amp; elem : myMultiset) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; // 查找元素 std::cout &lt;&lt; &quot;Count of 10: &quot; &lt;&lt; myMultiset.count(10) &lt;&lt; std::endl; // 删除元素 myMultiset.erase(10); // 输出删除后的 multiset 中的元素 std::cout &lt;&lt; &quot;Multiset elements after erasing 10: &quot;; for (const int&amp; elem : myMultiset) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 运行结果： 123Multiset elements: 10 10 20 30 Count of 10: 2Multiset elements after erasing 10: 20 30 总结 multiset 是一个非常有用的容器，特别是在需要存储重复元素并且对元素进行排序的场景中。它提供了高效的插入、删除和查找操作，但需要注意的是，它允许重复的元素存在，并且这些操作的时间复杂度通常为 O(log n)。"},{"title":"PyTorch进阶","path":"/wiki/PyTorch/PyTorch进阶.html","content":"Broadcast自动扩展 维度扩展 扩展无需复制数据 对于一个shape为`torch.size([x1,x2,x3,...,xn])`的`tensor`,靠近x1那一端为大维度，靠近xn那一端为小维度。那么对于broadcast有： 小维度指定，大维度随意这一规律（因为在大维度上进行广播） 拼接与拆分 cat stack split chunk cat 主要的功能是将tensor拼接起来 1234In [3]: a=torch.rand(4,32,8)In [4]: b=torch.rand(5,32,8)In [5]: torch.cat([a,b],dim=0).shapeOut[5]: torch.Size([9, 32, 8]) `dim`参数表示的是在哪个维度上进行合并。如果是`n`维tensor则取值范围是[0,n) stack stack的功能和cat有些像，但是他要求的是所拼接的两个tensor的shape必须完全一样。并且拼接后会在拼接维度之前增加一个维度。 注意：shape必须完全一样！ 然后在所拼接维度之前会新增加一个维度，用于区分是拼接的哪一块（前一块或后一块） 1234In [3]: a1=torch.rand(5,3,16,32)In [4]: a2=torch.rand(5,3,16,32)In [5]: torch.stack([a1,a2],dim=1).shapeOut[5]: torch.Size([5, 2, 3, 16, 32]) split split是按照长度对tensor进行拆分的函数，一般传入两个参数，第一个参数代表的是所拆维度的长度（长度不仅可以用标量表示固定的长度，也可以用list来表示不固定的长度，详情见下面的例子），第二个参数代表的是所拆的维度是第几维度。 12345678In [3]: c=torch.rand(3,32,8)In [4]: aa,bb,cc=c.split(1,dim=0)In [5]: aa.shape,bb.shape,cc.shapeOut[5]: (torch.Size([1, 32, 8]), torch.Size([1, 32, 8]), torch.Size([1, 32, 8]))In [6]: aa,bb=c.split([2,1],dim=0)In [7]: aa.shape,bb.shapeOut[7]: (torch.Size([2, 32, 8]), torch.Size([1, 32, 8])) chunck chunck是按照数量对tensor进行拆分的函数，一般传入两个参数，第一个参数代表的是所拆维度要拆出来的数量，第二个参数代表的是所拆的维度是第几维度。 12345In [3]: d=torch.rand(6,32,8)In [4]: c=torch.rand(3,32,8)In [5]: aa,bb,cc=d.chunk(3,dim=0)In [6]: aa.shape,bb.shape,cc.shapeOut[6]: (torch.Size([2, 32, 8]), torch.Size([2, 32, 8]), torch.Size([2, 32, 8])) 数学运算 基本四则运算(add/minus/multiply/divide) 矩阵乘法(Matmul) 乘方运算(Pow) 开方运算(Sqrt/rsqrt) 近似运算(Round) 基本四则运算 1234567891011121314151617181920212223In [3]: a=torch.tensor([[2,4,6],[8,10,12]])In [4]: b=torch.ones(2,3)*2In [5]: a+bOut[5]: tensor([[ 4., 6., 8.], [10., 12., 14.]])In [6]: a-bOut[6]: tensor([[ 0., 2., 4.], [ 6., 8., 10.]])In [7]: a*bOut[7]: tensor([[ 4., 8., 12.], [16., 20., 24.]])In [8]: a/bOut[8]: tensor([[1., 2., 3.], [4., 5., 6.]])In [9]: a/(a/b)Out[9]: tensor([[2., 2., 2.], [2., 2., 2.]]) pytorch中还专门有定义好的四则运算的函数和以上这些运算符号所运算出来的效果相同，他们分别是。 torch.add torch.minus torch.mul torch.div 123456789In [10]: torch.all(torch.eq(a+b,torch.add(a,b)))Out[10]: tensor(True)In [11]: torch.all(torch.eq(a-b,torch.sub(a,b)))Out[11]: tensor(True)In [12]: torch.all(torch.eq(a*b,torch.mul(a,b)))Out[12]: tensor(True)In [13]: torch.all(torch.eq(a/b,torch.div(a,b)))Out[13]: tensor(True) 注意：这里的乘法和除法，不同于矩阵乘法和除法，只是将两个矩阵对应位置的数进行相乘或相除操作。 这里还有一个整除运算符号//没有讲到，好像并没有找到与他对应的函数 矩阵乘法 torch.mm(只适用于二维tensor,不推荐) torch.matmul(适用于任意维度的矩阵) @(同上torch.matmul) 123456789101112131415In [3]: a=torch.tensor([[2,4],[8,12]])In [4]: b=torch.tensor([[1,2],[3,4]])In [5]: torch.mm(a,b)Out[5]: tensor([[14, 20], [44, 64]])In [6]: torch.matmul(a,b)Out[6]: tensor([[14, 20], [44, 64]])In [7]: a@bOut[7]: tensor([[14, 20], [44, 64]]) BP全连接神经网络的一个例子 x@w.t()+bx@w.t()+b x@w.t()+b 1234In [4]: x=torch.rand(4,784)In [5]: w=torch.rand(512,784)In [6]: (x@w.t()).shapeOut[6]: torch.Size([4, 512]) 高于2维矩阵乘法的原理 取最小两个维度的内容进行矩阵乘法，保留高维度的的大小。其实相当于是支持了多个矩阵并行相乘。 高维度大小不一定要完全一样，但是要符合broadcasting原则，具体见下面的例子 12345678910In [3]: a=torch.rand(4,3,28,64)In [4]: b=torch.rand(4,3,64,32)In [5]: torch.matmul(a,b).shapeOut[5]: torch.Size([4, 3, 28, 32])In [6]: b=torch.rand(4,1,64,32)In [7]: torch.matmul(a,b).shapeOut[7]: torch.Size([4, 3, 28, 32])In [8]: b=torch.rand(4,2,64,32)In [9]: torch.matmul(a,b).shapeTraceback 乘方运算 .pow()：括号中写次方数 **：同上面的pow函数，乘方运算符 sqrt()：开平方运算 rsqrt()：开平方后求倒数。 1234567891011121314151617181920212223242526272829In [3]: a=torch.full([2,2],3)In [4]: aOut[4]: tensor([[3, 3], [3, 3]])In [5]: a.pow(2)Out[5]: tensor([[9, 9], [9, 9]])In [6]: a**2Out[6]: tensor([[9, 9], [9, 9]])In [7]: torch.all(torch.eq(a**2,a.pow(2)))Out[7]: tensor(True)In [8]: aa=a**2In [9]: aa.sqrt()Out[9]: tensor([[3., 3.], [3., 3.]])In [10]: aa.rsqrt()Out[10]: tensor([[0.3333, 0.3333], [0.3333, 0.3333]])In [11]: aa**(0.5)Out[11]: tensor([[3., 3.], [3., 3.]]) exp和log和log2和log10 看名字应该就大概明白功能了，这里不再赘述，直接上一个例子吧 12345678910In [14]: a=torch.exp(torch.ones(2,2))In [15]: aOut[15]: tensor([[2.7183, 2.7183], [2.7183, 2.7183]])In [16]: torch.log(a)Out[16]: tensor([[1., 1.], [1., 1.]]) 1234567891011In [18]: a=torch.ones(2,2)*2In [19]: torch.log2(a)Out[19]: tensor([[1., 1.], [1., 1.]])In [20]: a=torch.ones(2,2)*100In [21]: torch.log10(a)Out[21]: tensor([[2., 2.], [2., 2.]]) 近似运算 .floor()：向下取整 .ceil()：向上取整 .round()：四舍五入 .trunc()：裁剪——只保留整数部分 /frac()：裁剪——只保留小数部分 12345678910In [3]: a=torch.tensor(3.14)In [4]: a.floor(),a.ceil(),a.trunc(),a.frac()Out[4]: (tensor(3.), tensor(4.), tensor(3.), tensor(0.1400))In [5]: a=torch.tensor(3.499999)In [6]: a.round()Out[6]: tensor(3.)In [7]: a=torch.tensor(3.5)In [8]: a.round()Out[8]: tensor(4.) 梯度裁剪※ 我们在做和机器学习相关的项目中时，有可能会碰到这种情况，就是梯度太大或太小（接近0）从而导致我们的训练结果并不是非常的理想，因此我们可以采用梯度裁剪的方法，将梯度现有梯度控制在一个范围内，有效避免这类问题。 所使用的函数是clamp，一般传入一个或两个参数。 如果之传入一个参数，该参数表示梯度的最小值，因此比该值小的梯度都会被强行增大到该值。 如果传入两个参数，则两个参数分别表示最小值和最大值，比最小值小的值会强行变为设定的最小值，比最大值大的值会强行变为设定的最大值。 123456789101112131415161718192021222324In [3]: grad=torch.rand(2,3)*30-15In [4]: grad.max()Out[4]: tensor(6.2306)In [5]: grad.min()Out[5]: tensor(-11.8838)In [6]: grad.median()Out[6]: tensor(-0.5767)In [7]: grad.clamp(5)Out[7]: tensor([[5.0000, 5.0000, 5.0000], [5.0000, 5.0000, 6.2306]])In [8]: gradOut[8]: tensor([[ 3.8570, -0.2229, -11.8838], [ -8.5261, -0.5767, 6.2306]])In [9]: grad.clamp(0)Out[9]: tensor([[3.8570, 0.0000, 0.0000], [0.0000, 0.0000, 6.2306]])In [10]: grad.clamp(0,5)Out[10]: tensor([[3.8570, 0.0000, 0.0000], [0.0000, 0.0000, 5.0000]]) 统计属性 norm：范数 mean,sum：平均值，和 prod：累乘 max,min,argmin,argmax：最大值，最小值，最小值所在位置，最大值所在位置 kthvalue,topk：求第k个值，求前k个值 norm 一般是1范数和2范数使用的较多，但下面还是会将各个范数的含义进行简要的介绍。 0范数：矩阵中非零元素的个数 1范数：矩阵中各个元素的绝对值之和 2范数：矩阵中各个元素平方和的1/2次方，又被称为Euclidean范数或者Frobenius 范数。 p范数：为x向量（或矩阵）各个元素绝对值p次方和的1/p次方。 norm函数一般需要一个或者两个参数，如果只是提供一个参数，则该参数表示的是第几范数，如果提供2个参数，第一个参数意义同上，第二个参数的意义是在哪个维度上进行范数运算。 123456789101112131415161718192021222324252627282930313233343536In [3]: a=torch.tensor([1.,2.,3.,4.,5.,6.,7.,8.])In [4]: b=a.view(2,4)In [5]: c=a.view(2,2,2)In [6]: bOut[6]: tensor([[1., 2., 3., 4.], [5., 6., 7., 8.]])In [7]: cOut[7]: tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])In [8]: a.norm(1),b.norm(1),c.norm(1)Out[8]: (tensor(36.), tensor(36.), tensor(36.))In [9]: a.norm(2),b.norm(2),c.norm(2)Out[9]: (tensor(14.2829), tensor(14.2829), tensor(14.2829))In [10]: b.norm(1,dim=1)Out[10]: tensor([10., 26.])In [11]: b.norm(1,dim=0)Out[11]: tensor([ 6., 8., 10., 12.])In [12]: b.norm(2,dim=1)Out[12]: tensor([ 5.4772, 13.1909])In [13]: c.norm(1,dim=0)Out[13]: tensor([[ 6., 8.], [10., 12.]])In [14]: c.norm(1,dim=1)Out[14]: tensor([[ 4., 6.], [12., 14.]])In [15]: c.norm(1,dim=2)Out[15]: tensor([[ 3., 7.], [11., 15.]]) mean,max,min,sum,prod 123456789101112In [3]: a=torch.arange(8).view(2,4).float()In [4]: aOut[4]: tensor([[0., 1., 2., 3.], [4., 5., 6., 7.]])In [5]: a.min(),a.max(),a.mean(),a.prod()Out[5]: (tensor(0.), tensor(7.), tensor(3.5000), tensor(0.))In [6]: a.sum()Out[6]: tensor(28.)In [7]: a.argmax(),a.argmin()Out[7]: (tensor(7), tensor(0)) 可以发现argmax和argmin在寻找最大最小值的时候，是将整个tensor变为了一个一维向量的！如果我们想求每一行的一个最大值或最小值的位置，我们可以采用以下的这种做法。 123In [3]: a=torch.randn(4,10)In [4]: a.argmax(dim=1)Out[4]: tensor([3, 0, 6, 9]) dim&amp;keepdim 这两个都是函数中的参数，dim的作用在上面我们已经演示过了，这里就不再赘述。keepdim设置的是返回的答案是否要和原来的矩阵保持相同的维度信息。 12345678910111213141516171819202122232425262728In [3]: a=torch.randn(4,10)In [4]: a.argmax(dim=1)Out[4]: tensor([3, 0, 6, 9])In [5]: a.max(dim=1)Out[5]: torch.return_types.max(values=tensor([2.2597, 1.0554, 0.9288, 1.1149]),indices=tensor([3, 0, 6, 9]))In [6]: a.argmax(dim=1)Out[6]: tensor([3, 0, 6, 9])In [7]: a.max(dim=1,keepdim=True)Out[7]: torch.return_types.max(values=tensor([[2.2597], [1.0554], [0.9288], [1.1149]]),indices=tensor([[3], [0], [6], [9]]))In [8]: a.argmax(dim=1,keepdim=True)Out[8]: tensor([[3], [0], [6], [9]]) topk,kthvalue 123456789101112131415161718192021222324252627282930313233343536373839In [3]: a=torch.randn(4,10)In [4]: a.topk(3,dim=1)Out[4]: torch.return_types.topk(values=tensor([[2.0085, 0.5275, 0.4304], [1.5748, 0.8510, 0.6699], [1.1440, 0.8921, 0.7946], [0.9630, 0.8487, 0.8158]]),indices=tensor([[4, 0, 8], [9, 3, 5], [5, 9, 0], [6, 9, 4]]))In [5]: a.topk(3,dim=1,largest=False)Out[5]: torch.return_types.topk(values=tensor([[-1.8353, -1.5600, -0.7452], [-1.1623, -1.0671, -0.2221], [-1.1269, -0.3777, -0.2676], [-2.1377, -0.2639, -0.0832]]),indices=tensor([[2, 3, 9], [1, 8, 4], [7, 8, 6], [8, 1, 5]]))In [6]: a.kthvalue(1,dim=1)Out[6]: torch.return_types.kthvalue(values=tensor([-1.8353, -1.1623, -1.1269, -2.1377]),indices=tensor([2, 1, 7, 8]))In [7]: a.kthvalue(1)Out[7]: torch.return_types.kthvalue(values=tensor([-1.8353, -1.1623, -1.1269, -2.1377]),indices=tensor([2, 1, 7, 8]))In [8]: a.kthvalue(9)Out[8]: torch.return_types.kthvalue(values=tensor([0.5275, 0.8510, 0.8921, 0.8487]),indices=tensor([0, 3, 9, 9])) 对于topk函数默认是从大到小，如果把largest改为False则是从小到大。kthvalue函数默认是从小到大 比较 &gt;,&gt;=,&lt;=,!=,== torch.eq(a,b) 以上两种都是比较矩阵对应位置的数，因此返回的值是一个同大小的矩阵。 torch.equal(a,b)：判断两个矩阵是否完全相等 12345678910111213141516171819202122232425262728293031323334In [3]: a=torch.randn(4,10)In [4]: a&gt;0Out[4]: tensor([[ True, True, True, True, True, True, False, False, True, True], [ True, True, True, True, True, True, False, True, False, False], [False, True, False, True, False, True, False, True, True, False], [False, True, False, True, False, False, False, False, True, True]])In [5]: torch.gt(a,0)Out[5]: tensor([[ True, True, True, True, True, True, False, False, True, True], [ True, True, True, True, True, True, False, True, False, False], [False, True, False, True, False, True, False, True, True, False], [False, True, False, True, False, False, False, False, True, True]])In [6]: a!=0Out[6]: tensor([[True, True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True, True], [True, True, True, True, True, True, True, True, True, True]])In [7]: a=torch.ones(2,3)In [8]: b=torch.randn(2,3)In [9]: torch.eq(a,b)Out[9]: tensor([[False, False, False], [False, False, False]])In [10]: torch.eq(a,a)Out[10]: tensor([[True, True, True], [True, True, True]])In [11]: torch.equal(a,a)Out[11]: TrueIn [12]: torch.equal(a,b)Out[12]: False 高阶操作 where gather where torch.where(condition,x,y) where函数返回一个tensor，这个tensor中的元素不是从x中选出来的就是从y中选出来的，选择判断条件如下所示： outi={xiif conditioniyiotherwiseout_i=\\begin{cases} x_i \\quad if\\ condition_i\\\\ y_i\\quad otherwise \\end{cases} outi​={xi​if conditioni​yi​otherwise​ condition也是一个tensor，和x,y的大小一样。如果对应位置的conditon值为1，则取x对应位置的数据，否则取y对应位置的数据 1234567891011121314151617181920In [3]: cond=torch.rand(2,2)In [4]: a=torch.zeros(2,2)In [5]: b=torch.ones(2,2)In [6]: condOut[6]: tensor([[0.5486, 0.6658], [0.9244, 0.3691]])In [7]: aOut[7]: tensor([[0., 0.], [0., 0.]])In [8]: bOut[8]: tensor([[1., 1.], [1., 1.]])In [9]: torch.where(cond&gt;0.6,a,b)Out[9]: tensor([[1., 0.], [0., 1.]]) 虽然我们也可以用Python自带的for循环和if语句来实现同样的功能，但是这样的话由于这些语句运行在CPU上，我们不能享受到PyTorch库中带来的GPU加速的优势，因此会运行的更慢一些。 gather torch.gather(input,dim,index,out=None) gather函数实现的功能类似于查表函数，给定一个初始表，再给定一个索引表，自动生成一张按照索引表查完初始表的表。 12345678910111213141516171819In [3]: prob=torch.randn(4,10)In [4]: idx=prob.topk(dim=1,k=3)In [5]: idx=idx[1]In [6]: idxOut[6]: tensor([[9, 6, 7], [0, 3, 2], [1, 4, 2], [1, 5, 6]])In [7]: label=torch.arange(10)+100In [8]: labelOut[8]: tensor([100, 101, 102, 103, 104, 105, 106, 107, 108, 109])In [9]: torch.gather(label.expand(4,10),dim=1,index=idx.long())Out[9]: tensor([[109, 106, 107], [100, 103, 102], [101, 104, 102], [101, 105, 106]]) 总结 以上便是PyTorch中常用的操作了，后续就是实战环节了。"},{"title":"set 初始化","path":"/wiki/C++/set/set 初始化.html","content":"本文介绍一下set的不同初始化方法。 基本用法 在初始化 set 之前，首先需要包含 &lt;set&gt; 头文件，并使用 std::set 命名空间。 1#include &lt;set&gt; 特点和注意事项 初始化方法多样：可以使用默认构造函数、初始化列表、范围初始化、复制构造函数、移动构造函数等方法来初始化 set。 自动排序：插入元素时会自动排序。 唯一性：set 不允许有重复的元素。 高效性：查找、插入和删除操作的时间复杂度均为 O(log n)。 示例代码 1. 默认构造函数 创建一个空的 set，并根据需要插入元素。 1234567891011121314#include &lt;set&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s; // 创建一个空的set s.insert(10); s.insert(5); s.insert(20); for (const auto&amp; elem : s) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 2. 使用初始化列表 直接在声明时初始化 set。 12345678910#include &lt;set&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s = &#123;10, 5, 20, 5&#125;; // 使用初始化列表 for (const auto&amp; elem : s) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 3. 使用范围（区间）初始化 可以使用另一个容器的元素范围来初始化 set。 12345678910111213141516#include &lt;set&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int main() &#123; std::vector&lt;int&gt; vec = &#123;10, 5, 20, 15, 10&#125;; // 创建一个vector，其中包含一些重复元素 std::set&lt;int&gt; s(vec.begin(), vec.end()); // 使用vector的范围来初始化set std::cout &lt;&lt; &quot;Elements in set initialized from vector: &quot;; for (const auto&amp; elem : s) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; // 打印set中的元素 &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 4. 复制构造函数 可以使用已有的 set 来初始化新的 set。 123456789101112#include &lt;set&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s1 = &#123;10, 5, 20&#125;; std::set&lt;int&gt; s2(s1); // 使用另一个set来初始化 for (const auto&amp; elem : s2) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 5. 移动构造函数 可以使用已有的 set 来移动初始化新的 set。 123456789101112#include &lt;set&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s1 = &#123;10, 5, 20&#125;; std::set&lt;int&gt; s2(std::move(s1)); // 使用另一个set来移动初始化 for (const auto&amp; elem : s2) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 总结 C++ 提供了多种方法来初始化 set，从默认构造函数到使用初始化列表、范围初始化、复制构造函数和移动构造函数等。选择合适的初始化方法可以使代码更加简洁和高效。了解这些初始化方法有助于在实际编程中根据具体需求灵活地创建和初始化 set。 使用 vector 初始化 set 是一种方便的方法，可以将 vector 中的元素快速插入到 set 中，并自动去重和排序。这种方法特别适用于需要从其他容器转换为 set 的情况。理解这些初始化方法可以帮助更有效地利用 set 容器。"},{"title":"set 基础","path":"/wiki/C++/set/set 基础.html","content":"set 是 C++ 标准库中的一种关联容器，提供了一种高效的方式来存储和操作不重复的元素集合。set 使用平衡二叉树（通常是红黑树）来实现，因此它的元素是有序的，并且支持快速查找、插入和删除操作。 基本用法 在 C++ 中使用 set 需要包含 &lt;set&gt; 头文件，并使用 std::set 命名空间。set 的常见操作包括插入元素、删除元素、查找元素等。 特点和注意事项 有序性：set 中的元素按照特定的顺序（默认是从小到大）排列。 唯一性：set 不允许有重复的元素。 高效性：查找、插入和删除操作的时间复杂度均为 O(log n)。 自动排序：插入元素时会自动进行排序。 迭代器稳定性：set 的迭代器在插入和删除操作后仍然有效（指向被删除元素的迭代器除外）。 示例代码 以下是一些常见的 set 操作的示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;set&gt;int main() &#123; // 创建一个set容器 std::set&lt;int&gt; s; // 插入元素 s.insert(10); s.insert(5); s.insert(20); s.insert(10); // 重复元素不会被插入 // 显示set中的元素 std::cout &lt;&lt; &quot;Elements in set: &quot;; for (const auto&amp; elem : s) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; // 查找元素 auto it = s.find(10); if (it != s.end()) &#123; std::cout &lt;&lt; &quot;Element 10 found in set.&quot; &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;Element 10 not found in set.&quot; &lt;&lt; std::endl; &#125; // 删除元素 s.erase(5); // 显示删除后的元素 std::cout &lt;&lt; &quot;Elements in set after erasing 5: &quot;; for (const auto&amp; elem : s) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 总结 set 是 C++ 标准库中一个非常有用的关联容器，适用于需要存储不重复的有序元素集合的场景。了解其基本用法、特点和注意事项有助于在实际编程中有效地利用 set 提高代码的效率和可读性。"},{"title":"set 转 vector","path":"/wiki/C++/set/set 转 vector.html","content":"将 set 转换为 vector 是一种常见的操作，特别是在需要按特定顺序访问或操作集合中的元素时。以下是一些方法将 set 转换为 vector，并包括示例代码。 基本用法 在转换 set 为 vector 时，首先需要包含 &lt;set&gt; 和 &lt;vector&gt; 头文件，并使用相应的命名空间。 12#include &lt;set&gt;#include &lt;vector&gt; 特点和注意事项 自动排序：set 中的元素是有序的，在转换为 vector 后仍保持这种顺序。 去重：set 中的元素是唯一的，因此转换后的 vector 也不会包含重复的元素。 示例代码 方法 1：使用构造函数 可以直接使用 vector 的构造函数，将 set 的元素范围传递给 vector。 12345678910111213141516#include &lt;set&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s = &#123;10, 5, 20, 15&#125;; std::vector&lt;int&gt; v(s.begin(), s.end()); // 使用set的范围初始化vector std::cout &lt;&lt; &quot;Elements in vector: &quot;; for (const auto&amp; elem : v) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 方法 2：使用 std::copy 可以使用 std::copy 算法将 set 中的元素复制到 vector 中。 12345678910111213141516171819#include &lt;set&gt;#include &lt;vector&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;int main() &#123; std::set&lt;int&gt; s = &#123;10, 5, 20, 15&#125;; std::vector&lt;int&gt; v; v.reserve(s.size()); // 预留足够的空间以避免重新分配内存 std::copy(s.begin(), s.end(), std::back_inserter(v)); std::cout &lt;&lt; &quot;Elements in vector: &quot;; for (const auto&amp; elem : v) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 方法 3：使用范围 insert 方法 使用 vector 的 insert 方法将 set 中的元素范围插入到 vector 中。 1234567891011121314151617#include &lt;set&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int main() &#123; std::set&lt;int&gt; s = &#123;10, 5, 20, 15&#125;; std::vector&lt;int&gt; v; v.insert(v.end(), s.begin(), s.end()); // 使用insert方法将set的元素插入vector std::cout &lt;&lt; &quot;Elements in vector: &quot;; for (const auto&amp; elem : v) &#123; std::cout &lt;&lt; elem &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 总结 将 set 转换为 vector 有多种方法，包括直接使用构造函数、使用 std::copy 算法和使用 vector 的 insert 方法。这些方法都可以有效地将有序且不重复的 set 元素转换为 vector，以便在 vector 中进行进一步的操作。理解和掌握这些方法有助于在实际编程中灵活地进行容器之间的转换。"},{"title":"map 基础","path":"/wiki/C++/map/map 基础.html","content":"基本用法 map 是 C++ 标准模板库（STL）中的关联容器，用于存储键值对（key-value pairs），其中每个键是唯一的。键值对按照键的顺序自动排序。 声明与初始化 12345678910111213141516#include &lt;iostream&gt;#include &lt;map&gt;int main() &#123; std::map&lt;int, std::string&gt; myMap; // 插入键值对 myMap.insert(std::make_pair(1, &quot;one&quot;)); myMap[2] = &quot;two&quot;; // 访问元素 std::cout &lt;&lt; myMap[1] &lt;&lt; std::endl; // 输出 &quot;one&quot; std::cout &lt;&lt; myMap.at(2) &lt;&lt; std::endl; // 输出 &quot;two&quot; return 0;&#125; 特点和注意事项 有序性：map 中的元素按照键自动排序，默认情况下使用 &lt; 运算符进行比较。可以自定义比较器来改变排序规则。 唯一性：map 中的每个键都是唯一的。如果插入一个已存在的键，则会覆盖原有的值。 复杂度：map 的查找、插入和删除操作的平均时间复杂度为 O(log n)，这是因为它通常是用红黑树（Red-Black Tree）实现的。 内存占用：相比于 unordered_map，map 由于维护了元素的有序性，通常占用更多的内存和有略低的性能。 示例代码 以下是一个更详细的 map 使用示例，包括遍历、查找和删除操作： 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;map&gt;int main() &#123; std::map&lt;std::string, int&gt; ageMap; // 插入键值对 ageMap[&quot;Alice&quot;] = 30; ageMap[&quot;Bob&quot;] = 25; ageMap[&quot;Charlie&quot;] = 35; // 遍历 map for (const auto&amp; pair : ageMap) &#123; std::cout &lt;&lt; pair.first &lt;&lt; &quot;: &quot; &lt;&lt; pair.second &lt;&lt; std::endl; &#125; // 查找元素 auto it = ageMap.find(&quot;Bob&quot;); if (it != ageMap.end()) &#123; std::cout &lt;&lt; &quot;Found Bob, age: &quot; &lt;&lt; it-&gt;second &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;Bob not found&quot; &lt;&lt; std::endl; &#125; // 删除元素 ageMap.erase(&quot;Alice&quot;); // 检查元素是否存在 if (ageMap.count(&quot;Alice&quot;) == 0) &#123; std::cout &lt;&lt; &quot;Alice has been removed&quot; &lt;&lt; std::endl; &#125; return 0;&#125; 总结 map 是一个强大且灵活的关联容器，适用于需要自动排序键值对的场景。其有序性和高效的查找、插入、删除操作使其在许多应用中非常有用。但在使用时需要注意其相对较高的内存占用和略低的性能。"},{"title":"stringstream","path":"/wiki/C++/sstream/stringstream.html","content":"stringstream 是 C++ 标准库中的一个非常有用的类，它属于 &lt;sstream&gt; 头文件。这个类允许你把字符串当作流来处理，这意味着你可以使用类似于处理文件流（如 ifstream 和 ofstream）的方式来处理字符串。 基本用法 定义：你可以通过包含头文件 &lt;sstream&gt; 来使用 stringstream 类。std::stringstream 可用于读写字符串。 初始化：你可以直接初始化一个 stringstream 对象，也可以用一个字符串初始化它。 读写操作：使用 &lt;&lt; 操作符向 stringstream 写入数据，使用 &gt;&gt; 操作符从中读取数据。 转换为字符串：使用 str() 成员函数可以获取或设置 stringstream 的内容。 清空：使用 str(&quot;&quot;) 可以清空 stringstream 的内容，使用 clear() 可以重置它的状态。 特点和注意事项 灵活性：stringstream 非常适合做类型转换和字符串拼接。 性能：与直接操作字符串相比，使用 stringstream 可能会有一些性能损失，尤其是在大量数据操作时。 状态管理：在从 stringstream 读取数据后，应检查流的状态（如是否到达末尾）。 内存管理：stringstream 自动管理内部的字符串缓冲区，无需手动释放内存。 示例代码 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;sstream&gt;#include &lt;string&gt;int main() &#123; std::stringstream ss; ss &lt;&lt; &quot;Example &quot;; // 写入字符串 ss &lt;&lt; 2024; // 写入数字 std::string str = ss.str(); // 将 stringstream 转换为 string std::cout &lt;&lt; str &lt;&lt; std::endl; // 输出 &quot;Example 2024&quot; int num; ss.str(&quot;1234&quot;); // 设置新的字符串内容 ss &gt;&gt; num; // 从 stringstream 读取数字 std::cout &lt;&lt; num &lt;&lt; std::endl; // 输出 1234 return 0;&#125; 在这个例子中，我们展示了如何使用 stringstream 来进行基本的写入、读取和类型转换操作。"},{"title":"map 遍历","path":"/wiki/C++/map/map 遍历.html","content":"在 C++ 中，有多种方法可以遍历 map 容器。遍历 map 的过程通常涉及迭代器。下面我们将介绍三种常见的遍历方法：使用迭代器、使用范围 for 循环和使用 for_each 算法。 基本用法 使用迭代器遍历 这是最基本的方法，适用于所有 C++ 标准容器。迭代器可以让你精确控制遍历的过程。 12345678910111213141516#include &lt;iostream&gt;#include &lt;map&gt;int main() &#123; std::map&lt;int, std::string&gt; myMap = &#123; &#123;1, &quot;one&quot;&#125;, &#123;2, &quot;two&quot;&#125;, &#123;3, &quot;three&quot;&#125; &#125;; for (std::map&lt;int, std::string&gt;::iterator it = myMap.begin(); it != myMap.end(); ++it) &#123; std::cout &lt;&lt; it-&gt;first &lt;&lt; &quot;: &quot; &lt;&lt; it-&gt;second &lt;&lt; std::endl; &#125; return 0;&#125; 使用范围 for 循环遍历 C++11 引入了范围 for 循环，简化了遍历容器的过程。 12345678910111213141516#include &lt;iostream&gt;#include &lt;map&gt;int main() &#123; std::map&lt;int, std::string&gt; myMap = &#123; &#123;1, &quot;one&quot;&#125;, &#123;2, &quot;two&quot;&#125;, &#123;3, &quot;three&quot;&#125; &#125;; for (const auto&amp; pair : myMap) &#123; std::cout &lt;&lt; pair.first &lt;&lt; &quot;: &quot; &lt;&lt; pair.second &lt;&lt; std::endl; &#125; return 0;&#125; 使用 for_each 算法遍历 for_each 算法需要包含头文件 &lt;algorithm&gt; 和一个回调函数。 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;algorithm&gt;void printPair(const std::pair&lt;int, std::string&gt;&amp; p) &#123; std::cout &lt;&lt; p.first &lt;&lt; &quot;: &quot; &lt;&lt; p.second &lt;&lt; std::endl;&#125;int main() &#123; std::map&lt;int, std::string&gt; myMap = &#123; &#123;1, &quot;one&quot;&#125;, &#123;2, &quot;two&quot;&#125;, &#123;3, &quot;three&quot;&#125; &#125;; std::for_each(myMap.begin(), myMap.end(), printPair); return 0;&#125; 特点和注意事项 使用迭代器：迭代器方法灵活且功能强大，适用于需要对元素进行复杂操作的情况。 范围 for 循环：简洁且易于阅读，适用于大多数遍历需求。 for_each 算法：适用于需要将操作封装成独立函数的情况，可以使代码更加模块化。 注意：如果使用迭代器对map容器进行遍历，可以使用distance方法获得当前迭代器的下标distance(mp.begin(), it); 示例代码 以下是一个综合示例，展示了三种遍历方法： 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;map&gt;#include &lt;algorithm&gt;void printPair(const std::pair&lt;int, std::string&gt;&amp; p) &#123; std::cout &lt;&lt; p.first &lt;&lt; &quot;: &quot; &lt;&lt; p.second &lt;&lt; std::endl;&#125;int main() &#123; std::map&lt;int, std::string&gt; myMap = &#123; &#123;1, &quot;one&quot;&#125;, &#123;2, &quot;two&quot;&#125;, &#123;3, &quot;three&quot;&#125; &#125;; // 使用迭代器遍历 std::cout &lt;&lt; &quot;Using iterator:&quot; &lt;&lt; std::endl; for (std::map&lt;int, std::string&gt;::iterator it = myMap.begin(); it != myMap.end(); ++it) &#123; std::cout &lt;&lt; it-&gt;first &lt;&lt; &quot;: &quot; &lt;&lt; it-&gt;second &lt;&lt; std::endl; &#125; // 使用范围 for 循环遍历 std::cout &lt;&lt; &quot;Using range-based for loop:&quot; &lt;&lt; std::endl; for (const auto&amp; pair : myMap) &#123; std::cout &lt;&lt; pair.first &lt;&lt; &quot;: &quot; &lt;&lt; pair.second &lt;&lt; std::endl; &#125; // 使用 for_each 算法遍历 std::cout &lt;&lt; &quot;Using for_each algorithm:&quot; &lt;&lt; std::endl; std::for_each(myMap.begin(), myMap.end(), printPair); return 0;&#125; 输出结果如下： 123456789101112Using iterator:1: one2: two3: threeUsing range-based for loop:1: one2: two3: threeUsing for_each algorithm:1: one2: two3: three 总结 遍历 map 容器有多种方法，每种方法都有其适用场景和优缺点。使用迭代器方法灵活但代码较长，范围 for 循环简洁易读，for_each 算法适用于需要模块化的场景。选择合适的方法可以使代码更加清晰和高效。"},{"title":"accumulate","path":"/wiki/C++/numeric/accumulate.html","content":"accumulate 函数是 C++ 标准库中 &lt;numeric&gt; 头文件的一部分，主要用于计算给定范围内元素的累积和或其他累积操作。 基本用法 accumulate 函数的原型如下： 12345template &lt;class InputIterator, class T&gt;T accumulate (InputIterator first, InputIterator last, T init);template &lt;class InputIterator, class T, class BinaryOperation&gt;T accumulate (InputIterator first, InputIterator last, T init, BinaryOperation binary_op); first 和 last：确定要处理的元素范围的迭代器。 init：累积的初始值。 binary_op（可选）：一个二元操作，用于替代默认的加法操作。 特点和注意事项 默认行为：在不提供 binary_op 的情况下，默认执行加法操作。 自定义操作：通过提供 binary_op，可以执行任何二元操作，如乘法、最大值等。 类型匹配：init 的类型应与容器元素类型兼容，否则可能导致意外的结果或编译错误。 性能：这是一个线性时间复杂度的操作，因为它遍历整个范围。 示例代码 以下是使用 accumulate 的一个示例，展示了基本的累加操作和自定义操作： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;numeric&gt;int main() &#123; std::vector&lt;int&gt; nums = &#123;1, 2, 3, 4, 5&#125;; // 默认累加 int sum = std::accumulate(nums.begin(), nums.end(), 0); std::cout &lt;&lt; &quot;Sum: &quot; &lt;&lt; sum &lt;&lt; std::endl; // 使用自定义操作（乘法） int product = std::accumulate(nums.begin(), nums.end(), 1, std::multiplies&lt;int&gt;()); std::cout &lt;&lt; &quot;Product: &quot; &lt;&lt; product &lt;&lt; std::endl; return 0;&#125; 在这个例子中，第一个 accumulate 调用计算了 nums 的总和，而第二个调用使用了乘法操作来计算所有元素的乘积。"},{"title":"lower_bound","path":"/wiki/C++/algorithm/lower_bound.html","content":"在 C++ 中，&lt;algorithm&gt; 头文件中的lower_bound 函数是一个非常有用的算法，它用于在已排序的范围内查找第一个不小于给定值的元素。下面我将详细解释这个函数的基本用法、特点和注意事项，以及提供示例代码。 基本用法 函数原型：lower_bound(ForwardIterator first, ForwardIterator last, const T&amp; val) 参数： first 和 last 是定义要搜索的范围的迭代器。 val 是我们要查找的值。 返回值：指向范围中第一个不小于 val 的元素的迭代器。如果所有元素都小于 val，则返回 last。 特点和注意事项 lower_bound 需要预排序的范围。如果范围没有排序，结果是未定义的。 它使用二分查找算法，因此时间复杂度为 O(log n)，其中 n 是范围内元素的数量。 如果范围中存在多个等于 val 的元素，lower_bound 会返回指向这些元素中第一个的迭代器。 对于自定义类型，你可能需要提供比较函数或重载比较操作符。 示例代码 123456789101112131415161718#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main() &#123; std::vector&lt;int&gt; vec = &#123;1, 2, 4, 4, 5, 6, 7&#125;; // 查找第一个不小于4的元素 auto it = std::lower_bound(vec.begin(), vec.end(), 4); if (it != vec.end()) &#123; std::cout &lt;&lt; &quot;第一个不小于4的元素是: &quot; &lt;&lt; *it &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;没有找到不小于4的元素&quot; &lt;&lt; std::endl; &#125; return 0;&#125; 在这个例子中，我们在一个已排序的 vector 中查找第一个不小于4的元素。由于集合中有两个4，lower_bound 返回指向第一个4的迭代器。"},{"title":"max_element","path":"/wiki/C++/algorithm/max_element.html","content":"在 C++ 中，max_element 是一个来自 &lt;algorithm&gt; 头文件的函数，用于查找给定范围内的最大元素。以下是关于 max_element 的详细解释： 基本用法 函数原型: max_element 函数的基本原型为 max_element(Iterator first, Iterator last)，其中 Iterator 是指向容器元素的迭代器。 返回值: 它返回一个指向给定范围中最大元素的迭代器。如果有多个相同的最大元素，返回第一个这样的元素的迭代器。 范围: 第一个参数是范围的开始，第二个参数是范围的结束（不包括）。 特点和注意事项 容器类型: max_element 可以用于任何提供随机访问迭代器的容器，如 vector、deque、数组等。 比较函数: 可以提供自定义的比较函数来定义“最大”元素的条件。 空范围: 如果范围为空（即 first 等于 last），则返回 last。 性能: 时间复杂度通常是线性的，即 O(n)，其中 n 是范围内元素的数量。 示例代码 123456789101112131415#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main() &#123; std::vector&lt;int&gt; v = &#123;1, 3, 2, 8, 5&#125;; auto max_it = std::max_element(v.begin(), v.end()); if (max_it != v.end()) &#123; std::cout &lt;&lt; &quot;最大元素: &quot; &lt;&lt; *max_it &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;向量为空&quot; &lt;&lt; std::endl; &#125; return 0;&#125; 在这个例子中，max_element 用于查找 vector 中的最大元素。如果找到，它打印出该元素的值。"},{"title":"next_permutation","path":"/wiki/C++/algorithm/next_permutation.html","content":"next_permutation 是 C++ 标准库 &lt;algorithm&gt; 中的一个函数，用于对序列生成下一个字典序排列。如果当前序列已经是字典序的最后一个排列，则生成第一个排列（即最小的排列）。 基本用法 123456789101112131415#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int main() &#123; std::vector&lt;int&gt; vec = &#123;1, 2, 3&#125;; do &#123; for (int x : vec) std::cout &lt;&lt; x &lt;&lt; &quot; &quot;; std::cout &lt;&lt; &quot; &quot;; &#125; while (std::next_permutation(vec.begin(), vec.end())); return 0;&#125; 特点和注意事项 返回值：next_permutation 返回一个布尔值。如果成功生成下一个排列，返回 true；如果已经是最后一个排列并生成了第一个排列，返回 false。 原地操作：next_permutation 对传入的序列进行原地重排，不会分配额外的内存。 序列要求：适用于任何支持随机访问迭代器的序列，如 std::vector、std::array、原始数组等。 算法复杂度：在最坏情况下，next_permutation 的时间复杂度是 O(N)，其中 N 是序列的长度。 示例代码 下面是一个更详细的示例，演示了 next_permutation 的使用，并说明了如何在不同的数据类型上应用： 12345678910111213141516171819202122232425#include &lt;algorithm&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int main() &#123; // 示例 1：使用整数向量 std::vector&lt;int&gt; vec = &#123;1, 2, 3&#125;; std::cout &lt;&lt; &quot;整数向量的排列: &quot;; do &#123; for (int x : vec) std::cout &lt;&lt; x &lt;&lt; &quot; &quot;; std::cout &lt;&lt; &quot; &quot;; &#125; while (std::next_permutation(vec.begin(), vec.end())); // 示例 2：使用字符数组 char arr[] = &#123;&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;&#125;; std::cout &lt;&lt; &quot; 字符数组的排列: &quot;; do &#123; for (char x : arr) std::cout &lt;&lt; x &lt;&lt; &quot; &quot;; std::cout &lt;&lt; &quot; &quot;; &#125; while (std::next_permutation(arr, arr + 3)); return 0;&#125; 在这个示例中，我们分别对整数向量和字符数组进行了排列。使用 next_permutation 函数可以方便地生成序列的所有排列，并进行处理或输出。需要注意的是，如果需要生成排列的序列是已排序的，那么第一次调用 next_permutation 将生成其下一个字典序排列，否则可能需要多次调用才能达到效果。"},{"title":"sort","path":"/wiki/C++/algorithm/sort.html","content":"基本用法 sort 函数是 C++ 标准库中的一个强大的排序算法，通常实现为快速排序。这个函数定义在 &lt;algorithm&gt; 头文件中。sort 可以对一个序列进行排序，使之按照升序排列。基本语法如下： 12sort(RandomAccessIterator first, RandomAccessIterator last);sort(RandomAccessIterator first, RandomAccessIterator last, Compare comp); first 和 last 是定义待排序序列的随机访问迭代器。 comp 是一个可选的比较函数或者函数对象，用于自定义排序顺序。 特点和注意事项 时间复杂度: sort 的平均时间复杂度是 O(n log n)，其中 n 是[first, last)范围内元素的数量。 随机访问迭代器: sort 需要随机访问迭代器，因此适用于 std::vector、std::deque、数组等，但不适用于 std::list。 稳定性: 标准库中的 sort 不保证排序的稳定性。如果需要稳定排序，可以使用 stable_sort。 自定义排序: 通过提供比较函数，可以实现自定义排序逻辑。 示例代码 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main() &#123; std::vector&lt;int&gt; v = &#123;4, 2, 5, 1, 3&#125;; // 按升序排序 std::sort(v.begin(), v.end()); std::cout &lt;&lt; &quot;Sorted array: &quot;; for (int i : v) &#123; std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; // 使用自定义比较函数进行降序排序 std::sort(v.begin(), v.end(), [](int a, int b) &#123; return a &gt; b; &#125;); std::cout &lt;&lt; &quot;Sorted in descending order: &quot;; for (int i : v) &#123; std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 在这个例子中，首先使用 sort 将向量 v 按升序排序，然后使用自定义比较函数将其按降序排序。 这里展示的就是使用自定义匿名函数重新规定排序规则"},{"title":"upper_bound","path":"/wiki/C++/algorithm/upper_bound.html","content":"基本用法 upper_bound 函数在 C++ 中用于在有序范围内查找大于给定值的第一个元素。这个函数属于 &lt;algorithm&gt; 头文件。它使用二分查找算法，因此效率较高。基本语法如下： 1upper_bound(ForwardIterator first, ForwardIterator last, const T&amp; val); first 和 last 是定义待搜索区间的迭代器。 val 是我们要查找的值。 函数返回一个指向找到的元素的迭代器，如果没有找到，则返回 last。 特点和注意事项 时间复杂度: upper_bound 的时间复杂度是 O(log n)，其中 n 是[first,last)[first, last)[first,last) 范围内元素的数量。 要求有序: 使用 upper_bound 前，确保范围 [first,last)[first, last)[first,last) 已排序。 返回值: 如果范围内所有元素都小于或等于 val，则返回 last。 自定义比较: 可以提供自定义比较函数。 示例代码 123456789101112131415161718#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;int main() &#123; std::vector&lt;int&gt; v = &#123;1, 2, 4, 4, 5, 6, 7&#125;; // 查找第一个大于 4 的元素 auto it = std::upper_bound(v.begin(), v.end(), 4); if (it != v.end()) &#123; std::cout &lt;&lt; &quot;The first element greater than 4 is: &quot; &lt;&lt; *it &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;No element greater than 4 found.&quot; &lt;&lt; std::endl; &#125; return 0;&#125; 在此例中，upper_bound 会返回指向值 5 的迭代器，因为 5 是数组中第一个大于 4 的元素。 当然upper_bound和sort一样还可以自定义比较的内容，下面是另外一个例子： 取自2023-12-08 Leetcode 每日一题解法 123456789101112131415161718class Solution &#123;public: long long maxTaxiEarnings(int n, vector&lt;vector&lt;int&gt;&gt;&amp; rides) &#123; sort(rides.begin(),rides.end(), [](const vector&lt;int&gt; &amp;a, const vector&lt;int&gt; &amp;b)&#123; if(a[1]==b[1])return a[0] &lt; b[0]; return a[1] &lt; b[1]; &#125;); int p_num = rides.size(); vector&lt;long long&gt; dp(p_num+1); for(int i=0;i&lt;p_num;++i)&#123; int j = upper_bound(rides.begin(), rides.begin() + i, rides[i][0], [](int x, const vector&lt;int&gt; &amp;r)&#123; return x &lt; r[1]; &#125;) - rides.begin(); dp[i+1] = max(dp[i], dp[j]+rides[i][1]-rides[i][0]+rides[i][2]); &#125; return dp[p_num]; &#125;&#125;; 该内嵌自定义匿名函数主要实现了告知upper_bound函数如何将传入的整数和vector中的vector类型比较大小。"},{"title":"getline","path":"/wiki/C++/string/getline.html","content":"getline 是 C++ 中用于从输入流中读取字符串的函数，通常与文件流（如 ifstream）或标准输入（如 cin）一起使用。它属于 &lt;string&gt; 头文件。 基本用法 定义：getline 函数定义在 &lt;string&gt; 头文件中，通常与 &lt;iostream&gt; 一起使用。 函数原型：std::getline(std::istream&amp; stream, std::string&amp; str, char delim)。 stream：要读取的输入流，如 cin 或文件流对象。 str：用于存储读取到的字符串的 std::string 对象。 delim（可选）：作为行结束符的分隔符，默认为换行符 。 读取行：getline 会读取输入直到遇到分隔符（默认为换行符），并将读取的内容（不包括分隔符）存储在 str 中。 特点和注意事项 安全性：与使用 cin &gt;&gt; 直接读取字符串相比，getline 可以避免因字符串中的空格而导致的读取中断。 灵活性：你可以自定义分隔符，便于处理不同格式的输入。 输入结束：如果在达到分隔符之前到达文件末尾或发生错误，getline 将设置输入流的状态标志。 空行处理：getline 也能读取空行，此时返回的字符串将为空。 示例代码 12345678910111213#include &lt;iostream&gt;#include &lt;string&gt;int main() &#123; std::string line; std::cout &lt;&lt; &quot;请输入一些文本（包含空格）: &quot;; std::getline(std::cin, line); std::cout &lt;&lt; &quot;你输入的文本是: &quot; &lt;&lt; line &lt;&lt; std::endl; return 0;&#125; 这个例子展示了如何使用 std::getline 从标准输入读取一行文本。这种方式可以正确处理含有空格的字符串输入。"},{"title":"stoi","path":"/wiki/C++/string/stoi.html","content":"stoi 是 C++11 标准引入的一个函数，用于将字符串转换为整数。以下是详细的解释： 基本用法 stoi 是一个标准库函数，定义在 &lt;string&gt; 头文件中。它的主要功能是将 std::string 或 C 风格字符串转换为 int 类型。 函数签名如下： 1int stoi(const std::string&amp; str, std::size_t* pos = nullptr, int base = 10); 特点和注意事项 输入参数： str: 要转换的字符串。 pos: 一个指向 std::size_t 的指针，用于存储第一个未处理字符的位置。如果不需要这个信息，可以传递 nullptr。 base: 基数，用于指定数字的进制，默认为 10。支持 2 到 36 之间的任何进制。 返回值： 成功时，返回字符串转换后的 int 值。 异常处理： std::invalid_argument: 如果字符串不包含有效的数字，将抛出该异常。 std::out_of_range: 如果转换结果超出了 int 类型的范围，将抛出该异常。 注意事项： 确保输入字符串是有效的整数表示，否则会抛出异常。 注意检查异常以防止程序崩溃。 stoi 仅能处理表示整数的字符串，对于浮点数转换请使用 stof、stod 或 stold。 示例代码 以下是一些使用 stoi 的示例代码： 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;string&gt;int main() &#123; // 示例 1：基本用法 std::string str = &quot;123&quot;; int num = std::stoi(str); std::cout &lt;&lt; &quot;字符串 \\&quot;&quot; &lt;&lt; str &lt;&lt; &quot;\\&quot; 转换为整数: &quot; &lt;&lt; num &lt;&lt; std::endl; // 示例 2：处理无效输入 try &#123; std::string invalid_str = &quot;abc&quot;; int invalid_num = std::stoi(invalid_str); &#125; catch (const std::invalid_argument&amp; e) &#123; std::cerr &lt;&lt; &quot;无效的输入字符串: &quot; &lt;&lt; e.what() &lt;&lt; std::endl; &#125; // 示例 3：处理超出范围的输入 try &#123; std::string out_of_range_str = &quot;99999999999999999999999999&quot;; int out_of_range_num = std::stoi(out_of_range_str); &#125; catch (const std::out_of_range&amp; e) &#123; std::cerr &lt;&lt; &quot;输入字符串超出范围: &quot; &lt;&lt; e.what() &lt;&lt; std::endl; &#125; // 示例 4：使用不同的进制 std::string hex_str = &quot;1A&quot;; int hex_num = std::stoi(hex_str, nullptr, 16); std::cout &lt;&lt; &quot;16 进制字符串 \\&quot;&quot; &lt;&lt; hex_str &lt;&lt; &quot;\\&quot; 转换为整数: &quot; &lt;&lt; hex_num &lt;&lt; std::endl; return 0;&#125; 结论 stoi 是一个非常有用的函数，适用于从字符串中提取整数值。通过正确的异常处理，可以安全地使用它来处理不同格式的输入。"},{"title":"unordered_map 基础","path":"/wiki/C++/unordered_map/unordered_map 基础.html","content":"unordered_map 是 C++ 标准模板库（STL）中的一个非常有用的容器。它被用来存储键值对，其中每个键都是唯一的。这个容器使用哈希表来实现，因此其在平均情况下为键的查找、插入和删除操作提供了常数时间复杂度（O(1)）。下面是关于 unordered_map 的详细介绍： 基本用法 初始化：可以通过直接声明或使用初始化列表来创建 unordered_map。 插入元素：使用 insert 方法或 [] 操作符来插入新元素。 访问元素：使用 [] 操作符或 at 方法来访问元素。 删除元素：使用 erase 方法来删除元素。 查找元素：使用 find 方法来查找特定键的元素。 大小和容量：使用 size 方法获取元素个数，empty 检查是否为空。 特点和注意事项 性能：在平均情况下，插入、删除和查找操作的时间复杂度都是 O(1)。但在最坏情况下，这些操作的时间复杂度可能退化为 O(n)。 哈希函数：unordered_map 使用哈希函数将键映射到哈希表中的桶。对于自定义类型，可能需要定义自己的哈希函数。 碰撞处理：当两个键映射到同一个桶时，unordered_map 通过链表来处理碰撞。 无序性：如其名所示，unordered_map 中的元素是无序存储的，不保证元素的顺序。 唯一键：每个键在 unordered_map 中必须是唯一的。 示例代码 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;unordered_map&gt;int main() &#123; std::unordered_map&lt;std::string, int&gt; umap; // 插入元素 umap[&quot;apple&quot;] = 5; umap[&quot;banana&quot;] = 8; // 访问元素 std::cout &lt;&lt; &quot;apple count: &quot; &lt;&lt; umap[&quot;apple&quot;] &lt;&lt; std::endl; // 查找元素 if (umap.find(&quot;banana&quot;) != umap.end()) &#123; std::cout &lt;&lt; &quot;banana found&quot; &lt;&lt; std::endl; &#125; // 删除元素 umap.erase(&quot;apple&quot;); // 遍历元素 for (auto&amp; item : umap) &#123; std::cout &lt;&lt; item.first &lt;&lt; &quot; =&gt; &quot; &lt;&lt; item.second &lt;&lt; std::endl; &#125; return 0;&#125; 这个示例展示了 unordered_map 的基本用法，包括如何插入、访问、查找和删除元素。使用 unordered_map 可以有效地处理大量的数据，尤其是在需要快速查找和更新数据的场合。"},{"title":"move","path":"/wiki/C++/utility/move.html","content":"在 C++ 中，“move”操作是一种优化技术，主要用于减少不必要的对象复制，从而提高程序性能。它是 C++11 标准中引入的一个重要特性。下面，我将分别从基本用法、特点和注意事项、以及示例代码三个方面来讲解 move 操作。 基本用法 std::move 是一个函数模板，定义在头文件 &lt;utility&gt; 中。它可以将一个对象转换为右值引用，从而使得该对象的资源可以被“移动”而非复制。 使用 move 操作时，原对象会转入一个不确定但有效的状态。这意味着，原对象仍然可以析构或赋予新值，但其具体内容不再确定。 特点和注意事项 性能提升：通过移动对象而非复制对象，可以显著减少内存的分配和释放，提高程序的运行效率。 所有权转移：执行 move 操作后，资源的所有权从一个对象转移到另一个对象，原对象不再拥有这些资源。 安全使用：使用 move 后，原始对象处于一个不确定的状态。因此，除非重新赋值，否则不应再次使用这些被移动的对象。 兼容性：move 操作只适用于支持移动语义的对象。对于不支持移动语义的对象，move 操作会退化为复制操作。 示例代码 下面是一个使用 std::move 的示例，展示了如何通过移动构造函数和移动赋值操作符来实现两个对象之间的资源转移。 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;utility&gt;#include &lt;vector&gt;class MovableClass &#123;public: std::vector&lt;int&gt; data; // 移动构造函数 MovableClass(MovableClass&amp;&amp; other) noexcept : data(std::move(other.data)) &#123; std::cout &lt;&lt; &quot;Moved!&quot; &lt;&lt; std::endl; &#125; // 移动赋值操作符 MovableClass&amp; operator=(MovableClass&amp;&amp; other) noexcept &#123; if (this != &amp;other) &#123; data = std::move(other.data); std::cout &lt;&lt; &quot;Moved assignment!&quot; &lt;&lt; std::endl; &#125; return *this; &#125;&#125;;int main() &#123; MovableClass obj1; obj1.data = &#123;1, 2, 3&#125;; // 使用移动构造函数 MovableClass obj2 = std::move(obj1); // 使用移动赋值操作符 MovableClass obj3; obj3 = std::move(obj2); return 0;&#125; 在这个示例中，我们通过移动构造函数和移动赋值操作符实现了 MovableClass 对象之间的资源转移。注意，在移动操作之后，原对象（例如 obj1 和 obj2）进入了不确定的状态，但它们仍然处于一个有效的状态，可以被析构或重新赋值。"},{"title":"assign","path":"/wiki/C++/vector/assign.html","content":"在C++中，std::vector 类的 assign 方法用于在向量中设置新内容，替换其当前内容。以下是详细的解释： 基本用法 assign 方法有几种重载形式，允许你用不同的方式设置 vector 的内容： assign(size_type n, const T&amp; val)：将向量的内容替换为 n 个 val 的副本。 assign(InputIterator first, InputIterator last)：使用两个迭代器 first 和 last 指定的范围来替换向量的内容。 assign(initializer_list&lt;T&gt; il)：使用初始化列表 il 中的元素来替换向量的内容。 特点和注意事项 assign 方法改变向量的大小和内容。 如果新大小大于当前大小，会创建新元素。如果新大小小于当前大小，则多余的元素会被销毁。 使用迭代器范围重载时，范围 [first, last) 应有效且不应指向同一 vector 中的元素。 分配新内容可能导致所有现有的迭代器、引用和指针失效。 示例代码 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; vec; // 使用初始值赋值 vec.assign(4, 100); for (int i : vec) &#123; std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 100 100 100 100 &#125; std::cout &lt;&lt; &quot; &quot;; // 使用迭代器赋值 std::vector&lt;int&gt; anotherVec&#123;1, 2, 3, 4, 5&#125;; vec.assign(anotherVec.begin(), anotherVec.end()); for (int i : vec) &#123; std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 1 2 3 4 5 &#125; std::cout &lt;&lt; &quot; &quot;; // 使用初始化列表赋值 vec.assign(&#123;10, 20, 30&#125;); for (int i : vec) &#123; std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 10 20 30 &#125; std::cout &lt;&lt; &quot; &quot;; return 0;&#125; 在这个例子中，我们展示了如何使用不同的 assign 方法来设置 vector 的内容。每种方法都清除原有内容，并根据提供的新值设置 vector。"},{"title":"emplace_back","path":"/wiki/C++/vector/emplace_back.html","content":"std::vector 的 emplace_back 方法是 C++11 引入的一项功能，它用于在向量的末尾直接构造新元素，而不是先构造然后复制或移动到向量中。这可以提高效率，尤其是对于那些不支持复制或效率较低的复制操作的对象。 基本用法 emplace_back(args...)：此方法接受与元素类型构造函数相同的参数集，并在向量末尾直接构造一个新元素。这意味着它可以避免额外的复制或移动操作。 特点和注意事项 直接构造：与 push_back 不同，emplace_back 直接在向量的内存空间中构造元素，这可以提高性能，特别是对于大型对象或拥有复杂移动语义的对象。 参数转发：它将参数完美转发到元素的构造函数，意味着可以根据传递的参数自动选择合适的构造函数。 容量变化：如果当前向量的大小等于容量，emplace_back 会引起向量的重新分配，这可能导致所有现有的迭代器、引用和指针失效。 异常安全性：如果构造函数抛出异常，emplace_back 保证不会导致内存泄漏。 示例代码 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;struct MyClass &#123; MyClass(int x, std::string y) : num(x), str(y) &#123;&#125; int num; std::string str;&#125;;int main() &#123; std::vector&lt;MyClass&gt; vec; // 直接在向量末尾构造一个 MyClass 对象 vec.emplace_back(10, &quot;Hello&quot;); for (const auto&amp; item : vec) &#123; std::cout &lt;&lt; item.num &lt;&lt; &quot;, &quot; &lt;&lt; item.str &lt;&lt; std::endl; &#125; return 0;&#125; 在这个例子中，emplace_back 被用来直接在 vector 末尾构造一个 MyClass 对象。这避免了先构造一个临时对象然后将其复制或移动到向量中的情况。 与push_back的区别 push_back 和 emplace_back 都是用于向容器（如 std::vector）中添加元素的成员函数，但它们在添加新元素时的方式和性能有所不同。 push_back: 这个函数用于在容器的末尾添加一个新元素。当使用 push_back 时，它会创建该元素的副本或移动，这取决于提供的参数。如果参数是一个对象，那么 push_back 会调用该对象的拷贝构造函数或移动构造函数（如果适用）。这可能会导致额外的性能开销，特别是当对象较大或复杂时。 emplace_back: 这个函数也用于在容器末尾添加新元素，但它的工作方式略有不同。emplace_back 会直接在容器的末尾构造元素，而不是首先创建一个临时对象然后拷贝或移动到容器中。它通过接受构造函数的参数来实现这一点，而不是一个已经构造好的对象。这意味着使用 emplace_back 可以减少不必要的拷贝或移动操作，从而提高效率。 简而言之，emplace_back 通常提供更好的性能，尤其是在添加复杂对象到容器中时，因为它减少了拷贝和移动的次数。然而，在某些情况下，使用 push_back 和 emplace_back 的效果可能是相同的，尤其是对于简单或小型数据类型。"},{"title":"insert","path":"/wiki/C++/vector/insert.html","content":"在 C++ 中，vector 类型的 insert 方法是一个重要的成员函数，用于在指定位置插入元素。下面将详细介绍这个方法的基本用法、特点和注意事项，以及提供一个示例代码。 基本用法 std::vector 的 insert 方法允许在向量中的特定位置插入一个或多个元素。这个方法接收的第一个参数是一个迭代器，指示插入位置。其余参数取决于插入操作的类型。 特点和注意事项 位置指定：insert 方法需要一个迭代器来指定插入的位置。如果位置迭代器不合法（比如超出了向量的当前范围），程序可能会崩溃。 性能考虑：在 vector 中间插入元素可能导致后面所有元素的移动，这可能是一个开销较大的操作，特别是对于含有大量元素的 vector。 容量调整：如果需要的话，insert 操作会增加 vector 的容量，以容纳新元素。 多种重载：insert 方法有多种重载形式，可以插入单个元素、插入另一个容器中的元素范围，甚至插入多个重复的元素。 示例代码 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; vec = &#123;10, 20, 30, 40&#125;; // 插入单个元素 auto it = vec.begin() + 2; // 在第三个元素之前插入 vec.insert(it, 25); // 插入多个重复元素 vec.insert(vec.begin() + 1, 3, 15); // 在第二个元素位置插入三个15 // 输出结果 for (int num : vec) &#123; std::cout &lt;&lt; num &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 在这个例子中，我们首先在第三个元素前插入了一个值为 25 的元素，然后在第二个元素的位置插入了三个值为 15 的元素。注意在插入元素后，迭代器可能会失效，因此在插入后重新获取迭代器的位置是一个好习惯。 vector中insert另一个vector 如果要使用 std::vector 的 insert 方法插入另一个 vector，可以通过指定插入范围的方式来完成。这通常涉及到提供一个插入点（通过迭代器指定）以及被插入 vector 的开始和结束迭代器。 使用范围插入的基本语法如下： 1vector.insert(position, startIterator, endIterator); position 是一个指向原 vector 中的位置的迭代器，表示新元素插入的位置。 startIterator 和 endIterator 分别是被插入 vector 的开始和结束迭代器，定义了要插入的元素范围。 特点和注意事项 迭代器失效：在 insert 操作后，原 vector 的迭代器可能会失效，特别是如果发生内存重新分配时。 性能影响：插入操作会导致从插入点到 vector 末尾的所有元素被移动，因此在大型 vector 中执行插入操作可能会影响性能。 自我插入：要注意不要尝试将 vector 的部分内容插入到自身中，这可能导致未定义的行为。 示例代码 1234567891011121314151617#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; vec1 = &#123;1, 2, 3, 4&#125;; std::vector&lt;int&gt; vec2 = &#123;5, 6, 7, 8&#125;; // 在 vec1 的第三个元素前插入 vec2 的所有元素 vec1.insert(vec1.begin() + 2, vec2.begin(), vec2.end()); // 输出 vec1 的内容 for (int num : vec1) &#123; std::cout &lt;&lt; num &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 在这个例子中，vec1 初始包含元素 &#123;1, 2, 3, 4&#125;，然后我们在其第三个元素前插入了 vec2 中的所有元素，最终 vec1 的内容变为 &#123;1, 2, 5, 6, 7, 8, 3, 4&#125;。"},{"title":"resize","path":"/wiki/C++/vector/resize.html","content":"基本用法 在 C++ 中，std::vector 类的 resize 方法用于改变向量的大小。当你调用 resize(n) 时，它会将向量的大小调整为 n。如果 n 大于当前向量的大小，则新元素将被添加到向量的末尾，这些新元素会被初始化为默认值。如果 n 小于当前向量的大小，向量将被缩减，多余的元素会被丢弃。 特点和注意事项 改变大小的影响：当 resize 增加向量大小时，新添加的元素会被默认初始化。对于基本数据类型（如 int、double 等），这意味着新元素的初始值不确定。对于类对象，将调用默认构造函数。 效率考虑：频繁调用 resize 可能影响性能，因为每次大小改变可能涉及内存分配和元素复制或销毁。 安全性：与 reserve 方法不同，resize 会改变向量的实际元素数量，而 reserve 只改变容量。 重载版本：resize 还有一个重载版本 resize(n, value)，可以指定新添加元素的初始值。 示例代码 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; v = &#123;1, 2, 3&#125;; // 增加大小到 5，默认初始化新元素 v.resize(5); for (int i : v) std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 1 2 3 0 0 std::cout &lt;&lt; &quot; &quot;; // 减小大小到 2，移除多余元素 v.resize(2); for (int i : v) std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 1 2 std::cout &lt;&lt; &quot; &quot;; // 再次增加大小，并指定新元素的初始值为 99 v.resize(4, 99); for (int i : v) std::cout &lt;&lt; i &lt;&lt; &quot; &quot;; // 输出: 1 2 99 99 std::cout &lt;&lt; &quot; &quot;; return 0;&#125; 在这个例子中，你可以看到如何使用 resize 来增加和减少向量的大小，以及如何指定新元素的初始化值。"},{"title":"vector 基础","path":"/wiki/C++/vector/vector 基础.html","content":"在 C++ 中，vector 是一个基于模板的序列容器，它封装了可以动态改变大小的数组。以下是关于 vector 的详细解释： 基本用法 定义和初始化: vector&lt;T&gt; 定义一个类型为 T 的向量。例如，vector&lt;int&gt; v; 创建一个空的整数向量。你也可以用初始化列表 vector&lt;int&gt; v = &#123;1, 2, 3&#125;; 创建并初始化向量。 添加元素: 使用 push_back(value) 在向量的末尾添加元素。 访问元素: 通过 operator[] 或 at(index) 访问元素。at 方法在越界时抛出异常，而 operator[] 不进行边界检查。 大小和容量: 使用 size() 来获取向量中的元素个数，capacity() 获取向量的当前容量。 迭代器: 提供迭代器（如 begin(), end()）来遍历向量中的元素。 特点和注意事项 动态大小: 与普通数组不同，vector 可以根据需要动态增长和收缩。 内存管理: 当新元素添加到 vector 使得当前容量不足时，vector 会自动重新分配内存以容纳更多元素。 性能: vector 提供对尾部元素的快速访问和添加，但在中间或开始插入元素可能较慢，因为可能涉及元素的移动。 连续存储: vector 的元素存储在连续的内存位置上，这意味着你可以像使用数组一样，通过指针算术操作 vector 的元素。 示例代码 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; vec; vec.push_back(10); vec.push_back(20); vec.push_back(30); // 使用索引访问元素 std::cout &lt;&lt; &quot;第一个元素: &quot; &lt;&lt; vec[0] &lt;&lt; std::endl; // 使用迭代器遍历向量 for(auto it = vec.begin(); it != vec.end(); ++it) &#123; std::cout &lt;&lt; *it &lt;&lt; &#x27; &#x27;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 这个示例展示了如何创建 vector，向其中添加元素，访问元素以及使用迭代器遍历 vector。"},{"title":"tuple 基础","path":"/wiki/C++/tuple/tuple 基础.html","content":"在C++中，tuple是一个非常有用的工具，它允许你将不同类型的元素组合成单一的复合类型。这里，我将根据你的要求，依次介绍tuple的基本用法、特点和注意事项，以及提供一些示例代码。 基本用法 tuple是标准模板库（STL）的一部分，位于&lt;tuple&gt;头文件中。它可以存储任意数量和类型的元素，每个元素都有其对应的类型和位置。tuple的元素可以通过std::get函数访问，其索引是从0开始的。 特点和注意事项 类型安全：与数组或结构体相比，tuple提供了更强的类型安全性，因为它可以存储不同类型的元素。 不定长：tuple可以包含任意数量的元素，使得它非常灵活。 元素访问：通过std::get&lt;索引&gt;(tuple对象)来访问元素。需要注意的是，索引是在编译时确定的，因此不能动态地在运行时决定要访问的元素索引。 与结构体的比较：对于简单的数据聚合，结构体可能是更好的选择，因为它们提供了更清晰的语义。然而，对于需要存储不同类型数据或数据结构在运行时才能确定的情况，tuple则更加适用。 性能考虑：尽管tuple提供了很大的灵活性，但使用不当可能会导致性能下降。特别是频繁访问tuple元素时，应考虑是否有更合适的数据结构。 示例代码 创建一个tuple并访问其元素： 123456789101112131415#include &lt;tuple&gt;#include &lt;iostream&gt;#include &lt;string&gt;int main() &#123; // 创建一个包含整数、字符串和浮点数的tuple std::tuple&lt;int, std::string, double&gt; myTuple = std::make_tuple(10, &quot;Test&quot;, 3.14); // 访问并打印tuple的元素 std::cout &lt;&lt; &quot;整数: &quot; &lt;&lt; std::get&lt;0&gt;(myTuple) &lt;&lt; std::endl; std::cout &lt;&lt; &quot;字符串: &quot; &lt;&lt; std::get&lt;1&gt;(myTuple) &lt;&lt; std::endl; std::cout &lt;&lt; &quot;浮点数: &quot; &lt;&lt; std::get&lt;2&gt;(myTuple) &lt;&lt; std::endl; return 0;&#125; 这段代码首先包含了必要的头文件，然后创建了一个tuple对象myTuple，它包含了一个int类型、一个std::string类型和一个double类型的元素。随后通过std::get函数访问并打印了每个元素的值。 tuple是C++中一个非常强大且灵活的工具，能够帮助你处理复杂的数据结构。然而，合理选择数据结构对于提高程序性能和可读性都是非常重要的。"},{"title":"unordered_set 基础","path":"/wiki/C++/unordered_set/unordered_set 基础.html","content":"unordered_set 是 C++ 中的标准库容器，它是一个存储唯一元素的集合，其中每个元素的位置不由元素的值决定。unordered_set 基于哈希表实现，因此它能够提供平均常数时间复杂度的插入、删除和查找操作。接下来，我将从基本用法、特点和注意事项、以及示例代码三个方面来讲解 unordered_set。 基本用法 头文件：使用 unordered_set 前，需要包含头文件 &lt;unordered_set&gt;。 声明：可以通过 std::unordered_set&lt;T&gt; 声明一个集合，其中 T 是存储元素的类型。 操作： .insert(value)：向集合中插入元素 value。 .erase(value)：从集合中移除元素 value。 .find(value)：查找元素 value，如果找到则返回一个指向该元素的迭代器，否则返回 end()。 .size()：返回集合中元素的数量。 .clear()：清空集合中的所有元素。 特点和注意事项 唯一性：unordered_set 中的每个元素都必须是唯一的，即不允许重复的元素。 无序性：元素在 unordered_set 中的存储是无序的，不保证任何特定的元素顺序。 性能考虑：虽然 unordered_set 提供了平均常数时间复杂度的操作性能，但是哈希冲突和重哈希操作可能会导致性能下降。 自定义类型：如果要在 unordered_set 中存储自定义类型的对象，需要定义相应的哈希函数和相等判断函数。 示例代码 下面的示例展示了如何使用 unordered_set 来存储和操作一组整数： 1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;unordered_set&gt;int main() &#123; std::unordered_set&lt;int&gt; mySet; // 插入元素 mySet.insert(1); mySet.insert(2); mySet.insert(3); mySet.insert(4); mySet.insert(1); // 重复插入，不会增加新元素 std::cout &lt;&lt; &quot;The set contains: &quot;; for (int num : mySet) &#123; std::cout &lt;&lt; num &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; // 查找元素 if (mySet.find(2) != mySet.end()) &#123; std::cout &lt;&lt; &quot;Element found&quot; &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; &quot;Element not found&quot; &lt;&lt; std::endl; &#125; // 删除元素 mySet.erase(2); std::cout &lt;&lt; &quot;Element 2 removed&quot; &lt;&lt; std::endl; // 显示集合大小 std::cout &lt;&lt; &quot;The set size is: &quot; &lt;&lt; mySet.size() &lt;&lt; std::endl; return 0;&#125; 这段代码首先创建了一个 unordered_set 的实例 mySet 并插入了几个整数。注意到尝试插入重复的元素（如数字 1）不会增加新元素。接着，代码演示了如何查找、删除元素和获取集合的大小。"},{"title":"0-1背包","path":"/wiki/OI-Knowledge/动态规划/背包问题.html","content":"0-1背包 题目描述 有nnn个物品，第iii个物体的体积为w[i]w[i]w[i]，价值为v[i]v[i]v[i]，每个物品至多选一个，求体积和不超过capacitycapacitycapacity时的最大价值和。 解决思路 后续代码解决该例题：P1048 [NOIP2005 普及组] 采药 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 解法1：记忆化搜索 首先考虑最直观的想法，就是使用记忆化搜索，dfs(i,c)dfs(i,c)dfs(i,c)表示的是当剩余容量为c时，从前i个物品中得到的最大价值。 需要解决的子问题就是当剩余c的容量时，第i个物品选还是不选。当前状态可以由如下方程转移（含义是选还是不选第i个物品）： dfs(i,c)=max⁡(dfs(i−1,c),dfs(i−1,c−w[i])+v[i])dfs(i,c)=\\max (dfs(i-1, c),dfs(i-1, c-w[i])+v[i]) dfs(i,c)=max(dfs(i−1,c),dfs(i−1,c−w[i])+v[i]) 实现代码如下： 1234567891011121314151617class Solution &#123;public: int solve(vector&lt;int&gt;&amp; times, vector&lt;int&gt;&amp; vals, int T)&#123; int n = times.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(T+1,-1)); function&lt;int(int,int)&gt; dfs = [&amp;](int i, int c) &#123; if (i &lt; 0) return 0; if (dp[i][c] &gt;= 0) return dp[i][c]; dp[i][c] = dfs(i - 1, c); if (times[i] &gt; c) return dp[i][c]; dp[i][c] = max(dfs(i-1,c-times[i])+vals[i], dp[i][c]); return dp[i][c]; &#125;; return dfs(n-1,T); &#125;&#125;; 解法2：递推 知道了记忆化搜索的解决方法，那么递推也非常好实现。 1234567891011121314class Solution &#123;public: int solve(vector&lt;int&gt;&amp; times, vector&lt;int&gt;&amp; vals, int T)&#123; int n = times.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(T+1,0)); for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;=T;++j)&#123; if (j &lt; times[i]) dp[i+1][j] = dp[i][j]; else dp[i+1][j] = max(dp[i][j],dp[i][j-times[i]]+vals[i]); &#125; &#125; return dp[n][T]; &#125;&#125;; 解法3：滚动数组 当然还可以使用滚动数组对空间复杂度进一步优化： 注意：第二层循环是倒着循环的，确保每个物品最多被选1次 1234567891011121314class Solution &#123;public: int solve(vector&lt;int&gt;&amp; times, vector&lt;int&gt;&amp; vals, int T)&#123; int n = times.size();// vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(T+1,0)); vector&lt;int&gt; dp(T+1, 0); for(int i=0;i&lt;n;++i)&#123; for(int j=T;j&gt;=0;--j)&#123; if (j &gt;= times[i]) dp[j] = max(dp[j],dp[j-times[i]]+vals[i]); &#125; &#125; return dp[T]; &#125;&#125;; 完全背包 题目描述 有nnn个物品，第iii个物体的体积为w[i]w[i]w[i]，价值为v[i]v[i]v[i]，每个物品可以无限次重复选择，求体积和不超过capacitycapacitycapacity时的最大价值和。 解决思路 后续代码解决该例题：P1616 疯狂的采药 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 解法1：记忆化搜索 相比之前的公式，有一点小变化,就是在选择当前物品的dp数组上i-1变为i dfs(i,c)=max⁡(dfs(i−1,c),dfs(i,c−w[i])+v[i])dfs(i,c)=\\max (dfs(i-1, c),dfs(i, c-w[i])+v[i]) dfs(i,c)=max(dfs(i−1,c),dfs(i,c−w[i])+v[i]) 该变化表明，在选择了第i件物品后，你还可以继续选择第i件物品。 代码也仅有一点小变化，实现代码如下： 123456789101112131415class Solution &#123; public: int solve(vector&lt;int&gt;&amp; times, vector&lt;int&gt;&amp; vals, int T)&#123; int n = times.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(T+1,-1)); function&lt;int(int,int)&gt; dfs = [&amp;](int i, int c) &#123; if (i &lt; 0) return 0; if (dp[i][c] &gt;= 0) return dp[i][c]; dp[i][c] = dfs(i - 1, c); if (times[i] &gt; c) return dp[i][c]; dp[i][c] = max(dfs(i,c-times[i])+vals[i], dp[i][c]); return dp[i][c]; &#125;; return dfs(n-1,T); &#125;&#125;; 解法2：递推 同理，一点小变化： 1234567891011121314class Solution &#123;public: int solve(vector&lt;int&gt;&amp; times, vector&lt;int&gt;&amp; vals, int T)&#123; int n = times.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(T+1,0)); for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;=T;++j)&#123; if (j &lt; times[i]) dp[i+1][j] = dp[i][j]; else dp[i+1][j] = max(dp[i][j],dp[i+1][j-times[i]]+vals[i]); &#125; &#125; return dp[n][T]; &#125;&#125;; 解法3：滚动数组 同理也可以使用滚动数组对空间复杂度进一步优化： 注意：此时第二层循环是正着循环的，确保每个物品能够被多次选择 1234567891011121314class Solution &#123;public: long long solve(vector&lt;long long&gt;&amp; times, vector&lt;long long&gt;&amp; vals, long long T)&#123; long long n = times.size();// vector&lt;vector&lt;long long&gt;&gt; dp(n+1, vector&lt;long long&gt;(T+1,0)); vector&lt;long long&gt; dp(T+1, 0); for(long long i=0;i&lt;n;++i)&#123; for(long long j=0;j&lt;=T;++j)&#123; if (j &gt;= times[i]) dp[j] = max(dp[j],dp[j-times[i]]+vals[i]); &#125; &#125; return dp[T]; &#125;&#125;; 背包变体 上面我们展现了两种常见背包的模板问题以及他们的解决方法，下面我们讨论一下背包问题常见的三种变形，他们分别是： 至多装capacitycapacitycapacity，求方案数/最大价值和 恰好装capacitycapacitycapacity，求方案数/最大/最小价值和 至少装capacitycapacitycapacity，求方案数/最小价值和 上面展示的两种背包的板题都是基于第一种变形。 变形2 01背包 题目传送门：494. 目标和 - 力扣（LeetCode） 使用数学公式转换一下，可以发现此题是一个恰好得到n的方案数01背包变形。（最后边界条件和至多有些不同其他一样） 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int findTargetSumWays(vector&lt;int&gt;&amp; nums, int target) &#123; if (target &lt; 0) target = -target; int sum_nums = 0; for(auto num : nums) sum_nums += num; if ((sum_nums+target)%2) return 0; int nw_target = (sum_nums+target)/2; vector&lt;int&gt; dp(nw_target+1, 0); dp[0] = 1; for(auto num : nums)&#123; for(int i=nw_target;i&gt;=0;--i)&#123; if(i&gt;=num) dp[i] += dp[i-num]; &#125; &#125; return dp[nw_target]; &#125;&#125;; 完全背包 题目传送门：322. 零钱兑换 - 力扣（LeetCode） 同上，此题是一个恰好得到n的方案数完全背包变形，求的是最小。 最后实现代码如下： 12345678910111213class Solution &#123;public: int coinChange(vector&lt;int&gt;&amp; coins, int amount) &#123; vector&lt;int&gt; dp(amount+1,1e9+7); dp[0] = 0; for(auto coin : coins)&#123; for(int i=coin;i&lt;=amount;++i)&#123; dp[i] = min(dp[i],dp[i-coin]+1); &#125; &#125; return dp[amount]==1e9+7?-1:dp[amount]; &#125;&#125;; 变形3 01背包 题目传送门：2742. 给墙壁刷油漆 - 力扣（LeetCode） 这道题简单转换一下其实还是一个01背包问题，只不过最后需要保证付费刷墙的时间要大于等于免费刷墙的墙数。若有n面墙，则：免费刷墙数 = n - 付费刷墙数。 带入前面不等式得： 付费刷墙数 + 付费刷墙时间 &gt;= n 相当于每堵被我们选来付费刷的墙他的所需时间+1，最后需要保证总时间大于等于n的情况下开销尽可能小。 问题成功转变为一个至少变形的01背包问题 最后实现代码如下： 123456789101112131415class Solution &#123;public: int paintWalls(vector&lt;int&gt; &amp;cost, vector&lt;int&gt; &amp;time) &#123; int n = cost.size(); vector&lt;int&gt; dp(n + 1, INT_MAX / 2); // 防止加法溢出 dp[0] = 0; for (int i = 0; i &lt; n; i++) &#123; int nw_cost = cost[i], nw_time = time[i]+1; for(int j=n;j&gt;=0;--j)&#123; dp[j] = min(dp[j], dp[max(0, j-nw_time)]+nw_cost); &#125; &#125; return dp[n]; &#125;&#125;; 可能中间那个max有一点不好理解，此题为min/max模型，如果是方案数模型，则动态规划方程会变为（初始dp[0]=1）： dp[i]=dp[i]+dp[max⁡(0,i−c)]dp[i] = dp[i] + dp[\\max(0,i-c)] dp[i]=dp[i]+dp[max(0,i−c)] 可以想想最极限的情况，就是至少为0的情况下，在该情况下，所有样本都可以选或者不选，只有循环到0并且按照上面这样写才能对至少为0的情况的方案数进行正确的更新。"},{"title":"快速幂","path":"/wiki/OI-Knowledge/数学/快速幂.html","content":"算法简介 在计算数学中，快速幂（Exponentiation by Squaring）是一种高效的计算大整数幂的方法。它通过将指数分解为二进制形式，从而减少了乘法运算的次数，显著提高了计算效率。快速幂广泛应用于密码学、计算机图形学和科学计算等领域。 快速幂算法的原理 快速幂算法的基本思想是利用指数的二进制表示，将幂运算转化为一系列的平方和乘积运算。具体步骤如下： 二进制分解：将指数以二进制形式表示。例如，指数13的二进制表示为1101。 平方和乘积：根据二进制位的值，将幂运算转化为一系列的平方和乘积运算。具体来说，如果当前二进制位为1，则将当前结果乘以基数的相应幂次。 迭代计算：通过迭代计算每一位的平方和乘积，最终得到结果。 快速幂算法的实现 快速幂算法可以通过递归和迭代两种方式实现。以下是C++的递归和迭代实现代码： 递归实现 12345678910111213141516171819#include &lt;iostream&gt;long long recursive_pow(long long base, long long exp, long long mod) &#123; if (exp == 0) return 1; long long half = recursive_pow(base, exp / 2, mod); half = (half * half) % mod; if (exp % 2 != 0) &#123; half = (half * base) % mod; &#125; return half;&#125;int main() &#123; long long base = 2; long long exp = 13; long long mod = 1000000007; std::cout &lt;&lt; &quot;Result: &quot; &lt;&lt; recursive_pow(base, exp, mod) &lt;&lt; std::endl; return 0;&#125; 迭代实现 123456789101112131415161718192021#include &lt;iostream&gt;long long iterative_pow(long long base, long long exp, long long mod) &#123; long long result = 1; while (exp &gt; 0) &#123; if (exp % 2 != 0) &#123; result = (result * base) % mod; &#125; base = (base * base) % mod; exp /= 2; &#125; return result;&#125;int main() &#123; long long base = 2; long long exp = 13; long long mod = 1000000007; std::cout &lt;&lt; &quot;Result: &quot; &lt;&lt; iterative_pow(base, exp, mod) &lt;&lt; std::endl; return 0;&#125; 快速幂算法的优势 效率高：快速幂算法通过将指数分解为二进制，减少了乘法运算的次数，其时间复杂度为O(log n)。 适用性广：快速幂算法适用于大整数的幂运算，尤其在模运算场景下表现优异，如密码学中的RSA算法。 易于实现：算法简单明了，既可以通过递归实现，也可以通过迭代实现，便于理解和应用。 应用场景 快速幂算法广泛应用于计算科学和工程领域，如： 密码学：在RSA加密和解密过程中，需要进行大量的大整数幂运算，快速幂算法显著提高了计算效率。 计算机图形学：在图形变换和投影计算中，需要频繁进行幂运算。 科学计算：在数值分析和模拟计算中，快速幂算法用于高效计算大数值的幂次。 总结 快速幂算法是一种高效计算大整数幂的方法，通过二进制分解指数，减少了乘法运算的次数。其递归和迭代两种实现方式简单易懂，适用于广泛的应用场景。掌握快速幂算法不仅能提高计算效率，还能在实际应用中解决大量的幂运算问题。"},{"title":"Trie树","path":"/wiki/OI-Knowledge/字符串/Trie树.html","content":"在计算机科学中，字典树（Trie树），是一种用于快速检索字符串的数据结构。字典树特别适用于词典、前缀匹配和自动补全等场景，可以在 O(m) 时间内完成插入和查询操作（其中 m 为字符串的长度）。 基本概念 字典树的核心思想是通过多叉树结构来存储字符串，每个节点代表一个字符。其主要功能包括： 插入操作：将一个字符串插入到字典树中。 查询操作：判断某个字符串是否存在于字典树中。 前缀匹配：查找具有共同前缀的所有字符串。 字典树的结构 字典树由一系列节点组成，每个节点包含以下信息： 一个长度为 26 的子节点数组（假设处理小写英文字母）。 一个布尔值标记，表示是否是一个完整字符串的结尾。 以下是字典树节点的定义： 1234567891011struct TrieNode &#123; TrieNode* children[26]; bool isEndOfWord; TrieNode() &#123; for (int i = 0; i &lt; 26; i++) &#123; children[i] = nullptr; &#125; isEndOfWord = false; &#125;&#125;; 字典树的操作 插入操作 插入操作用于将一个字符串插入到字典树中，逐字符插入，并在字符串结尾处标记： 1234567891011void insert(TrieNode* root, const string&amp; key) &#123; TrieNode* node = root; for (char c : key) &#123; int index = c - &#x27;a&#x27;; if (node-&gt;children[index] == nullptr) &#123; node-&gt;children[index] = new TrieNode(); &#125; node = node-&gt;children[index]; &#125; node-&gt;isEndOfWord = true;&#125; 查询操作 查询操作用于判断某个字符串是否存在于字典树中： 1234567891011bool search(TrieNode* root, const string&amp; key) &#123; TrieNode* node = root; for (char c : key) &#123; int index = c - &#x27;a&#x27;; if (node-&gt;children[index] == nullptr) &#123; return false; &#125; node = node-&gt;children[index]; &#125; return node != nullptr &amp;&amp; node-&gt;isEndOfWord;&#125; 前缀匹配 前缀匹配用于查找具有共同前缀的所有字符串： 1234567891011bool startsWith(TrieNode* root, const string&amp; prefix) &#123; TrieNode* node = root; for (char c : prefix) &#123; int index = c - &#x27;a&#x27;; if (node-&gt;children[index] == nullptr) &#123; return false; &#125; node = node-&gt;children[index]; &#125; return true;&#125; 示例 假设我们有一个单词列表 [&quot;apple&quot;, &quot;app&quot;, &quot;application&quot;, &quot;bat&quot;, &quot;ball&quot;, &quot;cat&quot;]，我们可以使用字典树来进行快速的插入和查询操作。 以下是示例代码： 1234567891011121314151617int main() &#123; TrieNode* root = new TrieNode(); vector&lt;string&gt; words = &#123;&quot;apple&quot;, &quot;app&quot;, &quot;application&quot;, &quot;bat&quot;, &quot;ball&quot;, &quot;cat&quot;&#125;; for (const string&amp; word : words) &#123; insert(root, word); &#125; cout &lt;&lt; search(root, &quot;app&quot;) &lt;&lt; endl; // 输出 1（true） cout &lt;&lt; search(root, &quot;application&quot;) &lt;&lt; endl; // 输出 1（true） cout &lt;&lt; search(root, &quot;appl&quot;) &lt;&lt; endl; // 输出 0（false） cout &lt;&lt; startsWith(root, &quot;app&quot;) &lt;&lt; endl; // 输出 1（true） cout &lt;&lt; startsWith(root, &quot;bat&quot;) &lt;&lt; endl; // 输出 1（true） cout &lt;&lt; startsWith(root, &quot;baller&quot;) &lt;&lt; endl; // 输出 0（false） return 0;&#125; 例题1：3045. 统计前后缀下标对 II 题目传送门：3045. 统计前后缀下标对 II - 力扣（LeetCode） LeetCode第 385 场周赛的最后一题，比较板子，下面我们来分析一下，相比字典树传统解决的前缀匹配问题，这里题目还要求我们对后缀进行匹配。这里运用一个小trick就可以将这个问题转化为前缀匹配问题。就是我们把字符串的第n位变化成第n位和倒数第n位的组合，这样我们就可以只进行前缀匹配了（仔细思考一下就能明白为啥） 最后实现代码如下： 12345678910111213141516171819202122232425struct TrieNode&#123; unordered_map&lt;int, TrieNode*&gt; son; int cnt = 0;&#125;;class Solution &#123;public: long long countPrefixSuffixPairs(vector&lt;string&gt; &amp;words) &#123; long long ans = 0; TrieNode *root = new TrieNode(); for (string &amp;s : words) &#123; int n =s.size(); TrieNode *cur = root; for(int i=0;i&lt;n;++i)&#123; // 创建树链 int p = (int)(s[i]-&#x27;a&#x27;)&lt;&lt;5 | (s[n - 1 - i] - &#x27;a&#x27;); if (cur-&gt;son[p]==nullptr) &#123; cur-&gt;son[p] = new TrieNode(); &#125; cur = cur-&gt;son[p]; ans += cur-&gt;cnt; // 创建的过程中更新答案 &#125; cur-&gt;cnt++; &#125; return ans; &#125;&#125;; 总结 字典树是一种高效且易于实现的数据结构，特别适用于字符串的插入和查询操作。通过多叉树结构，字典树能够在 O(m) 的时间复杂度内完成操作，极大地提高了性能。"},{"title":"质数筛法","path":"/wiki/OI-Knowledge/数学/质数筛法.html","content":"本文以P5736 【深基7.例2】质数筛 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn)为例，介绍一下两种常用的质数筛法：埃氏筛与欧拉筛 埃氏筛 埃氏筛（Sieve of Eratosthenes）是一种古老而高效的算法，用于找出一定范围内的所有质数。质数是指除了1和其本身外，不能被其他任何数整除的自然数。埃氏筛算法的名字来源于古希腊数学家埃拉托色尼，他在公元前3世纪发明了这个算法。 埃氏筛算法的原理 埃氏筛的基本思想是从2开始，将每个质数的倍数标记为非质数，剩下的未标记的数就是质数。具体步骤如下： 初始化一个数组：假设我们要找出小于等于n的所有质数，先创建一个大小为n+1的布尔数组isPrime，并将所有元素初始化为true，表示这些数都是潜在的质数。然后将isPrime[0]和isPrime[1]设置为false，因为0和1不是质数。 从2开始：从第一个质数2开始，标记所有2的倍数（从4开始，间隔为2的数）为非质数，即将isPrime[4], isPrime[6], isPrime[8]等设置为false。 找到下一个未标记的数：找到数组中下一个值为true的数，它是下一个质数。然后标记这个数的所有倍数为非质数。 重复步骤3：继续这个过程，直到处理到数组的平方根位置。 埃氏筛算法的时间复杂度 埃氏筛算法的时间复杂度为O(n log log n)。这是因为在标记过程中，每个数的倍数仅被标记一次。相较于直接判断每个数是否为质数的O(n√n)时间复杂度，埃氏筛在处理大规模数据时具有显著的效率优势。 埃氏筛算法的实现 以下是Python实现埃氏筛算法的代码： 123456789101112131415def sieve_of_eratosthenes(n): is_prime = [True] * (n + 1) is_prime[0], is_prime[1] = False, False p = 2 while (p * p &lt;= n): if is_prime[p]: for i in range(p * p, n + 1, p): is_prime[i] = False p += 1 prime_numbers = [p for p in range(n + 1) if is_prime[p]] return prime_numbers# 示例n = 30print(f&quot;小于等于 &#123;n&#125; 的质数有: &#123;sieve_of_eratosthenes(n)&#125;&quot;) C++实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;vector&gt;// 埃氏筛算法函数std::vector&lt;int&gt; sieve_of_eratosthenes(int n) &#123; std::vector&lt;bool&gt; is_prime(n + 1, true); is_prime[0] = is_prime[1] = false; // 0和1不是质数 for (int p = 2; p * p &lt;= n; ++p) &#123; if (is_prime[p]) &#123; // 将p的所有倍数标记为非质数 for (int i = p * p; i &lt;= n; i += p) &#123; is_prime[i] = false; &#125; &#125; &#125; // 收集所有的质数 std::vector&lt;int&gt; prime_numbers; for (int p = 2; p &lt;= n; ++p) &#123; if (is_prime[p]) &#123; prime_numbers.push_back(p); &#125; &#125; return prime_numbers;&#125;int main() &#123; int n = 30; std::vector&lt;int&gt; primes = sieve_of_eratosthenes(n); std::cout &lt;&lt; &quot;小于等于 &quot; &lt;&lt; n &lt;&lt; &quot; 的质数有: &quot;; for (int prime : primes) &#123; std::cout &lt;&lt; prime &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 上述例题AC代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;bits/stdc++.h&gt;using namespace std;// 埃氏筛算法函数vector&lt;int&gt; sieve_of_eratosthenes(int n) &#123; vector&lt;bool&gt; is_prime(n + 1, true); is_prime[0] = is_prime[1] = false; // 0和1不是质数 for (int p = 2; p * p &lt;= n; ++p) &#123; if (is_prime[p]) &#123; // 将p的所有倍数标记为非质数 for (int i = p * p; i &lt;= n; i += p) &#123; is_prime[i] = false; &#125; &#125; &#125; // 收集所有的质数 vector&lt;int&gt; prime_numbers; for (int p = 2; p &lt;= n; ++p) &#123; if (is_prime[p]) &#123; prime_numbers.push_back(p); &#125; &#125; return prime_numbers;&#125;int main() &#123; int N = 100000,n; vector&lt;int&gt; primes = sieve_of_eratosthenes(N); set&lt;int&gt; primes_set(primes.begin(), primes.end()); scanf(&quot;%d&quot;, &amp;n); for(int i=1;i&lt;=n;++i)&#123; int a; cin &gt;&gt; a; if (primes_set.find(a)!=primes_set.end()) cout &lt;&lt; a &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; return 0;&#125; 应用与优势 埃氏筛不仅在理论上具有重要意义，在实际应用中也非常广泛。它不仅适用于编程竞赛中寻找质数的问题，也可以用于加密算法（如RSA算法）中的大质数生成。由于其高效性，埃氏筛在处理大范围数据时表现出色。 总之，埃氏筛是一种简单而高效的质数筛选算法，通过巧妙地标记非质数，显著减少了判断质数的计算量，是计算数学中的一个经典算法。 欧拉筛 欧拉筛（Sieve of Euler）是一种用于寻找一定范围内所有质数的高效算法，是埃氏筛的优化版本。欧拉筛在标记合数时，避免了重复标记，使得其时间复杂度和空间复杂度都优于埃氏筛。 欧拉筛算法的原理 欧拉筛的基本思想是通过线性筛选的方法，在标记合数时，每个合数只会被其最小的质因数标记一次，从而避免了重复标记。具体步骤如下： 初始化数组：创建一个大小为n+1的布尔数组isPrime，所有元素初始化为true，表示这些数都是潜在的质数。同时，创建一个空列表primes，用于存储找到的质数。 从2开始筛选：从2开始，依次检查每个数是否为质数。如果是质数，将其加入primes列表中。 标记合数：对每一个质数p，标记从p开始的所有p的倍数为非质数。需要注意的是，对于每个合数，只用其最小的质因数来标记，这样可以避免重复标记。 继续筛选：重复上述过程，直到遍历到n。 欧拉筛算法的时间复杂度 欧拉筛的时间复杂度为O(n)，比埃氏筛的O(n log log n)更为高效。这是因为欧拉筛在标记过程中，每个合数只会被标记一次，不会像埃氏筛那样存在重复标记的情况。 欧拉筛算法的实现 以下是Python实现欧拉筛算法的代码： 1234567891011121314151617def sieve_of_euler(n): is_prime = [True] * (n + 1) primes = [] for i in range(2, n + 1): if is_prime[i]: primes.append(i) for p in primes: if i * p &gt; n: break is_prime[i * p] = False if i % p == 0: break return primes# 示例n = 30print(f&quot;小于等于 &#123;n&#125; 的质数有: &#123;sieve_of_euler(n)&#125;&quot;) C++实现代码如下： 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;vector&gt;std::vector&lt;int&gt; sieve_of_euler(int n) &#123; std::vector&lt;bool&gt; is_prime(n + 1, true); std::vector&lt;int&gt; primes; for (int i = 2; i &lt;= n; ++i) &#123; if (is_prime[i]) &#123; primes.push_back(i); &#125; for (int p : primes) &#123; if (i * p &gt; n) break; is_prime[i * p] = false; if (i % p == 0) break; &#125; &#125; return primes;&#125;int main() &#123; int n = 30; std::vector&lt;int&gt; primes = sieve_of_euler(n); std::cout &lt;&lt; &quot;小于等于 &quot; &lt;&lt; n &lt;&lt; &quot; 的质数有: &quot;; for (int prime : primes) &#123; std::cout &lt;&lt; prime &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; return 0;&#125; 上述例题AC代码如下： 12345678910111213141516171819202122232425262728293031323334#include &lt;bits/stdc++.h&gt;using namespace std;// 欧拉筛算法函数std::vector&lt;int&gt; sieve_of_euler(int n) &#123; std::vector&lt;bool&gt; is_prime(n + 1, true); std::vector&lt;int&gt; primes; for (int i = 2; i &lt;= n; ++i) &#123; if (is_prime[i]) &#123; primes.push_back(i); &#125; for (int p : primes) &#123; if (i * p &gt; n) break; is_prime[i * p] = false; if (i % p == 0) break; &#125; &#125; return primes;&#125;int main() &#123; int N = 100000,n; vector&lt;int&gt; primes = sieve_of_euler(N); set&lt;int&gt; primes_set(primes.begin(), primes.end()); scanf(&quot;%d&quot;, &amp;n); for(int i=1;i&lt;=n;++i)&#123; int a; cin &gt;&gt; a; if (primes_set.find(a)!=primes_set.end()) cout &lt;&lt; a &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; return 0;&#125; 应用与优势 欧拉筛在寻找质数的应用中表现出色，特别是在处理大规模数据时，其线性时间复杂度显著提升了算法的效率。与埃氏筛相比，欧拉筛通过减少重复标记的操作，优化了空间和时间的使用，是质数筛选算法中的一种重要改进。 总的来说，欧拉筛是一种高效且优化的质数筛选算法，通过避免重复标记合数，显著提升了筛选速度和空间利用率，是计算数学中不可或缺的工具。无论是在理论研究还是实际应用中，欧拉筛都展现出了其独特的优势。 总结与对比 埃氏筛 原理：从2开始，将每个质数的倍数标记为非质数，剩下的即为质数。 时间复杂度：O(n log log n)。 空间复杂度：O(n)。 优点：实现简单，适合初学者。 欧拉筛 原理：线性筛选，每个合数只会被其最小质因数标记一次，避免重复标记。 时间复杂度：O(n)。 空间复杂度：O(n)。 优点：效率更高，适合处理大规模数据。 对比 效率：欧拉筛更高效，时间复杂度为O(n)，适合大规模数据；埃氏筛为O(n log log n)。 标记过程：欧拉筛避免了重复标记，标记过程更优化。 实现复杂度：埃氏筛简单直观，欧拉筛稍复杂但更高效。 总结 埃氏筛和欧拉筛各有优势，埃氏筛实现简单，适合初学者和处理较小规模的数据。而欧拉筛在效率上更胜一筹，适合处理大规模的数据，在高效性上表现尤为突出。选择哪种算法取决于具体应用场景和数据规模。在学习和研究过程中，可以从埃氏筛入手，逐步理解和实现欧拉筛，深入掌握质数筛选算法的优化和高效实现。"},{"title":"树状数组","path":"/wiki/OI-Knowledge/高级数据结构/树状数组.html","content":"在计算机科学中，树状数组（Binary Indexed Tree，BIT），也称为Fenwick Tree，是一种用于高效处理前缀和查询和更新操作的数据结构。树状数组非常适合解决动态数组的前缀和问题，可以在O(log n)时间内完成更新和查询操作。 基本概念 树状数组的核心思想是通过维护一组累加和来实现快速的前缀和查询和更新。其主要功能包括： 前缀和查询（区间查询）：在给定数组中快速求出前缀和。 单点更新：在给定数组中快速更新某个位置的值。 树状数组的结构 树状数组使用一个辅助数组 BIT 来维护信息。对于一个长度为 n 的数组，我们构建一个长度为 n+1 的 BIT 数组，其中 BIT[i] 表示从某个起始点到位置 i 的部分和。具体来说，BIT 的索引和原数组的索引之间有一定的映射关系，这种关系通过位运算来确定。 注意树状数组的下标从1开始(同线段树root=1) 树状数组的操作 构建树状数组 初始化树状数组需要遍历原数组，并调用更新操作来填充 BIT 数组： 123456void buildBIT(vector&lt;int&gt;&amp; arr, vector&lt;int&gt;&amp; BIT) &#123; int n = arr.size(); for (int i = 0; i &lt; n; i++) &#123; updateBIT(BIT, n, i, arr[i]); &#125;&#125; 更新操作 更新操作用于在数组的某个位置添加一个值，并相应地更新 BIT 数组： 1234567void updateBIT(vector&lt;int&gt;&amp; BIT, int n, int index, int val) &#123; index += 1; // BIT array is 1-indexed while (index &lt;= n) &#123; BIT[index] += val; index += index &amp; (-index); &#125;&#125; 查询操作 查询操作用于计算数组中从起始位置到某个位置的前缀和： 123456789int queryBIT(vector&lt;int&gt;&amp; BIT, int index) &#123; int sum = 0; index += 1; // BIT array is 1-indexed while (index &gt; 0) &#123; sum += BIT[index]; index -= index &amp; (-index); &#125; return sum;&#125; 示例 假设我们有一个数组 arr 为 [3, 2, -1, 6, 5, 4, -3, 3, 7, 2, 3]，我们可以使用树状数组来进行快速的前缀和查询和单点更新。 正如之前所说，树状数组广泛应用于单点修改，区间查询的题目中 例题1：【模板】树状数组1 题目传送门：P3374 【模板】树状数组 1 - 洛谷 | 计算机科学教育新生态(luogu.com.cn) 树状数组单点修改+区间查询模板题，实现代码如下： %% 不同于一般使用树状数组所解决的单点修改+区间查询问题，此题需要我们解决的是区间修改+单点查询问题。简单分析不难发现通过把原来的队列转化为一个差分序列，就可以将区间修改变为单点修改，单点查询变为区间查询。 %% 实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/********************************************************************************************************************** * Problem: P3374 【模板】树状数组 1 * URL: https://www.luogu.com.cn/problem/P3374 * Description: Binary Indexed Tree * Created by Vegetabot on 6/8/2024.**********************************************************************************************************************/#include&lt;bits/stdc++.h&gt;#define N 500003using namespace std;int lowbit(int x) &#123;return x &amp; (-x);&#125;int n, m;vector&lt;int&gt; arr(N), bit(N);void updateBIT(int index, int val) &#123;// index += 1; // BIT array is 1-indexed while (index &lt;= n) &#123; bit[index] += val; index += lowbit(index); &#125;&#125;int queryBIT(int index) &#123; int sum = 0;// index += 1; // BIT array is 1-indexed while (index &gt; 0) &#123; sum += bit[index]; index -= lowbit(index); &#125; return sum;&#125;void buildBIT() &#123; for (int i = 1; i &lt;= n; i++) &#123; updateBIT(i, arr[i]); &#125;&#125;int main() &#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for(int i=1;i&lt;=n;++i)&#123; scanf(&quot;%d&quot;, &amp;arr[i]); &#125; buildBIT(); for(int i=1; i&lt;=m; ++i) &#123; int op, a, b; cin &gt;&gt; op &gt;&gt; a &gt;&gt; b; if (op == 1) &#123; updateBIT(a, b); &#125; else &#123; cout &lt;&lt; queryBIT(b) - queryBIT(a-1) &lt;&lt; endl; &#125; &#125; return 0;&#125; 总结 树状数组是一种高效且易于实现的数据结构，特别适用于动态数组的前缀和问题。通过巧妙的位运算，树状数组能够在 O(log n) 的时间复杂度内完成更新和查询操作，极大地提高了性能。"},{"title":"线段树","path":"/wiki/OI-Knowledge/高级数据结构/线段树.html","content":"线段树是一种用于存储区间或段的树形数据结构，使得在其上进行快速的查询和修改操作成为可能。它常用于处理涉及区间的查询问题，如求区间和、区间最小值、区间最大值等。 基本概念 线段树是一棵二叉树，主要用于高效地解决一类区间查询问题。其每个节点表示一个区间，叶节点表示数组中的单个元素，内部节点表示其子节点区间的并集。 线段树的结构 叶节点：叶节点对应数组的单个元素。 内部节点：每个内部节点表示其子节点所代表的区间的并集。 例如，给定数组 [1, 3, 5, 7, 9, 11]，其线段树的结构如下： 1234567 [0, 5] / \\ [0, 2] [3, 5] / \\ / \\ [0, 1] [2, 2] [3, 4] [5, 5] / \\ / \\[0, 0] [1, 1] [3, 3] [4, 4] 构建线段树 线段树的构建时间复杂度为 O(n)，其中 n 是数组的长度。构建过程如下： 递归分割区间：从数组的中间分割，将数组分为左右两个区间。 构建子树：对左右区间分别构建线段树。 计算节点值：内部节点的值由其左右子节点的值计算得到。 一般建树代码如下所示： 12345678910111213void buildTree(LL k, LL l, LL r)&#123; if (l == r) &#123; node[k].val = arr[l]; node[k].left = l; node[k].right = r; return; &#125; node[k].left = l; node[k].right = r; LL mid = l + (r - l) / 2; buildTree(k*2, l, mid); buildTree(k*2+1, mid+1, r); node[k].val = node[k*2].val + node[k*2+1].val;&#125; 线段树的操作 单点/区间查询：查询某一范围内的数据，如区间和、区间最小值等。 单点/区间更新：更新数组中的某个/区间元素，并更新相关区间的信息。 一般在线段树中会实现如下函数： query(left, right) 方法用于查询 [left, right] 区间的和。 update(index, value) 方法用于更新数组中 index 位置的值，并更新相关区间的信息。 pushDown(k)用于下放节点k上的lazyTag 上述代码实现见下面应用部分的例题。 线段树的应用 线段树广泛应用于解决以下问题： 区间求和 区间最小值/最大值查询 区间更新操作 动态顺序统计 通过线段树，可以在 O(log n) 时间复杂度内完成区间查询和更新操作，使其在处理动态数组问题时非常高效。 例题1：【模板】线段树1 题目传送门：P3372 【模板】线段树 1 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 线段树区间修改+区间查询模板题，实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/********************************************************************************************************************** * Problem: P3372 【模板】线段树 1 * URL: https://www.luogu.com.cn/problem/P3372 * Description: Segment Tree（区间修改，区间查询） * Created by Vegetabot on 6/7/2024.**********************************************************************************************************************/#include&lt;bits/stdc++.h&gt;#define N 100005#define LL long longusing namespace std;LL n,m,root=1;vector&lt;LL&gt; arr(N);struct TreeNode&#123; LL val, add, left, right; TreeNode() : val(0), add(0), left(-1), right(-1) &#123;&#125;&#125;;vector&lt;TreeNode&gt; node(N*3);void buildTree(LL k, LL l, LL r)&#123; if (l == r) &#123; node[k].val = arr[l]; node[k].left = l; node[k].right = r; return; &#125; node[k].left = l; node[k].right = r; LL mid = l + (r - l) / 2; buildTree(k*2, l, mid); buildTree(k*2+1, mid+1, r); node[k].val = node[k*2].val + node[k*2+1].val;&#125;void addTag(LL k, LL val)&#123; node[k].val += val * (node[k].right - node[k].left + 1); node[k].add += val;&#125;void pushDown(LL k)&#123; addTag(k*2, node[k].add); addTag(k*2+1, node[k].add); node[k].add = 0;&#125;void modify(LL k, LL l, LL r, LL val)&#123; if (l &lt;= node[k].left &amp;&amp; node[k].right &lt;= r) &#123; addTag(k, val); return; &#125; pushDown(k); LL mid = node[k].left + (node[k].right - node[k].left)/2; if (l&lt;=mid) modify(k*2, l, r, val); if (r&gt;mid) modify(k*2+1, l, r, val); node[k].val = node[k*2].val + node[k*2+1].val;&#125;LL query(LL k, LL l, LL r)&#123; if (l &lt;= node[k].left &amp;&amp; node[k].right &lt;= r) &#123; return node[k].val; &#125; pushDown(k); LL mid = node[k].left + (node[k].right - node[k].left)/2, res = 0; if (l &lt;= mid) res += query(k*2, l, r); if (r &gt; mid) res += query(k*2+1, l, r); return res;&#125;int main() &#123; cin &gt;&gt; n &gt;&gt; m; for(int i=1; i&lt;=n; ++i)&#123; scanf(&quot;%lld&quot;, &amp;arr[i]); &#125; buildTree(root, 1, n); for(int i=1; i&lt;=m; ++i) &#123; LL op, a, b, c; scanf(&quot;%lld&quot;, &amp;op); if (op == 1) &#123; scanf(&quot;%lld%lld%lld&quot;, &amp;a, &amp;b, &amp;c); modify(root, a, b, c); &#125; else &#123; scanf(&quot;%lld%lld&quot;, &amp;a, &amp;b); printf(&quot;%lld &quot;, query(root, a, b)); &#125; &#125; return 0;&#125; 例题2：【模板】线段树2 题目传送门：P3373 【模板】线段树 2 - 洛谷 | 计算机科学教育新生态 (luogu.com.cn) 此题同样是线段树区间修改+区间查询模板题，相比上一道模板题，难度主要加在了如何打lazyTag上，本题涉及两种lazyTag（乘法，加法），解决这道题只需牢记下放lazyTag时严格遵循先乘后加即可。 实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/********************************************************************************************************************** * Problem: P3373 【模板】线段树 2 * URL: https://www.luogu.com.cn/problem/P3373 * Description: Segment Tree * Created by Vegetabot on 6/8/2024.**********************************************************************************************************************/#include&lt;bits/stdc++.h&gt;#define N 100003#define LL long long using namespace std;LL n, q, m, root=1;vector&lt;LL&gt; arr(N);struct TreeNode&#123; LL val, add, multi, left, right; TreeNode(): val(0), add(0), multi(1), left(-1), right(-1) &#123;&#125;&#125;;vector&lt;TreeNode&gt; node(N*3);void buildTree(LL k, LL l, LL r)&#123; node[k].left = l; node[k].right = r; if (l == r) &#123; node[k].val = arr[l]; return; &#125; LL mid = l + (r - l) / 2; buildTree(k*2, l, mid); buildTree(k*2+1, mid+1, r); node[k].val = node[k*2].val + node[k*2+1].val;&#125;void addTag(LL k,LL val, LL op) &#123; if (op == 1)&#123; node[k].val = (node[k].val * val) % m; node[k].add = (node[k].add * val) % m; node[k].multi = (node[k].multi * val) % m; &#125; else &#123; node[k].val = (node[k].val + val*(node[k].right - node[k].left+1)) % m; node[k].add = (node[k].add + val) % m; &#125;&#125;void pushDown(LL k)&#123; addTag(k*2 ,node[k].multi, 1); addTag(k*2, node[k].add, 2); addTag(k*2+1, node[k].multi, 1); addTag(k*2+1, node[k].add, 2); node[k].multi = 1; node[k].add = 0;&#125;void modify(LL k, LL l, LL r, LL val ,LL op) &#123; if (l &lt;= node[k].left &amp;&amp; node[k].right &lt;= r) &#123; addTag(k, val, op); return; &#125; pushDown(k); LL mid = node[k].left + (node[k].right - node[k].left)/2; if (l &lt;= mid) modify(k*2, l, r, val, op); if (r &gt; mid) modify(k*2+1, l, r, val, op); node[k].val = node[k*2].val + node[k*2+1].val;&#125;LL query(LL k, LL l, LL r) &#123; if (l &lt;= node[k].left &amp;&amp; node[k].right &lt;= r) &#123; return node[k].val; &#125; pushDown(k); LL mid = node[k].left + (node[k].right - node[k].left)/2, res = 0; if (l &lt;= mid) res = (res + query(k*2, l, r)) % m; if (r &gt; mid) res = (res + query(k*2+1, l, r)) % m; return res;&#125;int main() &#123; scanf(&quot;%lld%lld%lld&quot;, &amp;n, &amp;q, &amp;m); for(LL i=1; i&lt;=n; ++i) &#123; cin &gt;&gt; arr[i]; &#125; buildTree(root, 1, n); for(LL i=1;i&lt;=q;++i)&#123; LL op, a, b, c; scanf(&quot;%lld&quot;, &amp;op); if (op == 1 || op == 2) &#123; scanf(&quot;%lld%lld%lld&quot;, &amp;a, &amp;b, &amp;c); modify(root, a, b, c%m, op); &#125; else &#123; scanf(&quot;%lld%lld&quot;, &amp;a, &amp;b); printf(&quot;%lld &quot;,query(root, a, b)); &#125; &#125; return 0;&#125; 注意事项 书写线段树的时候需要注意以下事项： buildTree和modify函数在递归完成后的回溯阶段需要更新上层的统计值 下放lazyTag所涉及的函数pushDown在query和modify中都是先调用再进行递归 加lazyTag标记以及区间更新函数addTag都是对当前区间进行操作（标记是加给儿子节点看的,方便后续pushDown下去对儿子区间进行更新） 总结 线段树是一种强大的数据结构，特别适合于解决区间查询和更新问题。理解线段树的构建和操作原理，并掌握其实现方法，可以帮助我们高效地解决许多复杂的数据处理问题。"},{"title":"🔧使用工具增强Chatbot","path":"/wiki/LLM/LangGraph/🔧使用工具增强Chatbot.html","content":"引言 在上一篇教程中，我们使用 LangGraph 构建了一个简单的 Chatbot。虽然这个 Chatbot 能够处理基本的对话，但在面对一些需要实时信息或超出其知识范围的问题时，它的表现可能会显得力不从心。例如，当用户询问“今天的天气如何？”或“最新的新闻是什么？”时，Chatbot 无法“凭记忆”回答这些问题。 为了解决这个问题，我们可以为 Chatbot 集成一个网络搜索工具。通过这个工具，Chatbot 可以实时查找相关信息，并提供更准确、更及时的响应。在本教程中，我们将使用 tavily-python 和 langchain_community 库来实现这一功能。 什么是网络搜索工具？ 网络搜索工具是一种能够通过 API 访问互联网资源并检索相关信息的工具。通过集成网络搜索工具，Chatbot 可以在对话过程中动态获取外部数据，从而增强其回答能力。例如，当用户询问某个特定话题的最新信息时，Chatbot 可以通过网络搜索工具查找相关内容，并将其整合到对话中。 在本教程中，我们将使用 tavily-python 库来实现网络搜索功能。tavily-python 是一个简单易用的 Python 库，能够快速检索网络上的相关信息。 安装依赖 在开始之前，我们需要安装一些额外的依赖库。你可以通过以下命令安装这些库： 1pip install -U tavily-python langchain_community tavily-python: 用于实现网络搜索功能。 langchain_community: 提供与 LangChain 相关的工具和集成。 安装完成后，我们就可以开始集成网络搜索工具了。 定义工具 在 LangGraph 中，工具（Tool）是一种可以扩展 Chatbot 功能的组件。我们可以通过定义工具来实现网络搜索功能。首先，我们需要创建一个工具类，并在其中实现搜索逻辑。 123456789101112from langchain_community.tools.tavily_search import TavilySearchResults# 设置搜索密钥import osimport getpassif not os.environ.get(&quot;TAVILY_API_KEY&quot;): os.environ[&quot;TAVILY_API_KEY&quot;] = getpass.getpass(&quot;Enter API key for Tavily: &quot;)tool = TavilySearchResults(max_results=2)tools = [tool]tool.invoke(&quot;What&#x27;s a &#x27;node&#x27; in LangGraph?&quot;) 在这个例子中，我们定义了一个 TavilySearchResults 工具，它接收一个查询字符串并返回搜索结果。max_results=2 表示每次搜索最多返回 2 个结果。 集成工具到 Chatbot 接下来，我们需要将这个工具集成到 Chatbot 中。我们可以通过修改 Chatbot 的逻辑，使其在遇到无法回答的问题时调用搜索工具。 12345678910from langchain_openai import ChatOpenAI# 设置LLM密钥if not os.environ.get(&quot;OPENAI_API_KEY&quot;): os.environ[&quot;OPENAI_API_KEY&quot;] = getpass.getpass(&quot;Enter API key for OpenAI: &quot;)llm = ChatOpenAI(model=&quot;gpt-4o-mini&quot;,base_url=&quot;https://api.chatanywhere.tech/v1&quot;)# 绑定工具到LLMllm_with_tools = llm.bind_tools(tools) 在这个例子中，我们使用 bind_tools 方法将工具绑定到语言模型（LLM）。这样，LLM 在生成响应时，可以根据需要调用工具来获取外部信息。 构建增强版对话图 现在，我们可以使用增强版的 Chatbot 函数来构建一个新的对话图。 1234567891011from langgraph.graph import StateGraph, START, ENDfrom langgraph.graph.message import add_messagesclass State(TypedDict): messages: Annotated[list, add_messages]def chatbot(state: State): return &#123;&quot;messages&quot;: [llm_with_tools.invoke(state[&quot;messages&quot;])]&#125;graph_builder = StateGraph(State)graph_builder.add_node(&quot;chatbot&quot;, chatbot) 在这个例子中，我们创建了一个新的对话图，并使用 chatbot 函数作为节点。对话图的结构与之前的版本类似，但 Chatbot 的功能得到了增强。 添加工具节点 为了让 Chatbot 能够调用工具，我们需要添加一个工具节点。这个节点负责执行工具调用，并将结果返回给 Chatbot。 1234567891011121314151617181920212223242526import jsonfrom langchain_core.messages import ToolMessageclass BasicToolNode: def __init__(self, tools: list) -&gt; None: self.tools_by_name = &#123;tool.name: tool for tool in tools&#125; def __call__(self, inputs: dict): if messages := inputs.get(&quot;messages&quot;, []): message = messages[-1] else: raise ValueError(&quot;No message found in input&quot;) outputs = [] for tool_call in message.tool_calls: tool_result = self.tools_by_name[tool_call[&quot;name&quot;]].invoke(tool_call[&quot;args&quot;]) outputs.append( ToolMessage( content=json.dumps(tool_result), name=tool_call[&quot;name&quot;], tool_call_id=tool_call[&quot;id&quot;], ) ) return &#123;&quot;messages&quot;: outputs&#125;tool_node = BasicToolNode(tools=[tool])graph_builder.add_node(&quot;tools&quot;, tool_node) 在这个例子中，我们定义了一个 BasicToolNode 类，它负责执行工具调用并返回结果。然后，我们将这个工具节点添加到对话图中。 定义路由器函数 为了让 Chatbot 在需要时调用工具，我们需要定义一个路由器函数。这个函数会根据 Chatbot 的输出决定下一步是调用工具还是结束对话。 12345678910111213141516171819def route_tools(state: State): if isinstance(state, list): ai_message = state[-1] elif messages := state.get(&quot;messages&quot;, []): ai_message = messages[-1] else: raise ValueError(f&quot;No messages found in input state to tool_edge: &#123;state&#125;&quot;) if hasattr(ai_message, &quot;tool_calls&quot;) and len(ai_message.tool_calls) &gt; 0: return &quot;tools&quot; return ENDgraph_builder.add_conditional_edges( &quot;chatbot&quot;, route_tools, &#123;&quot;tools&quot;: &quot;tools&quot;, END: END&#125;,)graph_builder.add_edge(&quot;tools&quot;, &quot;chatbot&quot;)graph_builder.add_edge(START, &quot;chatbot&quot;)graph = graph_builder.compile() 在这个例子中，route_tools 函数会检查 Chatbot 的输出是否包含工具调用。如果有工具调用，则路由到工具节点；否则，结束对话。 根据上面建边情况，可以得到如下的路由图： 运行增强版 Chatbot 最后，我们可以运行增强版 Chatbot 并观察其行为。通过 graph.stream 方法，我们可以将用户输入传递给 Chatbot，并逐步生成对话。 12345678910111213141516171819def stream_graph_updates(user_input: str): for event in graph.stream(&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;): for value in event.values(): print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)while True: try: user_input = input(&quot;User: &quot;) if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]: print(&quot;Goodbye!&quot;) break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = &quot;What do you know about LangGraph?&quot; print(&quot;User: &quot; + user_input) stream_graph_updates(user_input) break 在这个例子中，我们创建了一个简单的交互式循环，用户可以输入消息，增强版 Chatbot 会生成响应。如果用户输入 &quot;quit&quot;、&quot;exit&quot; 或 &quot;q&quot;，程序将退出。 结论 通过本教程，我们学习了如何为 Chatbot 集成网络搜索工具，从而增强其回答能力。我们定义了搜索工具、修改了 Chatbot 的逻辑，并最终运行了增强版 Chatbot。这个增强版 Chatbot 能够处理更多类型的问题，并提供更准确、更及时的响应。 希望这篇教程对你理解如何增强 Chatbot 的功能有所帮助！如果你有任何问题或建议，欢迎在评论区留言。 作者: Mudrobot 日期: 2025.01.24 标签: LangGraph, Chatbot, 网络搜索, NLP, 自然语言处理"},{"title":"🤖构建简易聊天机器人","path":"/wiki/LLM/LangGraph/🤖构建简易聊天机器人.html","content":"引言 在人工智能和自然语言处理（NLP）领域，构建一个能够与用户进行自然对话的Chatbot是一个非常有趣且具有挑战性的任务。Chatbot的应用场景非常广泛，从客户服务到个人助手，再到教育工具，Chatbot正在改变我们与技术交互的方式。然而，构建一个高效且灵活的Chatbot并不容易，尤其是当我们需要处理复杂的对话流程时。 LangGraph是一个强大的工具，它可以帮助我们更轻松地构建和理解Chatbot的工作流程。通过图形化的方式定义对话流程，LangGraph使得开发者能够更直观地设计和调试对话系统。本教程的目的是通过构建一个简单的Chatbot来理解LangGraph的基本概念和工作流程。 什么是LangGraph？ LangGraph是一个用于构建和可视化自然语言处理工作流程的Python库。它允许开发者通过图形化的方式定义Chatbot的对话流程，从而更直观地理解和调试对话系统。LangGraph的核心思想是将对话流程表示为一个有向图，其中节点代表对话状态或操作，边代表状态之间的转换。 通过使用LangGraph，开发者可以更容易地管理复杂的对话逻辑，确保Chatbot能够根据用户的输入做出正确的响应。无论是简单的问答系统，还是多轮对话的复杂场景，LangGraph都能提供强大的支持。 安装LangGraph 在开始构建Chatbot之前，我们需要先安装LangGraph库。你可以通过以下命令使用pip进行安装： 1pip install langgraph 安装完成后，我们就可以开始使用LangGraph来构建我们的Chatbot了。 好的！我们可以将这两部分合并到一个标题下，保持内容的连贯性。以下是调整后的内容： 设置 OpenAI API 密钥并初始化语言模型 在构建 Chatbot 之前，我们需要配置 OpenAI 的 API 密钥，并初始化一个语言模型实例。OpenAI 提供了强大的语言模型（如 GPT-4），我们可以通过 API 调用来生成对话内容。以下是相关代码及其解释： 123456789import getpassimport osif not os.environ.get(&quot;OPENAI_API_KEY&quot;): os.environ[&quot;OPENAI_API_KEY&quot;] = getpass.getpass(&quot;Enter API key for OpenAI: &quot;)from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=&quot;gpt-4o-mini&quot;, base_url=&quot;https://api.chatanywhere.tech/v1&quot;) 代码解释： 获取 API 密钥： getpass 模块：用于安全地获取用户输入的 API 密钥，避免密钥直接显示在终端或脚本中。 os.environ.get(&quot;OPENAI_API_KEY&quot;)：检查环境变量中是否已经设置了 OPENAI_API_KEY。如果已经设置，则无需再次输入。 getpass.getpass(&quot;Enter API key for OpenAI: &quot;)：如果环境变量中没有设置 API 密钥，程序会提示用户输入密钥，并将其存储在环境变量中。 初始化语言模型： ChatOpenAI：这是 langchain_openai 库中的一个类，用于与 OpenAI 的聊天模型进行交互。 model=&quot;gpt-4o-mini&quot;：指定使用的语言模型。这里使用的是 gpt-4o-mini，你可以根据需要替换为其他模型，例如 gpt-4 或 gpt-3.5-turbo。 base_url=&quot;https://api.chatanywhere.tech/v1&quot;：指定 OpenAI API 的基础 URL。这里使用的是 https://api.chatanywhere.tech/v1，这是一个第三方代理服务，用于访问 OpenAI 的 API。如果你有官方的 OpenAI API 访问权限，可以将 base_url 替换为官方的 API 地址（如 https://api.openai.com/v1）。 总结： 这段代码的作用是： 安全地获取并设置 OpenAI 的 API 密钥。 初始化一个 OpenAI 的语言模型实例，用于后续的对话生成。 通过这段代码，我们为 Chatbot 提供了强大的语言生成能力，使其能够根据用户输入生成自然、流畅的对话内容。 当然可以！基于你提供的 demo.ipynb 文件内容，我们可以继续完善博客的后续部分。接下来，我们将从定义对话状态开始，逐步完成教程的剩余部分。 定义对话状态 在LangGraph中，对话状态是Chatbot的核心组成部分。它代表了Chatbot在对话过程中所处的不同阶段。我们可以通过定义一个 State 类来表示对话状态。在这个例子中，State 类包含一个 messages 字段，用于存储对话中的消息列表。 123456789from typing import Annotatedfrom typing_extensions import TypedDictfrom langgraph.graph.message import add_messagesclass State(TypedDict): # Messages have the type &quot;list&quot;. The `add_messages` function # in the annotation defines how this state key should be updated # (in this case, it appends messages to the list, rather than overwriting them) messages: Annotated[list, add_messages] 在这个定义中，messages 是一个列表，用于存储对话中的消息。add_messages 函数确保每次更新状态时，新的消息会被追加到列表中，而不是覆盖原有的消息。 构建对话图 接下来，我们需要构建一个对话图。对话图由节点和边组成，节点代表对话状态或操作，边代表状态之间的转换。我们可以使用 StateGraph 类来构建这个图。 123from langgraph.graph import StateGraph, START, ENDgraph_builder = StateGraph(State) 在这个例子中，我们创建了一个 StateGraph 对象，并传入了之前定义的 State 类作为状态类型。 添加节点 在对话图中，节点代表Chatbot的操作或状态。我们可以通过 add_node 方法向图中添加节点。每个节点都有一个唯一的名称和一个对应的函数或对象，用于在节点被调用时执行相应的操作。 1234567def chatbot(state: State): return &#123;&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;])]&#125;# The first argument is the unique node name# The second argument is the function or object that will be called whenever# the node is used.graph_builder.add_node(&quot;chatbot&quot;, chatbot) 在这个例子中，我们定义了一个 chatbot 函数，它接收当前的对话状态并调用 llm.invoke 方法来生成新的消息。然后，我们将这个函数添加为图中的节点，命名为 &quot;chatbot&quot;。 添加边 边代表状态之间的转换。我们可以通过 add_edge 方法向图中添加边。边的起点和终点分别是两个节点的名称。 12graph_builder.add_edge(START, &quot;chatbot&quot;)graph_builder.add_edge(&quot;chatbot&quot;, END) 在这个例子中，我们添加了两条边：一条从 START 节点到 &quot;chatbot&quot; 节点，另一条从 &quot;chatbot&quot; 节点到 END 节点。这意味着对话将从 START 节点开始，经过 &quot;chatbot&quot; 节点生成消息，然后结束。 编译对话图 在添加完节点和边之后，我们需要编译对话图，使其可以运行。 1graph = graph_builder.compile() 编译后的 graph 对象可以用于执行对话流程。 可视化对话图 为了更好地理解对话图的结构，我们可以将其可视化。LangGraph 提供了 draw_mermaid_png 方法，可以将对话图绘制为 Mermaid 格式的图表。 1234567from IPython.display import Image, displaytry: display(Image(graph.get_graph().draw_mermaid_png()))except Exception: # This requires some extra dependencies and is optional pass 如果环境支持，这段代码将显示对话图的可视化结果。 运行Chatbot 最后，我们可以运行Chatbot并观察其行为。通过 graph.stream 方法，我们可以将用户输入传递给Chatbot，并逐步生成对话。 12345678910111213141516171819def stream_graph_updates(user_input: str): for event in graph.stream(&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;): for value in event.values(): print(&quot;Assistant:&quot;, value[&quot;messages&quot;][-1].content)while True: try: user_input = input(&quot;User: &quot;) if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;q&quot;]: print(&quot;Goodbye!&quot;) break stream_graph_updates(user_input) except: # fallback if input() is not available user_input = &quot;What do you know about LangGraph?&quot; print(&quot;User: &quot; + user_input) stream_graph_updates(user_input) break 在这个例子中，我们创建了一个简单的交互式循环，用户可以输入消息，Chatbot 会生成响应。如果用户输入 &quot;quit&quot;、&quot;exit&quot; 或 &quot;q&quot;，程序将退出。 结论 通过本教程，我们学习了如何使用 LangGraph 构建一个简单的 Chatbot。我们定义了对话状态、构建了对话图、添加了节点和边，并最终运行了 Chatbot。虽然这个 Chatbot 非常简单，但它展示了 LangGraph 的基本工作流程。你可以在此基础上进一步扩展和优化 Chatbot 的功能，例如添加更多的对话状态、处理复杂的用户输入等。 希望这篇教程对你理解 LangGraph 有所帮助！如果你有任何问题或建议，欢迎在评论区留言。 作者: Mudrobot 日期: 2025.01.23 标签: LangGraph, Chatbot, NLP, 自然语言处理"},{"title":"二分查找","path":"/wiki/LeetCode/专项训练/二分查找.html","content":"基础原理 本板块整理自灵茶山艾府b站视频二分查找 红蓝染色法_哔哩哔哩_bilibili 通过一道例题进行讲解： 例题返回数组中第一个≥8\\geq 8≥8 数字的位置，如果所有数都&lt;8&lt; 8&lt;8 则返回长度值 非常明显这道题我们可以采用二分查找的方法来解决，首先我们先讲解一下闭区间二分解决这道题的写法： 闭区间二分写法 假设队列长度为m，首先设左指针L指向0，右指针指向m-1。每次取中间的元素进行判断，如果中间的元素≥8\\geq 8≥8 则 R= mid-1否则L = mid+1，持续更新直到L&gt;R 1234567891011121314#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; arr = &#123;5,7,7,8,8,10&#125;;int main()&#123; int len = arr.size(); int l = 0, r = len-1; while(l&lt;=r)&#123; int mid = l+(r-l)/2; if (arr[mid]&gt;=8) r = mid-1; else l = mid+1; &#125; cout&lt;&lt;l; return 0;&#125; 注意：二分中求mid的写法，像上面这样写能有效的避免 这里其实就可以感觉到，在更新的过程中，无论何时L-1指向的一定是&lt;8的数R+1指向的一定是&gt;=8的数 左闭右开二分写法 这个是经常看到的二分的写法 同样假设队列长度为m，左指针L指向0，右指针指向m，每次取中间元素进行判断，如果中间元素≥8\\geq 8≥8则R = mid，否则L = mid+1，持续更新直到 L≥RL\\geq RL≥R 1234567891011121314#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; arr = &#123;5,7,7,8,8,10&#125;;int main() &#123; int len = arr.size(); int l = 0, r = len; while(l&lt;r)&#123; int mid = l+(r-l)/2; if(arr[mid] &gt;= 8) r = mid; else l = mid+1; &#125; cout&lt;&lt; l; return 0;&#125; 开区间二分写法 同样假设队列长度为m，左指针指向-1，右指针指向m，每次取中间元素进行判断，如果中间元素≥8\\geq 8≥8则R = mid, 否则L = mid，持续更新直到L+1≥RL+1 \\geq RL+1≥R 1234567891011121314#include&lt;bits/stdc++.h&gt;using namespace std;vector&lt;int&gt; arr = &#123;5,7,7,8,8,10&#125;;int main() &#123; int len = arr.size(); int l = -1, r = len; while(l+1&lt;r)&#123; int mid = l + (r-l)/2; if (mid&gt;=8) r = mid; else l = mid; &#125; cout &lt;&lt; r; return 0;&#125; 一道例题 题目传送门：34. 在排序数组中查找元素的第一个和最后一个位置 - 力扣（LeetCode） 其中找当前数出现的最后一个，就是找比他大1的数的第一个位置，然后把这个位置-1。代码如下： 123456789101112131415161718192021222324252627class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; int len = nums.size(); int l = 0, r = len; while(l&lt;r)&#123; int mid = l+(r-l)/2; if (nums[mid]&gt;=target) r=mid; else l = mid+1; &#125; if (l == len || nums[l]!=target)&#123; vector&lt;int&gt; nw = &#123;-1,-1&#125;; return nw; &#125; vector&lt;int&gt; ans; ans.emplace_back(l); l = 0; r = len; while(l&lt;r)&#123; int mid= l+(r-l)/2; if(nums[mid]&gt;=target+1) r=mid; else l = mid+1; &#125; ans.emplace_back(l-1); return ans; &#125;&#125;; 当然这道题使用lower_bound和upper_bound也是可以的，具体使用方法见C++部分。 123456789class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt;&amp; nums, int target) &#123; auto it = lower_bound(nums.begin(),nums.end(),target); if (it == nums.end() || *it != target) return vector&lt;int&gt;&#123;-1,-1&#125;; auto it2 = upper_bound(nums.begin(), nums.end(), target); return vector&lt;int&gt;&#123;int(it-nums.begin()),int(it2-nums.begin())-1&#125;; &#125;&#125;; 总结 可能看完上面的几种写法，还是没有理解二分的本质是什么，这里就来总结一下。其实通过观察上面常见的三种写法我们不难发现，二分的目的是判断整个区间和一个目标值的大小关系，为了达到这个目的，我们需要牢记区间的定义！区间内的数（下标）都是还未确定与 target 的大小关系的，有的是 &lt; target，有的是 ≥ target；区间外的数（下标）都是确定与 target 的大小关系的。 理解了上述定义，则二分查找算法也就理解了。 二分题单（右边数字为难度分） 二分答案 H 指数 II 使结果不超过阈值的最小除数 1542 完成旅途的最少时间 1641 每个小孩最多能分到多少糖果 1646 准时到达的列车最小时速 1676 在 D 天内送达包裹的能力 1725 爱吃香蕉的珂珂 1766 可移除字符的最大数目 1913 制作 m 束花所需的最少天数 1946 可以到达的最远建筑 1962 最大合金数 1981 逃离火灾 2347 275. H指数 题目传送门：275. H 指数 II - 力扣（LeetCode） 二分判定条件就是如果当前citation数量仍然大于等于后面的文章数量就还可以往前二分。需要注意的是要判断一下数组有没有越界。 左闭右开二分如下： 1234567891011121314151617class Solution &#123;public: int hIndex(vector&lt;int&gt;&amp; citations) &#123; int len = citations.size(); int l = 0,r = len; function&lt;bool(int)&gt; judge = [&amp;](int x)&#123; if (citations[x]&gt;=len-x) return true; else return false; &#125;; while(l &lt; r)&#123; int mid = l+(r-l)/2; if (judge(mid)) r = mid; else l = mid+1; &#125; return l==len?0:min(len-l,citations[l]); &#125;&#125;; 1283. 使结果不超过阈值的最小除数 题目传送门：1283. 使结果不超过阈值的最小除数 - 力扣（LeetCode） 一道非常明显的二分题目，需要注意的是是上取整，这点有点坑。 开区间二分如下： 1234567891011121314151617181920class Solution &#123;public: int smallestDivisor(vector&lt;int&gt;&amp; nums, int threshold) &#123; function&lt;bool(int)&gt; judge = [&amp;](int x)&#123; int ans = 0; for (auto &amp;num:nums) &#123; if (num%x) ans = ans+num/x+1; else ans += num/x; &#125; return ans&lt;=threshold; &#125;; int l=0,r=1000001; while (l+1&lt;r) &#123; int mid = l+(r-l)/2; if (judge(mid)) r=mid; else l=mid; &#125; return r; &#125;&#125;; 2187. 完成旅途的最少时间 题目传送门：2187. 完成旅途的最少时间 - 力扣（LeetCode） 闭区间二分如下： 1234567891011121314151617181920class Solution &#123;public: long long minimumTime(vector&lt;int&gt;&amp; time, int totalTrips) &#123; long long r = 100000000000003; long long l = 0; function&lt;bool(long long)&gt; judge=[&amp;](long long x)&#123; long long ans = 0; for (auto tim:time) &#123; ans += x / tim; &#125; return ans&gt;=(long long)totalTrips?true:false; &#125;; while (l &lt;= r) &#123; long long mid = l + (r - l)/2; if (judge(mid)) r = mid-1; else l = mid+1; &#125; return l; &#125;&#125;; 2226. 每个小孩最多能分到多少糖果 题目传送门：2226. 每个小孩最多能分到多少糖果 - 力扣（LeetCode） 闭区间二分如下： 1234567891011121314151617181920212223class Solution &#123;public: int maximumCandies(vector&lt;int&gt;&amp; candies, long long k) &#123; auto maxit = max_element(candies.begin(),candies.end()); int max_ele = *maxit; function&lt;bool(long long)&gt; judge = [&amp;](int x) &#123; long long ans = 0; if (x==0) return true; for (auto candy:candies) &#123; ans += candy/x; &#125; return ans&gt;=k?true:false; &#125;; int l=0,r=max_ele; while (l&lt;=r) &#123; int mid = l + (r-l)/2; if(judge(mid)) l=mid+1; else r=mid-1; &#125; return r; &#125;&#125;; 1870. 准时到达的列车最小时速 题目传送门：1870. 准时到达的列车最小时速 - 力扣（LeetCode） 左闭右开区间二分如下： 12345678910111213141516171819202122class Solution &#123;public: int minSpeedOnTime(vector&lt;int&gt;&amp; dist, double hour) &#123; int len = dist.size(); function&lt;bool(int)&gt; judge = [&amp;](int v) &#123; int time=0; for(int i=0;i&lt;len-1;++i) &#123; time += dist[i]/v; if (dist[i]%v) time += 1; &#125; double fin_time = (double)time + (double) dist[len-1]/(double)v; return fin_time &lt;= hour?true:false; &#125;; int l = 1,r = 10000001; while (l&lt;r) &#123; int mid = l + (r-l)/2; if(judge(mid)) r = mid; else l = mid+1; &#125; return judge(l)?l:-1; &#125;&#125;; 注意：最后返回的时候要判断一下当前速度能不能按规定到达目的地 1011. 在 D 天内送达包裹的能力 题目传送门：1011. 在 D 天内送达包裹的能力 - 力扣（LeetCode） 左闭右开区间二分代码如下所示： 12345678910111213141516171819202122232425class Solution &#123;public: int shipWithinDays(vector&lt;int&gt;&amp; weights, int days) &#123; function&lt;bool(int)&gt; judge = [&amp;](int x)&#123; int cur=0,cur_day=1; for (auto weight:weights) &#123; if(cur+weight &lt;= x)&#123; cur = cur+weight; &#125; else &#123; cur_day++; cur = weight; &#125; &#125; return cur_day&lt;=days?true:false; &#125;; int l = *max_element(weights.begin(), weights.end()); int r = accumulate(weights.begin(),weights.end(),0)+1; while (l&lt;r) &#123; int mid = l + (r-l)/2; if (judge(mid)) r = mid; else l = mid+1; &#125; return l; &#125;&#125;; 可以留意一下里面accumulate求和函数的用法 875. 爱吃香蕉的珂珂 875. 爱吃香蕉的珂珂 - 力扣（LeetCode） 开区间二分代码如下： 1234567891011121314151617181920class Solution &#123;public: int minEatingSpeed(vector&lt;int&gt;&amp; piles, int h) &#123; function&lt;bool(int)&gt; judge = [&amp;](int v)&#123; long long cos = 0; for(auto pile:piles)&#123; cos += pile/v; if(pile%v) cos += 1; &#125; return cos&lt;=(long long)h?true:false; &#125;; int l = 0,r = INT_MAX; while (l+1&lt;r) &#123; int mid = l + (r-l)/2; if(judge(mid)) r = mid; else l = mid; &#125; return r; &#125;&#125;; 1898. 可移除字符的最大数目 题目传送门：1898. 可移除字符的最大数目 - 力扣（LeetCode） 同样首先观察数据范围，1e5的数据范围，预感这道题应该使用二分。 这道题比之前的题目要稍微难一点，难点主要集中在judge函数的书写上，其实只要在每次判断时使用一个哈希数组记录一下当前字符有没有被删除，就可以较为简单的解决这道题。以下提供两种开区间二分写法，注意区分其中细节，如果能够区分出来差别，那恭喜，说明你很好的掌握了的二分查找🎇。 二分下标（判断到当前下标之前（包含当前下标）能否被remove） 12345678910111213141516171819202122232425class Solution &#123;public: int maximumRemovals(string s, string p, vector&lt;int&gt;&amp; removable) &#123; int len = s.size(); function&lt;bool(int)&gt; judge=[&amp;](int x)&#123; vector&lt;bool&gt; vis(len,true); for (int i=0;i&lt;=x;++i) vis[removable[i]] = false; int pp = 0; for(int i=0;i&lt;len;++i)&#123; if (s[i]==p[pp]&amp;&amp;vis[i])&#123; pp++; if (pp == p.size()) return true; &#125; &#125; return false; &#125;; int l = -1, r = removable.size(); while (l+1&lt;r) &#123; int mid = l + (r-l)/2; if (judge(mid)) l = mid; else r = mid; &#125; return l+1; &#125;&#125;; 二分前n个remove的这个n： 12345678910111213141516171819202122232425class Solution &#123;public: int maximumRemovals(string s, string p, vector&lt;int&gt;&amp; removable) &#123; int len = s.size(); function&lt;bool(int)&gt; judge=[&amp;](int x)&#123; vector&lt;bool&gt; vis(len,true); for (int i=0;i&lt;x;++i) vis[removable[i]] = false; int pp = 0; for(int i=0;i&lt;len;++i)&#123; if (s[i]==p[pp]&amp;&amp;vis[i])&#123; pp++; if (pp == p.size()) return true; &#125; &#125; return false; &#125;; int l = -1, r = removable.size()+1; while (l+1&lt;r) &#123; int mid = l + (r-l)/2; if (judge(mid)) l = mid; else r = mid; &#125; return l; &#125;&#125;; 1482. 制作 m 束花所需的最少天数 题目传送门：1482. 制作 m 束花所需的最少天数 - 力扣（LeetCode） 同样首先观察数据范围，1e5的数据范围，预感这道题应该使用二分。 同上，就是需要设计一下judge函数 123456789101112131415161718192021222324252627class Solution &#123;public: int minDays(vector&lt;int&gt;&amp; bloomDay, int m, int k) &#123; auto judge = [&amp;](int x) &#123; int cnt=0,cur=0; for (auto bloom:bloomDay) &#123; if (bloom &lt;= x) &#123; cur++; if (cur == k) &#123; cur = 0; cnt ++; if (cnt == m) return true; &#125; &#125; else cur = 0; &#125; return false; &#125;; int l = 1, r = *max_element(bloomDay.begin(),bloomDay.end()); while (l &lt;= r) &#123; int mid = l + (r-l)/2; if (judge(mid)) r=mid-1; else l=mid+1; &#125; return judge(l)?l:-1; &#125;&#125;; 1642. 可以到达的最远建筑 题目传送门：1642. 可以到达的最远建筑 - 力扣（LeetCode） 同样首先观察数据范围，1e5的数据范围，预感这道题应该使用二分。 左闭右开区间二分写法如下所示： 12345678910111213141516171819202122class Solution &#123;public: int furthestBuilding(vector&lt;int&gt;&amp; heights, int bricks, int ladders) &#123; auto judge = [&amp;](int x) &#123; vector&lt;int&gt; h_diffs; for(int i=1;i&lt;=x;++i)&#123; if (heights[i] &gt; heights[i-1]) h_diffs.emplace_back(heights[i]-heights[i-1]); &#125; if (ladders&gt;=h_diffs.size()) return true; sort(h_diffs.begin(),h_diffs.end()); int sum_diff = accumulate(h_diffs.begin(),h_diffs.end()-ladders,0); return sum_diff &lt;= bricks?true:false; &#125;; int l=0,r=heights.size(); while(l&lt;r)&#123; int mid = l + (r-l)/2; if (judge(mid)) l = mid+1; else r=mid; &#125; return l-1; &#125;&#125;; 注意：该题合法区间在左边，所以最终左闭右开区间二分返回值应该是l-1 2861. 最大合金数 题目传送门：2861. 最大合金数 - 力扣（LeetCode） 题目中规定只能使用一个机器生产所有合金，所以就非常简单了，和之前的二分差不多，只不过中间计算开销的时候会爆int所以设置成long long。 左开右闭二分书写如下： 1234567891011121314151617181920212223class Solution &#123;public: int maxNumberOfAlloys(int n, int k, int budget, vector&lt;vector&lt;int&gt;&gt;&amp; composition, vector&lt;int&gt;&amp; stock, vector&lt;int&gt;&amp; cost) &#123; auto judge = [&amp;](int x) &#123; for(int i=0; i&lt;k; ++i) &#123; long long cost_all = 0; for(int j=0; j&lt;n; ++j) &#123; cost_all += max((long long)0, ((long long)x*composition[i][j]-stock[j]))*cost[j]; if(cost_all &gt; budget) break; &#125; if (cost_all &lt;= budget) return true; &#125; return false; &#125;; int l = 0, r = INT_MAX; while (l&lt;r) &#123; int mid = l + (r-l)/2; if(judge(mid)) l = mid+1; else r = mid; &#125; return l-1; &#125;&#125;; 2258. 逃离火灾 这道题还是非常有意思的，是一道bfs和二分的结合，首先我们先使用bfs预处理一遍将每个格子着火的时间都先处理出来。然后我们使用二分，二分等待时间，二分判断函数就是第二次bfs，判断在当前等待时间下能否进入安全屋。所以一共需要提供两个bfs函数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class Solution &#123;public: int maximumMinutes(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; vector&lt;vector&lt;int&gt;&gt; dirs = &#123;&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,1&#125;,&#123;0,-1&#125;&#125;; int n = grid.size(),m = grid[0].size(); vector&lt;vector&lt;int&gt;&gt; tim_burn(n,vector&lt;int&gt;(m,INT_MAX)); auto judge = [&amp;](int x,int y) &#123; if (x&lt;0||y&lt;0||x&gt;=n||y&gt;=m) return false; if (grid[x][y] == 2) return false; return true; &#125;; auto bfs1 = [&amp;](int x, int y)&#123; &#125;; auto bfs2 = [&amp;](int stay_time)&#123; vector&lt;vector&lt;int&gt;&gt; reach(n,vector&lt;int&gt;(m,INT_MAX)); if (stay_time &gt; tim_burn[0][0]) return false; queue&lt;vector&lt;int&gt;&gt; que; que.push(vector&lt;int&gt;&#123;0,0&#125;); reach[0][0] = stay_time; while (!que.empty()) &#123; vector&lt;int&gt; nw = que.front();que.pop(); for(auto dir:dirs) &#123; if (judge(nw[0]+dir[0],nw[1]+dir[1])) &#123; int nx = nw[0]+dir[0], ny = nw[1]+dir[1]; if (nx == n-1 &amp;&amp; ny == m-1 &amp;&amp; tim_burn[nx][ny] &gt;= reach[nw[0]][nw[1]]+1) return true; if (tim_burn[nx][ny] &gt; reach[nw[0]][nw[1]]+1) &#123; if (reach[nx][ny] &gt; reach[nw[0]][nw[1]]+1)&#123; reach[nx][ny] = reach[nw[0]][nw[1]]+1; que.push(vector&lt;int&gt;&#123;nx,ny&#125;); &#125; &#125; &#125; &#125; &#125; return false; &#125;; queue&lt;vector&lt;int&gt;&gt; que1; for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;m;++j)&#123; if (grid[i][j]==1) &#123; tim_burn[i][j] = 0; que1.push(vector&lt;int&gt;&#123;i,j&#125;); &#125; &#125; &#125; while (!que1.empty()) &#123; vector&lt;int&gt; nw = que1.front();que1.pop(); for(auto dir:dirs) &#123; if (judge(nw[0]+dir[0],nw[1]+dir[1])) &#123; int nx = nw[0]+dir[0], ny = nw[1]+dir[1]; if (tim_burn[nx][ny] &gt; tim_burn[nw[0]][nw[1]]+1) &#123; tim_burn[nx][ny] = tim_burn[nw[0]][nw[1]]+1; que1.push(vector&lt;int&gt;&#123;nx,ny&#125;); &#125; &#125; &#125; &#125; // close interval int l = 0, r = 1000000000; while(l &lt;= r) &#123; int mid = l + (r-l)/2; if(bfs2(mid)) l = mid+1; else r = mid-1; &#125; return bfs2(1000000000)?1000000000:l-1; &#125;&#125;; 注意第一次预处理的时候就将所有的火源都压入bfs队列中预处理一次就行了，不要每次发现一个火源就bfs一次，非常的浪费时间。 最小化最大值 分配给商店的最多商品的最小值 1886 袋子里最少数目的球 1940 最小化数组中的最大值 1965 打家劫舍 IV 2081 水位上升的泳池中游泳 2097 最小化数对的最大差值 2155 最小化两个数组中的最大值 2302 最大化最小值 两球之间的磁力 1920 最大合金数 1981 礼盒的最大甜蜜度 2021 找出最安全路径 2154 最大化城市的最小供电站数目 2236"},{"title":"动态规划","path":"/wiki/LeetCode/专项训练/动态规划.html","content":"改题单选自灵神：分享丨【题单】动态规划（入门/背包/状态机/划分/区间/状压/数位/树形/数据结构优化） - 力扣（LeetCode） 一、入门 DP 1.0 题目清单 1. 爬楼梯 70. 爬楼梯 - 力扣（LeetCode） 746. 使用最小花费爬楼梯 - 力扣（LeetCode） 377. 组合总和 Ⅳ - 力扣（LeetCode） 2466. 统计构造好字符串的方案数 - 力扣（LeetCode） 2266. 统计打字方案数 - 力扣（LeetCode） 2. 打家劫舍 198. 打家劫舍 - 力扣（LeetCode）~1500 740. 删除并获得点数 - 力扣（LeetCode）~1600 2320. 统计放置房子的方式数 - 力扣（LeetCode）1608 213. 打家劫舍 II - 力扣（LeetCode）~1650 3. 最大子数组和（最大子段和） 53. 最大子数组和 - 力扣（LeetCode） 1400 2606. 找到最大开销的子字符串 - 力扣（LeetCode） 1422 1749. 任意子数组和的绝对值的最大值 - 力扣（LeetCode） 1542 1191. K 次串联后最大子数组之和 - 力扣（LeetCode） 1748 918. 环形子数组的最大和 - 力扣（LeetCode） 1777 2321. 拼接数组的最大分数 - 力扣（LeetCode） 1791 思维拓展题：152. 乘积最大子数组 - 力扣（LeetCode） 1.1 爬楼梯类 70. 爬楼梯 题目传送门：70. 爬楼梯 - 力扣（LeetCode） 简单递推直接秒了，最后实现代码如下： 1234567891011class Solution &#123;public: int climbStairs(int n) &#123; vector&lt;int&gt; dp(n+1,0); dp[0] = 1; dp[1] = 1; for(int i=2;i&lt;=n;++i)&#123; dp[i] = dp[i-1] + dp[i-2]; &#125; return dp[n]; &#125;&#125;; 746. 使用最小花费爬楼梯 题目传送门：746. 使用最小花费爬楼梯 - 力扣（LeetCode） 同上，递推即可。 最后实现代码如下： 1234567891011class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(); vector&lt;int&gt; dp(n+1,0); for(int i=2;i&lt;=n;++i)&#123; dp[i] = min(dp[i-1]+cost[i-1], dp[i-2]+cost[i-2]); &#125; return dp[n]; &#125;&#125;; 377. 组合总和 Ⅳ 题目传送门：377. 组合总和 Ⅳ - 力扣（LeetCode） 此题类似一个完全背包问题，这里需要注意一下内外循环顺序，先循环target再循环给定数组中的数。 最后实现代码如下： 123456789101112131415class Solution &#123;public: int combinationSum4(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;unsigned&gt; dp(target+1,0); dp[0] = 1; int n = nums.size(); for(int i=1; i&lt;=target; ++i)&#123; for(int j=0; j&lt;n; ++j)&#123; if (nums[j] &gt; i) continue; dp[i] += dp[i-nums[j]]; &#125; &#125; return dp[target]; &#125;&#125;; 2466. 统计构造好字符串的方案数 题目传送门：2466. 统计构造好字符串的方案数 - 力扣（LeetCode） 跟上面那道题目思路一样。 最后实现代码如下： 1234567891011121314151617class Solution &#123;public: int countGoodStrings(int low, int high, int zero, int one) &#123; vector&lt;int&gt; dp(1e5+2,0); dp[0] = 1; int MOD = 1e9+7; for(int i=1;i&lt;=high;++i)&#123; if (i&gt;=zero) dp[i] += dp[i-zero]; if (i&gt;=one) dp[i] += dp[i-one]; dp[i] %= MOD; &#125; int ans = 0; for(int i=low; i&lt;=high; ++i)&#123; ans = (ans + dp[i]) % MOD; &#125; return ans; &#125;&#125;; 2266. 统计打字方案数 题目传送门：2266. 统计打字方案数 - 力扣（LeetCode） 仍然是爬楼梯类问题，但是并不是那种可以直接解决的，可以采用如下方式解决： 将相同字符分为一组，每组内只有一种字符。考虑以下动态规划（DP）方法： 对于字符不为7或9的情况，定义( f[i] )表示长度为( i )的只有一种字符的字符串对应的文字信息种类数。我们可以将末尾的1个、2个或3个字符单独视作一个字母，那么有转移方程： f[i]=f[i−1]+f[i−2]+f[i−3]f[i] = f[i-1] + f[i-2] + f[i-3] f[i]=f[i−1]+f[i−2]+f[i−3] 对于字符为7或9的情况，定义( g[i] )表示长度为( i )的只有一种字符的字符串对应的文字信息种类数，可以得到类似的转移方程： g[i]=g[i−1]+g[i−2]+g[i−3]+g[i−4]g[i] = g[i-1] + g[i-2] + g[i-3] + g[i-4] g[i]=g[i−1]+g[i−2]+g[i−3]+g[i−4] 这样能算出每组字符串的文字信息种类数。 由于不同组之间互不影响，根据乘法原理，把不同组的文字信息种类数相乘，得到答案。 最后实现代码如下： 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: int countTexts(string pressedKeys) &#123; int MOD = 1e9 +7, n = pressedKeys.size(); vector&lt;long long&gt; dp_3(100003,0),dp_4(100003,0); dp_3[0] = 1; dp_3[1] = 1; dp_3[2] = 2; dp_3[3] = 4; dp_4[0] = 1; dp_4[1] = 1; dp_4[2] = 2; dp_4[3] = 4; for (int i=4;i&lt;=n;++i) &#123; dp_3[i] = dp_3[i-1] + dp_3[i-2] + dp_3[i-3]; dp_4[i] = dp_4[i-1] + dp_4[i-2] + dp_4[i-3] + dp_4[i-4]; dp_3[i] %= MOD; dp_4[i] %= MOD; &#125; int cnt = 1; long long ans = 1; char ch = pressedKeys[0]; for(int i=1;i&lt;n;++i)&#123; if (pressedKeys[i]!=ch)&#123; if (ch == &#x27;7&#x27;||ch == &#x27;9&#x27;) ans = (dp_4[cnt] * ans) % MOD; else ans = (dp_3[cnt] * ans) % MOD; cnt = 1; ch = pressedKeys[i]; &#125; else &#123; cnt++; &#125; &#125; int tmp; if(ch == &#x27;7&#x27;||ch == &#x27;9&#x27;) tmp = dp_4[cnt]; else tmp = dp_3[cnt]; return ((ans*tmp)%MOD); &#125;&#125;; 1.2 打家劫舍 198. 打家劫舍 题目传送门：198. 打家劫舍 - 力扣（LeetCode） 定义动态规划数组dp[i]表示偷到第i个房子，所能盗取的最大金额。 那么不难写出状态转移方程： 1dp[i] = max(dp[i-1],dp[i-2]+nums[i-1]); 最后确定一下初始状态dp[0] = 0,dp[1] = nums[0],完成解答。 最后实现代码如下： 123456789101112class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; dp(n+2, 0); dp[1] = nums[0]; for(int i=2;i&lt;=n;++i)&#123; dp[i] = max(dp[i-1],dp[i-2]+nums[i-1]); &#125; return dp[n]; &#125;&#125;; 740. 删除并获得点数 题目传送门：740. 删除并获得点数 观察数据范围后不难发现就是上面打家劫舍这种类型的题目，排序后，动态规划同上一题目。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: int deleteAndEarn(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt;nums_all(10004, 0), dp(10004, 0); sort(nums.begin(), nums.end()); int prev = nums[0], cnt = 0; for(int i=0;i&lt;n;++i)&#123; if (nums[i]==prev) cnt++; else &#123; nums_all[prev] = cnt * prev; cnt = 1; prev = nums[i]; &#125; &#125; nums_all[prev] = prev * cnt; dp[1] = nums_all[1]; for(int i=2;i&lt;=nums[n-1];++i)&#123; dp[i] = max(dp[i-1], dp[i-2]+nums_all[i]); &#125; return dp[nums[n-1]]; &#125;&#125;; 2320. 统计放置房子的方式数 题目传送门：2320. 统计放置房子的方式数 - 力扣（LeetCode） 观察每个位置的放置状态一共有4种情况： 上无下无 上有下无 上无下有 上有下有 因此可以设计一个dp[n][4]动态规划数组，对于当前每一种状态，都可以从上一个位置其他状态推导而来，最后的答案就是最后一个位置的所有4种状态之和。 最后实现代码如下： 123456789101112131415class Solution &#123;public: int countHousePlacements(int n) &#123; int MOD = 1e9 + 7; vector&lt;vector&lt;int&gt;&gt; dp(10002,vector&lt;int&gt;(4,0)); dp[1][0] = 1; dp[1][1] = 1; dp[1][2] = 1; dp[1][3] = 1; for(int i=2;i&lt;=n;++i)&#123; dp[i][1] = (dp[i-1][0] + dp[i-1][2])%MOD; dp[i][2] = (dp[i-1][0] + dp[i-1][1])%MOD; dp[i][3] = dp[i-1][0]; dp[i][0] = ((dp[i-1][0]+dp[i-1][1])%MOD + (dp[i-1][2]+dp[i-1][3])%MOD)%MOD; &#125; return ((dp[n][0]+dp[n][1])%MOD + (dp[n][2]+dp[n][3])%MOD)%MOD; &#125;&#125;; 213. 打家劫舍 II 题目传送门：213. 打家劫舍 II - 力扣（LeetCode） 此题就是之前的打家劫舍但是将顺序的结构改成了环形的结构。为了防止同时选到第一个和最后一个，可以做两遍DP。 注意： 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int rob(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; dp1(n+2,0), dp2(n+2,0); // 特殊判断（只有一个元素） if (n == 1) return nums[0]; // 不选第一个 dp1[1] = 0; for(int i=2;i&lt;=n;++i)&#123; dp1[i] = max(dp1[i-1], dp1[i-2]+nums[i-1]); &#125; // 不选最后一个 dp2[1] = nums[0]; for(int i=2;i&lt;n;++i)&#123; dp2[i] = max(dp2[i-1], dp2[i-2]+nums[i-1]); &#125; return max(dp1[n], dp2[n-1]); &#125;&#125;; 1.3 最大子数组和（最大子段和） 53. 最大子数组和 题目传送门：53. 最大子数组和 - 力扣（LeetCode） 不同于之前的DP，这里定义的DP数组dp[i]含义是，以第i个数结尾的连续子数组最大和（因为条件中包含连续这一要求，因此只有这样才能描述状态转移方程） 最后实现代码如下： 1234567891011121314class Solution &#123;public: int maxSubArray(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; dp(n+2,0); dp[1] = nums[0]; for(int i=1;i&lt;=n;++i)&#123; dp[i] = max(dp[i-1]+nums[i-1],nums[i-1]); &#125; int ans = 0; for(int i=1;i&lt;=n;++i) ans = max(ans,dp[i]); return ans; &#125;&#125;; 2606. 找到最大开销的子字符串 题目传送门：2606. 找到最大开销的子字符串 - 力扣（LeetCode） 思路同上 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int maximumCostSubstring(string s, string chars, vector&lt;int&gt;&amp; vals) &#123; int n = s.size(); vector&lt;int&gt; dp(n+2,0); unordered_map&lt;char,int&gt; mp; int m = chars.size(); for(int i=0; i&lt;26; ++i) mp[&#x27;a&#x27;+i] = i+1; for(int i=0; i&lt;m; ++i) mp[chars[i]] = vals[i]; for(int i=1;i&lt;=n;++i) &#123; int val = mp[s[i-1]]; dp[i] = max(dp[i-1]+val, val); &#125; int ans = dp[0]; for(int i=1;i&lt;=n;++i) ans = max(ans, dp[i]); return ans; &#125;&#125;; 1749. 任意子数组和的绝对值的最大值 题目传送门：1749. 任意子数组和的绝对值的最大值 - 力扣（LeetCode） 思路上和之前一样， 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int maxAbsoluteSum(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(2,0)); for(int i=1;i&lt;=n;++i)&#123; dp[i][0] = max(dp[i-1][0]+nums[i-1],nums[i-1]); dp[i][1] = min(dp[i-1][1]+nums[i-1],nums[i-1]); &#125; int ans = 0; for(int i=1;i&lt;=n;++i)&#123; ans = max(ans,max(-dp[i][1], dp[i][0])); &#125; return ans; &#125;&#125;; 这道题这样做可能还麻烦了一些，还可以利用前缀和的思想进行简化。具体而言就是找到这个数组中前缀和最大和前缀和最小，相减就可以得出想要的答案。 12345678910111213class Solution &#123;public: int maxAbsoluteSum(vector&lt;int&gt;&amp; nums) &#123; int maxx = 0, minn = 0; int nw=0; for(int num : nums)&#123; nw += num; maxx = max(maxx, nw); minn = min(minn, nw); &#125; return maxx - minn; &#125;&#125;; 1191. K 次串联后最大子数组之和 题目传送门：1191. K 次串联后最大子数组之和 - 力扣（LeetCode） 此题定义的DP数组（实际未定义，使用一个变量优化掉了）同之前一样，也是表现的是以当前数结尾的最大子数组和。 这里需要注意的是题目中的k其实是吓人的，我们仔细分析一下便可得知: 当k为1时，同最大子数组和 当k为2时，由于存在特殊情况（选第一个数组后缀+第二个数组的前缀）将数组拼接在一起，同理可以转变为最大子数组和 当k大于2时，相比上一种情况，要多考虑，如果出现横跨数组的情况的话，相当于中间数组的和加上单个数组的最大前缀和与最大后缀和。 综上分三类讨论即可。实际上没有将数组扩展k倍。 最后实现代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int kConcatenationMaxSum(vector&lt;int&gt;&amp; arr, int k) &#123; int n = arr.size(); int MOD = 1e9 + 7; long long maxx = 0,minn = 0,sum_arr = 0;// 前缀和 // 前缀和预处理 for(int i=0;i&lt;n;++i)&#123; sum_arr += arr[i]; maxx = max(maxx, sum_arr); minn = min(minn, sum_arr); &#125; if (k!=1) arr.insert(arr.end(),arr.begin(),arr.end()); int m = arr.size(); long long tmp = 0,ans=0; for(int i=0;i&lt;m;++i)&#123; tmp = max(tmp,(long long)(0)) + arr[i]; ans = max(ans, tmp); &#125; if (k&lt;=2) return ans%MOD; else return max(ans, (k-2)*sum_arr + maxx + sum_arr-minn)%MOD; &#125;&#125;; 918. 环形子数组的最大和 题目传送门：918. 环形子数组的最大和 - 力扣（LeetCode） 同样还是分类讨论的思想。 对于非环形数组的情况，其解决方法类似于寻找最大子数组和，这里不再详述。而对于环形数组，可以将问题转化为寻找最大的前缀和与后缀和，这两部分不能有交集。通过计算每个位置的最大前后缀和，可以解决环形问题。因此，整体问题可视为求解“最大子数组和”与“最大前后缀和”的较大者。最终，比较这两个结果，返回较大的值即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728class Solution &#123;public: int maxSubarraySumCircular(vector&lt;int&gt;&amp; nums) &#123; // 数组内最大值 int ans1=nums[0], tmp=0; for(auto num : nums)&#123; tmp = max(num, num + tmp); ans1 = max(tmp,ans1); &#125; // 维护前后缀最大值 int n = nums.size(); vector&lt;int&gt; pre(n+1,0),pos(n+1,0); int pre_tmp = 0, pos_tmp = 0; for(int i=1;i&lt;=n;++i)&#123; pre_tmp += nums[i-1]; if(i&gt;1) pre[i] = max(pre[i-1], pre_tmp); else pre[i] = pre_tmp; pos_tmp += nums[n-i]; if(i&gt;1) pos[i] = max(pos[i-1], pos_tmp); else pos[i] = pos_tmp; &#125; int ans2 = pre[1]+pos[n-1]; for(int i=1;i&lt;n;++i)&#123; ans2 = max(ans2,pre[i]+pos[n-i]); &#125; return max(ans1, ans2); &#125;&#125;; 2321. 拼接数组的最大分数 题目传送门：2321. 拼接数组的最大分数 - 力扣（LeetCode） 我们将两个数组作差得到两个差值数组diff1和diff2，仔细观察后可以将此题转化为最大子数组和这个问题， 最后实现代码如下： 1234567891011121314class Solution &#123;public: int maximumsSplicedArray(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; int n = nums1.size(),sum1=0,sum2=0; int max2=0,max1=0,tmp1=0,tmp2=0; for(int i=0;i&lt;n;++i) &#123; sum1 += nums1[i]; sum2 += nums2[i]; tmp1 = max(tmp1, 0) + nums1[i] - nums2[i]; tmp2 = max(tmp2, 0) + nums2[i] - nums1[i]; max1 = max(max1, tmp1); max2 = max(max2, tmp2); &#125; return max(sum1 + max2, sum2 + max1); &#125;&#125;; 152. 乘积最大子数组 题目传送门：152. 乘积最大子数组 - 力扣（LeetCode） 该题作为最大子数组和的一个扩展，还是非常有意思，多维护一个最小值就可以。 注意：最大值同理也可以状态转移到最小值，最小值同理也可以状态转移到最大值 (比如当前位置是一个负数，就可能发生上述转化) 这里面有一个很“有趣”的测试用例: 1[0,10,10,10,10,10,10,10,10,10,-10,10,10,10,10,10,10,10,10,10,0] 这个用例很有趣，最大会有 10^19 要存储，加上符号位是65位，刚好不够 long long 来存。而double虽然也是64位，但它的数据结构有所不同，采用 1符号+11指数+52 尾数的方式，最多可以存到 2^1024 的大数。虽然在 52位二进制数以上的精度不能保证，但这题由题目保证最多只用到32位，再多的只是为了满足不溢出。 所以，我们将临时变量由 int 改为 double，即可通过此用例。 最后实现代码如下： 1234567891011121314class Solution &#123;public: int maxProduct(vector&lt;int&gt;&amp; nums) &#123; double ans=nums[0], max_now=nums[0],min_now=nums[0]; int n = nums.size(); for(int i=1;i&lt;n;++i)&#123; double max_tmp = max_now, min_tmp = min_now; max_now = max(max_now*nums[i], max((double)nums[i],min_tmp*nums[i])); min_now = min(min_now*nums[i], min((double)nums[i],max_tmp*nums[i])); ans = max(ans,max_now); &#125; return int(ans); &#125;&#125;; 二、网格图 DP 2.0 题目清单 1. 基础 LCR 166. 珠宝的最高价值 - 力扣（LeetCode） 62. 不同路径 - 力扣（LeetCode） 63. 不同路径 II - 力扣（LeetCode） 64. 最小路径和 - 力扣（LeetCode） 120. 三角形最小路径和 - 力扣（LeetCode） 931. 下降路径最小和 - 力扣（LeetCode） 1573 2684. 矩阵中移动的最大次数 - 力扣（LeetCode） 1626 2.1 基础 LCR 166. 珠宝的最高价值 题目传送门：LCR 166. 珠宝的最高价值 - 力扣（LeetCode） 网格DP求最大值题，网格图DP基础板子题，从左侧和上侧进行状态转移即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: int jewelleryValue(vector&lt;vector&lt;int&gt;&gt;&amp; frame) &#123; int n = frame.size(), m = frame[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(m+1,0)); for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; dp[i][j] = max(dp[i][j-1], dp[i-1][j]) + frame[i-1][j-1]; &#125; &#125; return dp[n][m]; &#125;&#125;; 62. 不同路径 题目传送门：62. 不同路径 - 力扣（LeetCode） 网格DP方案数题，同上，网格图DP基础板子题，从左侧和上侧进行状态转移即可。 注意一下初始化即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: int uniquePaths(int m, int n) &#123; vector&lt;vector&lt;int&gt;&gt; dp(m+1,vector&lt;int&gt;(n+1, 0)); dp[1][1] = 1; for(int i=1;i&lt;=m;++i)&#123; for(int j=1;j&lt;=n;++j)&#123; dp[i][j] += dp[i-1][j] + dp[i][j-1]; &#125; &#125; return dp[m][n]; &#125;&#125;; 63. 不同路径 II 题目传送门：63. 不同路径 II - 力扣（LeetCode） 网格DP方案数题，在上一道题的基础上增加了障碍物的设定，只需要在每次循环的时候判定当前是不是障碍物即可，如果是就不更新。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; obstacleGrid) &#123; int n = obstacleGrid.size(), m = obstacleGrid[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(m+1,0)); // if (obstacleGrid[0][0]) return 0; dp[1][1] = obstacleGrid[0][0]==0?1:0; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; if (obstacleGrid[i-1][j-1]) continue; dp[i][j] += dp[i-1][j] + dp[i][j-1]; &#125; &#125; return dp[n][m]; &#125;&#125;; 64. 最小路径和 题目传送门：64. 最小路径和 - 力扣（LeetCode） 网格DP求最小值题，上面已经遇到了方案数量和求最大的设定，此题为一个求最小设定。 最后实现代码如下： 1234567891011121314class Solution &#123;public: int minPathSum(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(m+1, INT_MAX/2)); dp[0][1] = 0; dp[1][0] = 0; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i-1][j-1]; &#125; &#125; return dp[n][m]; &#125;&#125;; 120. 三角形最小路径和 题目传送门：120. 三角形最小路径和 - 力扣（LeetCode） 网格DP求最小值题，上一道题的简单改版，从矩形变成了三角形，最后判断一下最后一行最小的值即可。 备注：顺便可以看一下min_element的用法 最后实现代码如下： 1234567891011121314class Solution &#123;public: int minimumTotal(vector&lt;vector&lt;int&gt;&gt;&amp; triangle) &#123; int n = triangle.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(n+1, (INT_MAX/2))); dp[1][1] = triangle[0][0]; for(int i=2;i&lt;=n;++i)&#123; for(int j=1;j&lt;=i;++j)&#123; dp[i][j] = triangle[i-1][j-1] + min(dp[i-1][j-1] , dp[i-1][j]); &#125; &#125; return *min_element(dp[n].begin(), dp[n].end()); &#125;&#125;; 931. 下降路径最小和 题目传送门：931. 下降路径最小和 - 力扣（LeetCode） 网格DP求最小值题，同上，从上一行相邻三个转移，使用上一行三个值中的最小值加上当前位置值即可。 最后实现代码如下： 1234567891011121314class Solution &#123;public: int minFallingPathSum(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; int n = matrix.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(n+2, INT_MAX/2)); for(int i=1;i&lt;=n;++i) dp[0][i] = 0; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=n;++j)&#123; dp[i][j] = min(dp[i-1][j], min(dp[i-1][j-1], dp[i-1][j+1])) + matrix[i-1][j-1]; &#125; &#125; return *min_element(dp[n].begin()+1, dp[n].begin()+1+n); &#125;&#125;; 2684. 矩阵中移动的最大次数 题目传送门：2684. 矩阵中移动的最大次数 - 力扣（LeetCode） 网格DP求最大值题，同上，只不过需要在更新的过程中时刻维护一个全局步数的最大值即可。 最后实现代码如下： 1"},{"title":"单调栈","path":"/wiki/LeetCode/专项训练/单调栈.html","content":"基础 本板块整理自灵茶山艾府b站视频单调栈【基础算法精讲 26】_哔哩哔哩_bilibili 通过一道例题进行讲解： 题目传送门：[739.每日温度 - 力扣（LeetCode）](https://leetcode.cn/problems/daily-temperatures/) 朴素算法就是，我们直接考虑对于每个元素遍历，找到他右边第一个比他大的数，但是这样明显是一个O(n2)O(n^2)O(n2)复杂度的算法，如何优化呢？ 首先不难想到的是我们倒着进行遍历，如果我们的温度序列是 1,6,3,5,5,2,1,61,6,3,5,5,2,1,61,6,3,5,5,2,1,6,并且我们已经遍历完了最后的4个数,那么我们不难推出，再后续的遍历中比当前值大的数不可能是2，1（因为前面有一个5更大）。 总结一下当前情况是，由于5的出现，右侧2，1一定不会成为左侧某个数下一个更大的数了。所以再后续的遍历中我们就可以把这两个数去掉，不去遍历他们两个了。 以上就是单调栈算法的核心思想了 下面介绍两种常见的单调栈写法： 从右到左（将下一个最大数存入栈中） 此时维护的栈是一个顶小底大的栈： 1234567891011121314151617class Solution &#123;public: vector&lt;int&gt; dailyTemperatures(vector&lt;int&gt;&amp; temperatures) &#123; int n = temperatures.size(); vector&lt;int&gt; ans(n); stack&lt;int&gt; st; for(int i=n-1;i&gt;=0;--i)&#123; int t = temperatures[i]; while (!st.empty()&amp;&amp;t &gt;= temperatures[st.top()]) st.pop(); if (!st.empty()) &#123; ans[i] = st.top() - i; &#125; st.push(i); &#125; return ans; &#125;&#125;; 从左到右（将还没有找到下一个最大的数存入栈中） 此时维护的栈仍是一个顶小底大的栈： 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; dailyTemperatures(vector&lt;int&gt;&amp; temperatures) &#123; int n = temperatures.size(); vector&lt;int&gt; ans(n); stack&lt;int&gt; st; for(int i=0;i&lt;n;++i)&#123; int t = temperatures[i]; while (!st.empty() &amp;&amp; t &gt; temperatures[st.top()]) &#123; ans[st.top()] = i-st.top(); st.pop(); &#125; st.push(i); &#125; return ans; &#125;&#125;; 单调栈题单 单调栈 每日温度（单调栈模板题） 商品折扣后的最终价格 下一个更大元素 I 下一个更大元素 II 链表中的下一个更大节点 1571 股票价格跨度 1709 表现良好的最长时间段 1908 132 模式 ~2000 美丽塔 II 2072 下一个更大元素 IV 2175 使数组按非递减顺序排列 2482 每个元素为最大值的最大范围（会员题） 矩形系列 柱状图中最大的矩形 最大矩形 统计全 1 子矩形 1845 字典序最小 去除重复字母 扩展：重复个数不超过 limit 移掉 K 位数字 ~1800 找出最具竞争力的子序列 1802 拼接最大数 贡献法（计算所有子数组的……的和） 子数组的最小值之和 1976 子数组范围和（最大值-最小值） O(n)\\mathcal{O}(n)O(n) 做法 ~2000 子数组最小乘积的最大值 2051 操作使得分最大 2397 巫师的总力量和（最小值*和） 2621"},{"title":"第三章 大语言模型资源","path":"/wiki/LLM/Basic/第三章 大语言模型资源.html","content":"3.1 公开可用的模型检查点或API 3.1.1 公开可用的通用大语言模型检查点 LLaMA和LLaMA-2：7B，13B，34B，70B ChatGLM：6B Falcon：7B，40B，180B Baichuan 和 Baichuan-2：7B，13B InternLM 和InternLM-2：7B，20B Qwen：从0.5B 到72B 的不同参数规模版本 Mistral：7B DeepSeek LLM：7B，67B Mixtral：MoE架构，46.7B，但处理时只会用到12.9B Gemma：2B和7B MiniCPM：2B YuLan-Chat：13B，65B 3.1.2 LLaMA 变体系列"},{"title":"第二章 基础介绍","path":"/wiki/LLM/Basic/第二章 基础介绍.html","content":"大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型 本部分将介绍大语言模型的构建过程、扩展法则（Scaling Law）、涌现能力（Emergent Abilities），然后将介绍GPT 系列模型的研发历程。 2.1 大语言模型的构建过程 Transformer结构 两步骤：大规模预训练，指令微调&amp;人类对齐 2.1.1 大规模预训练 解码器架构+预测下一个词 2.1.2 指令微调与人类对齐 指令微调（也叫做有监督微调，Supervised Fine-tuning, SFT） 人类对齐：将语言模型与人类价值观对齐 2.2 扩展法则 和小语言模型相似的结构，但是通过扩展参数规模、数据规模和计算算力，大语言模型的能力显著超越了小型语言模型的能力。 2.2.1 KM扩展法则（OpenAI） KM 扩展法则（Kaplan 等人, 2020）描述了神经语言模型的性能如何随模型规模（N）、数据规模（D）和计算算力（C） 变化，采用幂律公式刻画其关系： L(N)∼N−αN,L(D)∼D−αD,L(C)∼C−αCL(N) \\sim N^{-\\alpha_N}, \\quad L(D) \\sim D^{-\\alpha_D}, \\quad L(C) \\sim C^{-\\alpha_C} L(N)∼N−αN​,L(D)∼D−αD​,L(C)∼C−αC​ 其中，L(·) 代表交叉熵损失，实验表明模型扩展后，损失呈幂律下降。 核心结论 模型规模、数据量和算力三者紧密相关，增加任一因素都能改善性能，但需确保资源匹配。 幂律关系提供优化指导，用于合理分配计算资源，提升模型效果。 OpenAI 进一步拆分损失： 不可约损失（数据特性决定，无法优化）。 可约损失（可通过增加计算或优化算法减少）。 意义 该法则为大规模神经网络的优化提供了经验指导，说明如何在算力有限的情况下合理扩展模型规模和数据量以获得最佳性能。 2.2.2 Chinchilla 扩展法则（DeepMind） Chinchilla 扩展法则由 DeepMind 团队（2022）提出，旨在优化计算资源的利用，指导大语言模型如何在给定算力下合理分配模型规模（N）和数据规模（D），从而提高训练效率。 核心公式 Chinchilla 扩展法则给出的模型损失函数为： L(N,D)=E+ANα+BDβL(N, D) = E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta} L(N,D)=E+NαA​+DβB​ 其中，EEE、AAA、BBB 为实验拟合参数，α=0.34\\alpha = 0.34α=0.34，β=0.28\\beta = 0.28β=0.28。 在约束条件 C≈6NDC \\approx 6NDC≈6ND 下，最优的 N 和 D 分配方案： Nopt(C)=G(C6)a,Dopt(C)=G−1(C6)bN_{\\text{opt}}(C) = G \\left(\\frac{C}{6}\\right)^a, \\quad D_{\\text{opt}}(C) = G^{-1} \\left(\\frac{C}{6}\\right)^b Nopt​(C)=G(6C​)a,Dopt​(C)=G−1(6C​)b 其中，a=αα+βa = \\frac{\\alpha}{\\alpha + \\beta}a=α+βα​，b=βα+βb = \\frac{\\beta}{\\alpha + \\beta}b=α+ββ​，用于计算模型参数与数据的最佳配比。 与 KM 扩展法则的对比 研究发现，Chinchilla 扩展法则和 KM 扩展法则均可近似表示为计算力主导的幂律关系： Nopt∼Ca,Dopt∼CbN_{\\text{opt}} \\sim C^a, \\quad D_{\\text{opt}} \\sim C^b Nopt​∼Ca,Dopt​∼Cb 但二者在数据与参数的扩展趋势上有所不同： KM 扩展法则（a≈0.73,b≈0.27a \\approx 0.73, b \\approx 0.27a≈0.73,b≈0.27）：更倾向于增加模型参数 NNN。 Chinchilla 扩展法则（a≈0.46,b≈0.54a \\approx 0.46, b \\approx 0.54a≈0.46,b≈0.54）：主张模型参数与数据规模接近等比例增长。 重要意义 优化算力资源分配：以往预训练偏向扩大模型规模，忽视数据量，如 GPT-3（175B 参数）仅用 300B 词元训练，数据量远未达到模型性能饱和点。Chinchilla 研究表明，适量增加数据比单纯增大模型更有效，如 Chinchilla（70B 参数）使用 1.4T 词元 训练。 挑战“更大模型=更好”假设：LLama-2（7B）等新模型趋势表明，高质量数据+适当规模的模型，而非单纯增大参数，才是提升性能的关键。 Transformer 架构的局限：当前尚无充分实验支持无限扩大模型规模能继续提高性能，而适当扩展数据可能更具性价比。 总结 Chinchilla 扩展法则提出了一种更均衡的参数与数据增长策略，突破了大模型仅靠参数扩展的惯性思维，为优化大语言模型训练资源的分配提供了更科学的指导。 但是扩展法则意义不大（单纯纠结数字，）因为小模型在数据量更大的情况效果也会更好。 例如，LLaMA-2 (7B) 的模型就使用了2T 的词元进行训练，很多更小的模型也能够通过使用超大规模的预训练数据获得较大的模型性能提升。这种现象的一个重要原因是由于Transformer 架构具有较好的数据扩展性，到目前为止，还没有实验能够有效验证特定参数规模语言模型的饱和数据规模（即随着数据规模的扩展，模型性能不再提升）。 2.2.3 关于扩展法则的讨论 可预测的扩展： 较小算力资源预测较大资源投入后的模型性能 任务层面的可预测性 2.3 涌现能力 在小型模型中不存在但在大模型中出现的能力 具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。 2.3.1 代表性的涌现能力 上下文学习： 在提示中为语言模型提供自然语言指令和多个任务示例（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。 指令遵循： 指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务[28, 39, 40] 逐步推理： 大语言模型则可以利用思维链（Chain-of-Thought, CoT）提示策略[25] 来加强推理性能。具体来说，大语言模型可以在提示中引入任务相关的中间推理步骤来加强复杂任务的求解，从而获得更为可靠的答案。 2.3.2 涌现能力与扩展法则的关系 扩展法则：语言建模损失 涌现能力：任务性能衡量模型性能 顿悟：持续训练测试误差突然下降 2.4 GPT 系列模型的技术演变 训练能够准确预测下一个词的Transformer （只包含解码器）语言模型 扩展语言模型的规模以及扩展预训练数据的规模。 2.4.1 早期探索阶段 GPT-1：Generative Pre-Training（仅有解码器的）参数100M GPT-2：GPT-2 沿用了GPT-1 的类似架构，将参数规模扩大到1.5B，并使用 大规模网页数据集WebText 进行预训练。 2.4.2 规模扩展 GPT-3：175B，100倍的参数扩张，提出上下文学习概念（建立以提示学习方法为基础技术路线的任务求解范式） 2.4.3 能力增强 代码数据训练：在代码数据上进行训练有助于提高GPT模型的综合性能 人类对齐：使用人类反馈、协助人类评估、进行对齐研究 2.4.4 性能跃升 ChatGPT GPT-4：单模态变多模态，干预策略缓解幻觉，隐私泄漏等问题，引入红队攻击减少生成有毒有害内容。 GPT-4V，GPT-4 Turbo以及多模态支持模型"},{"title":"第 400 场周赛","path":"/wiki/LeetCode/周赛/第 400 场周赛.html","content":"总结 最值得做的还是最后一题，最后一题使用集合论的思想非常的有意思，值得总结。 Q1：3168. 候诊室中的最少椅子数 题目传送门：3168. 候诊室中的最少椅子数 - 力扣（LeetCode） 周常签到题目，遍历一遍即可。 最后实现代码如下： 123456789101112class Solution &#123;public: int minimumChairs(string s) &#123; int ans = 0, tmp = 0; for (char &amp;c : s)&#123; if (c == &#x27;E&#x27;) tmp++; else tmp--; ans = max(ans, tmp); &#125; return ans; &#125;&#125;; Q2：3169. 无需开会的工作日 题目传送门：3169. 无需开会的工作日 - 力扣（LeetCode） 对区间进行一个从小到大的排序，然后合并区间的途中计算区间覆盖的天数，最后使用总天数-开会天数得到最终的空闲天数。 最后实现代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int countDays(int days, vector&lt;vector&lt;int&gt;&gt;&amp; meetings) &#123; int n = meetings.size(); sort(meetings.begin(), meetings.end(), [](const vector&lt;int&gt;&amp; a, const vector&lt;int&gt;&amp; b)&#123; if (a[0]!=b[0]) return a[0] &lt; b[0]; return a[1] &lt; b[1]; &#125;); int meetdays = 0; int start = meetings[0][0], end = meetings[0][1]; for (int i=1;i&lt;n;++i) &#123; if (meetings[i][0] &gt;= start &amp;&amp; meetings[i][0] &lt;= end) &#123; end = max(meetings[i][1], end); &#125; else &#123; meetdays += (end - start + 1); start = meetings[i][0]; end = meetings[i][1]; &#125; &#125; meetdays += (end-start + 1); return days - meetdays; &#125;&#125;; Q3：3170. 删除星号以后字典序最小的字符串 题目传送门：3170. 删除星号以后字典序最小的字符串 - 力扣（LeetCode） 哈希表，使用一张哈希表存储每种字符（26个）出现的位置，遍历字符串，发现*时，从小到大遍历哈希表，从哈希表中删除最小字符最后出现的位置即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728class Solution &#123;public: string clearStars(string s) &#123; int n = s.size(); vector&lt;bool&gt; vis(100003,true); vector&lt;vector&lt;int&gt;&gt; mp(26); for (int i=0; i&lt;n; ++i) &#123; char c = s[i]; if (c != &#x27;*&#x27;) mp[c-&#x27;a&#x27;].push_back(i); else &#123; vis[i] = false; for (auto &amp;loc : mp) &#123; if (loc.size()) &#123; int nn = loc.size(); vis[loc[nn-1]] = false; loc.pop_back(); break; &#125; &#125; &#125; &#125; string ans = &quot;&quot;; for(int i=0; i&lt;n; ++i )&#123; if (vis[i]) ans += s[i]; &#125; return ans; &#125;&#125;; Q4：3171. 找到按位或最接近 K 的子数组 题目传送门：3171. 找到按位或最接近 K 的子数组 - 力扣（LeetCode） 题解参考”灵茶山艾府“ 暴力算法非常容易想到，双层循环计算所有可能的或即可。 从左到右正向遍历 numsnumsnums，对于 x=nums[i]x = nums[i]x=nums[i]，从 i−1i - 1i−1 开始倒着遍历 nums[j]nums[j]nums[j]，更新 nums[j]=nums[j]∣xnums[j] = nums[j] | xnums[j]=nums[j]∣x。 i=1i = 1i=1 时，我们会把 nums[0]nums[0]nums[0] 到 nums[1]nums[1]nums[1] 的 OR 记录在 nums[0]nums[0]nums[0] 中。 i=2i = 2i=2 时，我们会把 nums[1]nums[1]nums[1] 到 nums[2]nums[2]nums[2] 的 OR 记录在 nums[1]nums[1]nums[1] 中，nums[0]nums[0]nums[0] 到 nums[2]nums[2]nums[2] 的 OR 记录在 nums[0]nums[0]nums[0] 中。 i=3i = 3i=3 时，我们会把 nums[2]nums[2]nums[2] 到 nums[3]nums[3]nums[3] 的 OR 记录在 nums[2]nums[2]nums[2] 中；nums[1]nums[1]nums[1] 到 nums[3]nums[3]nums[3] 的 OR 记录在 nums[1]nums[1]nums[1] 中；nums[0]到nums[3]nums[0] 到 nums[3]nums[0]到nums[3] 的 OR 记录在 nums[0]nums[0]nums[0] 中。 按照该算法，可以计算出所有子数组的 OR。注意单个元素也算子数组。 O(n2)O(n^2)O(n2)复杂度，很明显会超时，所以需要优化。 我们将这个问题看成一个集合的问题，那么或运算就可以当作是集合求并集操作。 进行接下来的推理需要牢记下面这个定理： 对于两个二进制数aaa,bbb,如果a∣b=aa|b=aa∣b=a,从集合的角度上看，bbb对应的集合是aaa对应的集合的子集。 于是我们可以对上面的暴力解法做出如下优化： 仍然是从左到右正向遍历nums，对于x=nums[i]，从i-1开始倒着遍历nums[j]: nums[j]∣x≠nums[j]nums[j]|x eq nums[j]nums[j]∣x=nums[j]，说明nums[j]的集合可以继续扩展，则更新nums[j]=nums[j]∣xnums[j] = nums[j]|xnums[j]=nums[j]∣x 否则nums[j]∣x=nums[j]nums[j]|x=nums[j]nums[j]∣x=nums[j],此时我们可以得出结论，xxx不仅是nums[j]nums[j]nums[j]的子集，也是nums[k](k&lt;j)nums[k](k&lt;j)nums[k](k&lt;j)的子集（可以通过改变求并集顺序证明），发现等于后后面的计算就可以不用进行了，因为结果都一样，提前退出循环。 在循环中记得用∣nums[j]−k∣|nums[j]-k|∣nums[j]−k∣更新答案的最小值 最后实现代码如下： 1234567891011121314151617class Solution &#123;public: int minimumDifference(vector&lt;int&gt;&amp; nums, int k) &#123; int n = nums.size(); int ans = abs(k-nums[0]); for(int i=0;i&lt;n;++i)&#123; ans = min(ans, abs(nums[i]-k)); for(int j=i-1;j&gt;=0;--j)&#123; int tmp = nums[j] | nums[i]; if (nums[j]==tmp) break; ans = min(abs(tmp-k), ans); nums[j] = tmp; &#125; &#125; return ans; &#125;&#125;; 时间复杂度: O(nlogU)O(n log U)O(nlogU)，其中 nnn 是 numsnumsnums 的长度，U=max(nums)U = max(nums)U=max(nums)。由于 229−1&lt;109&lt;230−12^{29} - 1 &lt; 10^9 &lt; 2^{30} - 1229−1&lt;109&lt;230−1，二进制数对应集合的大小不会超过 292929，因此在 OR 运算下，每个数字至多可以增加 292929 次。总体上看，二重循环的总循环次数等于每个数字可以增大的次数之和，即 O(nlogU)O(n log U)O(nlogU)。"},{"title":"第 401 场周赛","path":"/wiki/LeetCode/周赛/第 401 场周赛.html","content":"总结 第四题作为第三题的延申，学习到了使用bitset对DP问题进行优化的技巧 Q1：3178. 找出 K 秒后拿着球的孩子 题目传送门：3178. 找出 K 秒后拿着球的孩子 - 力扣（LeetCode） 签到题。 最后实现代码如下： 1234567891011class Solution &#123;public: int numberOfChild(int n, int k) &#123; int res = k % ((n-1)*2); if (res &gt; n-1) &#123; res -= (n-1); return n-1-res; &#125; return res; &#125;&#125;; Q2：3179. K 秒后第 N 个元素的值 题目传送门：3179. K 秒后第 N 个元素的值 - 力扣（LeetCode） 签到题*2 最后实现代码如下： 123456789101112131415class Solution &#123;public: int valueAfterKSeconds(int n, int k) &#123; vector&lt;int&gt; a(n,1); int MOD = 1e9 + 7; for(int i=1;i&lt;=k;++i)&#123; int nw = 0; for(int j=0;j&lt;n;++j)&#123; a[j] = (a[j] + nw)%MOD; nw = a[j]; nw = nw % MOD; &#125; &#125; return a[n-1]; &#125;&#125;; Q3：3180. 执行操作可获得的最大总奖励 I 题目传送门：3180. 执行操作可获得的最大总奖励 I - 力扣（LeetCode） 有点意思的一道题,对于rewardValues中的数，如果先选大的，就没法再选小的，所以按照从小到大的顺序选是最好的。 排序后问题变为了一个01背包问题，定义dp[i]dp[i]dp[i]表示的是能否选到奖励为i，初始条件dp[0]=true。 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: int maxTotalReward(vector&lt;int&gt;&amp; rewardValues) &#123; sort(rewardValues.begin(),rewardValues.end()); vector&lt;bool&gt; dp(300000,false);dp[0]=true; int maxx = 0; for(auto rewardValue : rewardValues)&#123; for(int i=min(maxx,rewardValue-1);i&gt;=0;--i)&#123; if (dp[i]) &#123; dp[rewardValue+i] = true; maxx = max(rewardValue+i,maxx); &#125; &#125; &#125; for(int i=maxx;i&gt;=0;--i)&#123; if (dp[i]) return i; &#125; return 0; &#125;&#125;; Q4：3181. 执行操作可获得的最大总奖励 II 题目传送门：3181. 执行操作可获得的最大总奖励 II - 力扣（LeetCode） 该题相比上一题就是数据范围变大了一些，从2000变为了50000，因此上一题的做法无法直接用进来，需要一些优化。观察到我们的DP数组是一个bool数组，因此考虑可以用bitset进行优化。 将bool DP数组转化为一个二进制数后，选择数V相当于把当前二进制后V项左移V位和当前项进行或运算 因为Python对大数支持很好所以先展示Python代码： 12345678class Solution: def maxTotalReward(self, rewardValues: List[int]) -&gt; int: rewardValues.sort() dp = 1 for rewardValue in rewardValues: mask = (1&lt;&lt;rewardValue) - 1 # 生成mask选取最后v个项 dp |= (dp&amp;mask)&lt;&lt;rewardValue return dp.bit_length() - 1 CPP由于不支持大数不能像Python一样实现的这么直接，需要使用bitset库，bitset库详细用法见C++教程栏目。 此外还需注意的是，对于mask的生成不能像Python一样先定义一个全1 mask进行求并集运算，而是要采用先左移后右移的方式。 CPP最后实现代码如下： 1234567891011121314class Solution &#123;public: int maxTotalReward(vector&lt;int&gt;&amp; rewardValues) &#123; sort(rewardValues.begin(), rewardValues.end()); bitset&lt;100000&gt; dp(1); for (auto rewardValue : rewardValues) &#123; int shift = dp.size() - rewardValue; dp |= (dp&lt;&lt;shift&gt;&gt;shift)&lt;&lt;rewardValue; &#125; for(int i=rewardValues.back()*2-1;;i--)&#123; if(dp.test(i)) return i; &#125; &#125;&#125;; 时间复杂度 时间复杂度上，使用bitset优化的DP相比原来的数组DP能够提供32倍到64倍速度上的优化（取决于机器是32位还是64位的）"},{"title":"第 402 场周赛","path":"/wiki/LeetCode/周赛/第 402 场周赛.html","content":"总结 成功AK了，最后一题是一道数据结构板子题，但是没参赛有点可惜。 Q1：3184. 构成整天的下标对数目 I 题目传送门：3184. 构成整天的下标对数目 I - 力扣（LeetCode） 签个到 最后实现代码如下： 123456789101112class Solution &#123;public: int countCompleteDayPairs(vector&lt;int&gt;&amp; hours) &#123; int n = hours.size(), ans=0; for(int i=0;i&lt;n;++i)&#123; for(int j=i+1;j&lt;n;++j)&#123; if((hours[i]+hours[j])%24==0) ans++; &#125; &#125; return ans; &#125;&#125;; Q2：3185. 构成整天的下标对数目 II 题目传送门：3185. 构成整天的下标对数目 II - 力扣（LeetCode） 哈希一下，签到×2 最后实现代码如下： 1234567891011121314class Solution &#123;public: long long countCompleteDayPairs(vector&lt;int&gt;&amp; hours) &#123; int n = hours.size(); for(int i=0;i&lt;n;++i) hours[i] = hours[i] % 24; long long ans = 0; vector&lt;long long&gt; dp(24,0); for(int i=0;i&lt;n;++i)&#123; ans += dp[(24-hours[i])%24]; dp[hours[i]]++; &#125; return ans; &#125;&#125;; Q3：3186. 施咒的最大总伤害 题目传送门：3186. 施咒的最大总伤害 - 力扣（LeetCode） 打家劫舍超级加强版，使用一个数组预处理一下就好（排序后，相同的合并），详细实现见代码。 最后实现代码如下： 1234567891011121314151617181920212223class Solution &#123;public: long long maximumTotalDamage(vector&lt;int&gt;&amp; power) &#123; int n = power.size(); sort(power.begin(), power.end()); vector&lt;pair&lt;long long,long long&gt;&gt; nw_power; int prev = -1,m=0; for(int i=0;i&lt;n;++i)&#123; if (power[i]==prev) nw_power[m-1].second += power[i]; else nw_power.emplace_back(power[i], power[i]), prev = power[i], m++; &#125; vector&lt;long long&gt; dp(m+3,0); dp[1] = nw_power[0].second; for(int i=2;i&lt;=m;++i)&#123; long long nw_pw = nw_power[i-1].first; long long val = nw_power[i-1].second; int j = i-2; while(j&gt;=0&amp;&amp;(nw_power[j].first==nw_pw-1||nw_power[j].first==nw_pw-2)) j--; dp[i] = max(dp[i-1], dp[j+1]+val); &#125; return dp[m]; &#125;&#125;; Q4：3187. 数组中的峰值 题目传送门：3187. 数组中的峰值 - 力扣（LeetCode） 线段树大法好！这里看着就非常像线段树板题，但是还是有区别，需要注意的是合并区间的时候（左侧区间右端点或者右侧区间左端点）可能会产生新的峰值，需要写一个函数进行判定。这个函数的写法也有讲究，因为在最后合并小区间的时候，不同区间的长度可能决定了两个端点能否当端点，举个例子： 左 3， 右 2 进行合并和 左1 3 右2进行合并，产生的峰值数是不同的。 因此该函数需要传入合并时总区间的左侧和右侧边界位置： 123456int judge(int k,int l,int r,vector&lt;int&gt;&amp; arr)&#123;\tint loc = node[k*2].right;\tif (loc-1&gt;=l&amp;&amp;arr[loc]&gt;arr[loc-1]&amp;&amp;arr[loc]&gt;arr[loc+1]) return 1;\tif (loc+2&lt;=r&amp;&amp;arr[loc+1]&gt;arr[loc]&amp;&amp;arr[loc+1]&gt;arr[loc+2]) return 1;\treturn 0;&#125; 查询的时候也和传统的线段树不太相同，需要特别注意查询传入的区间左右位置： 123456789int queryAns(int k, int l,int r,vector&lt;int&gt;&amp; arr)&#123;\tif (l==node[k].left&amp;&amp;node[k].right==r) return node[k].val;\tint mid = node[k].left + (node[k].right-node[k].left)/2;\tint ans = 0;\tif (r&lt;=mid) ans = queryAns(k*2,l,r,arr);\telse if (l&gt;mid) ans = queryAns(k*2+1,l,r,arr);\telse ans = queryAns(k*2,l,mid,arr)+queryAns(k*2+1,mid+1,r,arr)+judge(k,l,r,arr);\treturn ans;&#125; 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution &#123;private: struct TreeNode&#123; int val, left, right; TreeNode() : val(0), left(-1), right(-1) &#123;&#125; &#125;; vector&lt;TreeNode&gt; node; int judge(int k,int l,int r,vector&lt;int&gt;&amp; arr)&#123; int loc = node[k*2].right; if (loc-1&gt;=l&amp;&amp;arr[loc]&gt;arr[loc-1]&amp;&amp;arr[loc]&gt;arr[loc+1]) return 1; if (loc+2&lt;=r&amp;&amp;arr[loc+1]&gt;arr[loc]&amp;&amp;arr[loc+1]&gt;arr[loc+2]) return 1; return 0; &#125; void update(int k,vector&lt;int&gt;&amp; arr)&#123; node[k].val = node[k*2].val + node[k*2+1].val + judge(k,node[k].left,node[k].right,arr); &#125; void buildTree(int k,int l,int r,vector&lt;int&gt;&amp; arr)&#123; node[k].left = l; node[k].right = r; if(l==r) return; int mid = l + (r-l)/2; buildTree(k*2,l,mid,arr); buildTree(k*2+1,mid+1,r,arr); update(k, arr); &#125; int queryAns(int k, int l,int r,vector&lt;int&gt;&amp; arr)&#123; if (l==node[k].left&amp;&amp;node[k].right==r) return node[k].val; int mid = node[k].left + (node[k].right-node[k].left)/2; int ans = 0; if (r&lt;=mid) ans = queryAns(k*2,l,r,arr); else if (l&gt;mid) ans = queryAns(k*2+1,l,r,arr); else ans = queryAns(k*2,l,mid,arr)+queryAns(k*2+1,mid+1,r,arr)+judge(k,l,r,arr); return ans; &#125; void modify(int k,int index,int val,vector&lt;int&gt;&amp; arr)&#123; if(node[k].left == node[k].right &amp;&amp; node[k].left == index) &#123; arr[index] = val; return;&#125; int mid = node[k].left + (node[k].right-node[k].left)/2; if(index&lt;=mid) modify(k*2,index,val,arr); else modify(k*2+1,index,val,arr); update(k, arr); &#125;public: Solution() : node(300005) &#123;&#125; vector&lt;int&gt; countOfPeaks(vector&lt;int&gt;&amp; nums, vector&lt;vector&lt;int&gt;&gt;&amp; queries) &#123; int root = 1, n = nums.size(); buildTree(root, 0, n-1, nums); vector&lt;int&gt; ans; for(auto query : queries)&#123; if(query[0]==1) ans.push_back(queryAns(root, query[1], query[2], nums)); else modify(root, query[1], query[2], nums); &#125; return ans; &#125;&#125;; 这也是第一次个人在类里实现平级函数（之前都是使用的函数嵌套）"},{"title":"第 405 场周赛","path":"/wiki/LeetCode/周赛/第 405 场周赛.html","content":"总结 前三题比较基础，第三题是一个前缀和，因为之前见过所以很快就过了。最后一题弃疗了，看上去像一个DP优化问题，但是确实实力限制没有想出来，准备听一下讲解。 Q1：100339. 找出加密后的字符串 题目传送门：找出加密后的字符串 - 力扣（LeetCode） 签到题 最后实现代码如下： 1234567891011class Solution &#123;public: string getEncryptedString(string s, int k) &#123; int n = s.size(); string ans = s; for(int i=0;i&lt;n;++i)&#123; ans[i] = s[(i+k)%n]; &#125; return ans; &#125;&#125;; Q2：100328. 生成不含相邻零的二进制字符串 题目传送门：生成不含相邻零的二进制字符串 - 力扣（LeetCode） 本来以为是一道DP（但应该也可以吧），一看数据范围果断暴搜。 最后实现代码如下： 1234567891011121314class Solution &#123;public: vector&lt;string&gt; validStrings(int n) &#123; vector&lt;string&gt; ans; function&lt;void(int,int,string)&gt; dfs = [&amp;](int now,int last,string tmp)&#123; if(now==n) &#123;ans.push_back(tmp);return;&#125; dfs(now+1,1,tmp+&#x27;1&#x27;); if(last==1) dfs(now+1,0,tmp+&#x27;0&#x27;); &#125;; dfs(0,1,&quot;&quot;); return ans; &#125;&#125;; Q3：100359. 统计 X 和 Y 频数相等的子矩阵数量 题目传送门：统计 X 和 Y 频数相等的子矩阵数量 - 力扣（LeetCode） 前缀和题目 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int numberOfSubmatrices(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(m+1, 0)); vector&lt;vector&lt;bool&gt;&gt; vis(n+1,vector&lt;bool&gt;(m+1, false)); int ans = 0; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; int nw = grid[i-1][j-1]==&#x27;.&#x27;?0:(grid[i-1][j-1]==&#x27;X&#x27;?1:-1); dp[i][j] = dp[i-1][j] + dp[i][j-1] - dp[i-1][j-1] + nw; vis[i][j] = vis[i-1][j] | vis[i][j-1] | (nw!=0); if (dp[i][j]==0&amp;&amp;vis[i][j]) ans++; &#125; &#125; return ans; &#125;&#125;; Q4：100350. 最小代价构造字符串 题目传送门：最小代价构造字符串 - 力扣（LeetCode） 做不来呜呜呜😭 官方给出的解法是使用字典树，但是灵神把他Hack了。 字典树做法核心思想就是用字典树这个数据结构优化我们每次的字符串匹配问题 正解是使用字符串哈希 最后实现代码如下： 1"},{"title":"第 406 场周赛","path":"/wiki/LeetCode/周赛/第 406 场周赛.html","content":"总结 彻底失败好吧，后面两个题确实没考虑清楚。这下要掉大分咯😭😭😭 Q1：100352. 交换后字典序最小的字符串 题目传送门：100352. 交换后字典序最小的字符串 - 力扣（LeetCode） 日常签到题 最后实现代码如下： 12345678910111213141516class Solution &#123;public: string getSmallestString(string s) &#123; int n = s.size(); for(int i=1;i&lt;n;++i)&#123; int a = s[i-1]-&#x27;0&#x27;, b = s[i]-&#x27;0&#x27;; if (a%2==b%2&amp;&amp;b&lt;a) &#123; char tmp = s[i]; s[i] = s[i-1]; s[i-1] = tmp; break; &#125; &#125; return s; &#125;&#125;; Q2：100368. 从链表中移除在数组中存在的节点 题目传送门：100368. 从链表中移除在数组中存在的节点 - 力扣（LeetCode） 链表题，链表好久没写了还卡了一下😭 最后实现代码如下： 1234567891011121314151617181920212223242526/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* modifiedList(vector&lt;int&gt;&amp; nums, ListNode* head) &#123; vector&lt;bool&gt; vis(100003, false); for(auto num : nums) vis[num] = true; ListNode *hhead = new ListNode(-1, head); ListNode *tmp = head; ListNode *prev = hhead; while (tmp!=nullptr) &#123; while (tmp!=nullptr&amp;&amp;vis[tmp-&gt;val]) tmp = tmp-&gt;next; prev-&gt;next = tmp; prev = tmp; if (tmp!=nullptr)tmp = tmp-&gt;next; &#125; return hhead-&gt;next; &#125;&#125;; 以上为考试时实现的代码，第22行需要注意要判断一下，不然会有nullptr去指下一个的情况。 上面的代码还是太丑了，copy一个标准答案，以后可以这么写逻辑会更清晰一些。 12345678910111213141516class Solution &#123;public: ListNode* modifiedList(vector&lt;int&gt;&amp; nums, ListNode* head) &#123; unordered_set&lt;int&gt; st(nums.begin(), nums.end()); ListNode dummy(0, head); ListNode* cur = &amp;dummy; while (cur-&gt;next) &#123; // 从下一个节点开始看 if (st.contains(cur-&gt;next-&gt;val)) &#123; cur-&gt;next = cur-&gt;next-&gt;next; // 删除 &#125; else &#123; cur = cur-&gt;next; // 向后移动 &#125; &#125; return dummy.next; &#125;&#125;; 注意：LeetCode中的链表题都是没有哨兵节点的！！！ Q3：100361. 切蛋糕的最小总开销 I 题目传送门：100361. 切蛋糕的最小总开销 I - 力扣（LeetCode） 划分型问题，可以用二维DP思想来做，但是用到第四题会超时。 确实，听完了灵神的解析，此题就是一道贪心，考试的时候大体思路是对的，贪心有一个地方写错了导致此题没有做出来，确实也是考试的时候思考不够细致导致的，这个还需多加练习。 解决这题初步需要想到： 每条边都会被切完 横切的代价是，当前竖块数 * 横切消耗 竖切的代价是，当前横块数 * 竖切消耗 此题会使用交换论证法这种思想，具体证明还是推荐灵神的题解：贪心及其证明：交换论证法（Python/Java/C++/Go） 什么是交换论证法呢？具体而言就是我们将这个大问题先简化为讨论相邻两个决策的性质（讨论交换决策顺序对最终结果造成的影响），对于本题一个操作序列不难想到： 交换相邻两个横切操作，对最终答案不会有影响 同理，交换相邻两个竖切操作，对最终答案也不会造成影响 因此关键需要讨论交换两个不同的操作，我们设这两个操作的代价分别是：竖切costVcostVcostV和横切costHcostHcostH，在进行这两个操作之前，已经有cntHcntHcntH个横块，cntVcntVcntV个竖块。不难得知先执行横切再执行竖切则所需耗费的代价是： costH×cntV+costV×(cntH+1)costH \\times cntV + costV \\times (cntH+1) costH×cntV+costV×(cntH+1) 同理，先执行竖切再执行横切所耗费的代价是： costV×cntH+costH×(cntV+1)costV\\times cntH + costH\\times (cntV+1) costV×cntH+costH×(cntV+1) 如果先执行竖切再执行横切会有更小的代价,不难列出下面的式子： costV×cntH+costH×(cntV+1)&lt;costH×cntV+costV×(cntH+1)costV\\times cntH + costH\\times (cntV+1) &lt; costH \\times cntV + costV \\times (cntH+1) costV×cntH+costH×(cntV+1)&lt;costH×cntV+costV×(cntH+1) 化简得： costH&lt;costVcostH &lt; costV costH&lt;costV 这意味着，无论水平切割还是垂直切割，优先切开销较大的那条线，并且这个优先顺序与水平和垂直切割的次数（cntVcntVcntV,cntHcntHcntH）无关。换句话说，如果按照这个规则进行切割，将开销较大的操作移动到后面，会导致总开销更大。 这下这个问题转变为了一个使用双指针实现的贪心问题。 最后实现代码如下： 123456789101112131415161718192021222324252627class Solution &#123;public: int minimumCost(int m, int n, vector&lt;int&gt;&amp; horizontalCut, vector&lt;int&gt;&amp; verticalCut) &#123; int cnt_vert = 1,cnt_horz = 1,ans=0; priority_queue&lt;int&gt; horiz,verti; for(auto cut : horizontalCut) horiz.push(cut); for(auto cut : verticalCut) verti.push(cut); while (!horiz.empty()&amp;&amp;!verti.empty()) &#123; if (horiz.top() &lt; verti.top()) &#123; ans += verti.top()*cnt_horz; cnt_vert++; verti.pop(); &#125; else &#123; ans += horiz.top()*cnt_vert; cnt_horz++; horiz.pop(); &#125; &#125; while (!horiz.empty())&#123; ans += horiz.top()*cnt_vert; cnt_horz++; horiz.pop(); &#125; while (!verti.empty())&#123; ans += verti.top()*cnt_horz; cnt_vert++; verti.pop(); &#125; return ans; &#125;&#125;; Q4：100367. 切蛋糕的最小总开销 II 题目传送门：100367. 切蛋糕的最小总开销 II - 力扣（LeetCode） 同Q3 最后实现代码如下： 12345678910111213141516171819202122232425262728class Solution &#123;public: long long minimumCost(int m, int n, vector&lt;int&gt;&amp; horizontalCut, vector&lt;int&gt;&amp; verticalCut) &#123; int cnt_vert = 1,cnt_horz = 1; long long ans=0; priority_queue&lt;int&gt; horiz,verti; for(auto cut : horizontalCut) horiz.push(cut); for(auto cut : verticalCut) verti.push(cut); while (!horiz.empty()&amp;&amp;!verti.empty()) &#123; if (horiz.top() &lt; verti.top()) &#123; ans += verti.top()*cnt_horz; cnt_vert++; verti.pop(); &#125; else &#123; ans += horiz.top()*cnt_vert; cnt_horz++; horiz.pop(); &#125; &#125; while (!horiz.empty())&#123; ans += horiz.top()*cnt_vert; cnt_horz++; horiz.pop(); &#125; while (!verti.empty())&#123; ans += verti.top()*cnt_horz; cnt_vert++; verti.pop(); &#125; return ans; &#125;&#125;;"},{"title":"第 407 场周赛","path":"/wiki/LeetCode/周赛/第 407 场周赛.html","content":"总结 因为早上要开会，所以并没有参加该场周赛，有点可惜，下面就来做一下。 前三题还是比较简单的，但是前两题做的时候都先错了一遍非常的可惜，说明自己代码能力还是不够的熟练需要继续锻炼。 第四题差分数组题目，之前没有见过，但是感觉是一个模板可以记忆一下。 Q1：100372. 使两个整数相等的位更改次数 题目传送门：100372. 使两个整数相等的位更改次数 - 力扣（LeetCode） 锻炼一下二进制能力，签到题目。 最后实现代码如下： 123456789101112131415class Solution &#123;public: int minChanges(int n, int k) &#123; int ans = 0; while(n||k)&#123; int a = n&amp;1, b = k&amp;1; if (a^b)&#123; if (a==0) return -1; ans++; &#125; n = n&gt;&gt;1; k = k&gt;&gt;1; &#125; return ans; &#125;&#125;; Q2：100335. 字符串元音游戏 题目传送门：100335. 字符串元音游戏 - 力扣（LeetCode） 脑筋急转弯，做的时候还错了一次😂， 最后实现代码如下： 12345678910111213class Solution &#123;public: bool doesAliceWin(string s) &#123; int ans = 0; for(char c : s)&#123; if (c==&#x27;a&#x27;||c==&#x27;e&#x27;||c==&#x27;i&#x27;||c==&#x27;o&#x27;||c==&#x27;u&#x27;) ans++; &#125; if (ans==0) return false; if (ans==1) return true; if (ans==2) return true; return true; &#125;&#125;; Q3：100360. 将 1 移动到末尾的最大操作次数 题目传送门：100360. 将 1 移动到末尾的最大操作次数 - 力扣（LeetCode） 模拟题，从头开始遍历，记录1的个数，碰到0时，就要把前面所有的1往后移动直到碰到下一个1或者末尾。 最后实现代码如下： 123456789101112class Solution &#123;public: int maxOperations(string s) &#123; int cnt=0, ans=0; bool flag = true; for (char c : s) &#123; if(c==&#x27;1&#x27;) flag=true, cnt++; if(c==&#x27;0&#x27;&amp;&amp;flag) flag=false, ans += cnt; &#125; return ans; &#125;&#125;; Q4：100329. 使数组等于目标数组所需的最少操作次数 题目传送门：100329. 使数组等于目标数组所需的最少操作次数 - 力扣（LeetCode） 这个题做出来运气成分比较大，观察题目发现是对子数组（具有连续性）进行操作，就将差分数组就直接拿来用了。 核心思想：通过差分数组，把对子数组的操作，转换成两个位置的变化，计算target[i]-nums[i]的差分数组，分析这个差分数组需要的操作次数 整体步骤分为如下3步： 计算target[i]-nums[i]得到一个数组a，相当于我需要一个全为0的数组，操作成数组a 求a的差分数组，原来的每次子数组操作（一堆）转化成每次两个位置的操作 从左到右遍历差分数组，统计操作次数（统计相同符号的和即可） 为什么只统计相同符号的和即可？ 由于每次操作会产生一个 +1 和一个 −1，所以操作次数就等于所有正数 d[i] 之和。 最后实现的代码如下： 123456789101112131415class Solution &#123;public: long long minimumOperations(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; target) &#123; int n = nums.size(); vector&lt;int&gt; op_nums(n), op_diff(n+1,0); for (int i=0; i&lt;n; ++i) &#123; op_nums[i] = target[i] - nums[i]; &#125;op_diff[0] = op_nums[0]; for(int i=1;i&lt;n;++i) op_diff[i] = op_nums[i] - op_nums[i-1]; op_diff[n] = 0 - op_nums[n-1]; long long ans = 0; for(int i=0;i&lt;=n;++i)&#123; if (op_diff[i]&gt;0) ans += op_diff[i]; &#125; return ans; &#125;&#125;;"},{"title":"第 408 场周赛","path":"/wiki/LeetCode/周赛/第 408 场周赛.html","content":"总结 根据灵神总结的规律，果然这周的周赛非常的难，Q1周常签到题不多赘述，Q2先看出来特殊数是平方数，但是大意了没仔细想只是质数平方才行，因此WA了一发。Q3考试的时候感觉是一个O(nn)O(n\\sqrt{n})O(nn​)或者O(nlog⁡n)O(n\\log n)O(nlogn)的做法，感觉是暴力但是带优化（剪枝？）,但是没想出来😂。考试最后的时间主要在攻克Q4，看出来规律是判定连通块+判定圆和边界是否相交，但是最后由于采用的是DFS判连通，RE了导致没过。 本次周赛只通过了前两题，但依然和之前一样排名600+，说明最后两题是非常有含金量的可以好好总结一下。 Q1：3232. 判断是否可以赢得数字游戏 题目传送门：3232. 判断是否可以赢得数字游戏 - 力扣（LeetCode） 周常签到题，遍历一遍即可。 最后实现代码如下： 123456789101112class Solution &#123;public: bool canAliceWin(vector&lt;int&gt;&amp; nums) &#123; int sum_num = 0,sum_sing = 0; for(auto num : nums)&#123; sum_num += num; if (num &lt; 10) sum_sing += num; &#125; return sum_num - sum_sing == sum_sing?false:true; &#125;&#125;; Q2：3233. 统计不是特殊数字的数字数量 题目传送门：3233. 统计不是特殊数字的数字数量 - 力扣（LeetCode） 质数预处理+单次区间查询，这道题就开始上强度了，仔细理解题目后不难发现，满足特殊数字的要求是：当且仅当当前数是一个质数平方时，才能恰好有两个真因子。 质数预处理部分，由于数据范围是1e9，因此不能用暴力进行预处理，可以使用埃氏筛或者欧拉筛，质数筛法讲解详情见OI-Knowledge专题数学部分。 单次区间查询直接遍历一遍就可以了。 最后实现代码如下： 123456789101112131415161718192021222324252627282930class Solution &#123;public: int nonSpecialCount(int l, int r) &#123; int n = 100000; vector&lt;bool&gt; is_prime(n+1, true); vector&lt;int&gt; primes; for (int i = 2; i &lt;= n; ++i) &#123; if (is_prime[i]) &#123; primes.push_back(i); &#125; for (int j = 0; j &lt; primes.size() &amp;&amp; i * primes[j] &lt;= n; ++j) &#123; is_prime[i * primes[j]] = false; if (i % primes[j] == 0) &#123; break; &#125; &#125; &#125; vector&lt;long long&gt; speics; for(auto prime : primes)&#123; speics.push_back((long long)prime*prime); &#125; int all_num = r-l+1; int ll = 0; while (speics[ll]&lt;l)ll++; int rr = ll; while (speics[rr]&lt;=r)rr++; return all_num - (rr-ll); &#125;&#125;; Q3：3234. 统计 1 显著的字符串的数量 题目传送门：3234. 统计 1 显著的字符串的数量 - 力扣（LeetCode） 暴力优化，设字符串长为n，其中有cnt0个0，cnt1个1，要找的子串需要满足的条件是cnt0*cnt0 &lt;= cnt1,又已知cnt1 &lt;= n，所以不难推出cnt0 &lt;= sqrt(n)。 再次观察n的取值范围，可以发现n&lt;1e4可以承受一个sqrt(n)三次方时间复杂度的算法，不难想到优化暴力需要和0的个数牵扯上关系。 最后实现代码如下： 1 Q4： 题目传送门： 最后实现代码如下： 1"},{"title":"2023年11月每日一题","path":"/wiki/LeetCode/每日一题/2023年11月每日一题.html","content":"2023-11-22 题目传送门：2304. 网格中的最小路径代价 - 力扣（LeetCode） 一道比较简单的dp刷表题： 1234567891011121314151617181920212223242526#include&lt;bits/stdc++.h&gt;using namespace std;class Solution &#123;public: int minPathCost(vector&lt;vector&lt;int&gt;&gt;&amp; grid, vector&lt;vector&lt;int&gt;&gt;&amp; moveCost) &#123; int m = grid.size(), n = grid[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(m, vector&lt;int&gt;(n, 0)); for(int i=0;i&lt;n;++i) dp[0][i] = grid[0][i]; for(int i=1;i&lt;m;++i)&#123; for(int j=0;j&lt;n;++j)&#123; for(int k=0;k&lt;n;++k)&#123; if(k==0) dp[i][j] = dp[i-1][k] + moveCost[grid[i-1][k]][j] + grid[i][j]; else dp[i][j] = min(dp[i][j],dp[i-1][k] + moveCost[grid[i-1][k]][j] + grid[i][j]); &#125; &#125; &#125; int maxx = dp[m-1][0]; for(int i=0;i&lt;n;++i) maxx = min(maxx,dp[m-1][i]); return maxx; &#125;&#125;;int main()&#123; vector&lt;vector&lt;int&gt;&gt; grid = &#123;&#123;5,3&#125;,&#123;4,0&#125;,&#123;2,1&#125;&#125;,moveCost = &#123;&#123;9,8&#125;,&#123;1,5&#125;,&#123;10,12&#125;,&#123;18,6&#125;,&#123;2,4&#125;,&#123;14,3&#125;&#125;; cout&lt;&lt;Solution().minPathCost(grid,moveCost)&lt;&lt;endl; return 0;&#125; 2023-11-24 题目传送门：2824. 统计和小于目标的下标对数目 - 力扣（LeetCode） 非常基本的一道签到题目 123456789101112131415161718192021#include&lt;bits/stdc++.h&gt;using namespace std;class Solution &#123;public: int countPairs(vector&lt;int&gt;&amp; nums, int target) &#123; int n = nums.size(),cnt=0; for(int i=0;i&lt;n;++i)&#123; for(int j=i+1;j&lt;n;++j)&#123; if(nums[i]+nums[j] &lt; target) ++cnt; &#125; &#125; return cnt; &#125;&#125;;int main() &#123; vector&lt;int&gt; nums = &#123;-1, 1, 2, 3, 1&#125;; int target = 2; cout&lt;&lt;Solution().countPairs(nums, target); return 0;&#125;"},{"title":"2023年12月每日一题","path":"/wiki/LeetCode/每日一题/2023年12月每日一题.html","content":"2023-12-01 题目传送门：2661. 找出叠涂元素 - 力扣（LeetCode） 本题主要就是一个哈希表的思想，先使用一个map数据结构将每个元素和他的二维位置对应起来，这样可以实现O(1)查询，然后设置两个数组大小为每行每列，记录当前行列还剩的没染色的格子的数量，然后遍历arr中的每个元素，更新元素对应行列剩余未染色格子的个数。当发现当前元素对应行列有一个未染色格子剩余量为0时，输出即可。 1234567891011121314151617181920class Solution &#123;public: int firstCompleteIndex(vector&lt;int&gt;&amp; arr, vector&lt;vector&lt;int&gt;&gt;&amp; mat) &#123; int m = mat.size(), n =mat[0].size(); vector&lt;int&gt; row(m,n); vector&lt;int&gt; col(n,m); map&lt;int,pair&lt;int,int&gt;&gt; mp; for(int i=0;i&lt;m;++i)&#123; for(int j=0;j&lt;n;++j)&#123; mp[mat[i][j]] = pair&lt;int, int&gt;(i,j); &#125; &#125; for(int i=0;i&lt;m*n;++i)&#123; int x = mp[arr[i]].first, y = mp[arr[i]].second; row[x]--; col[y]--; if (row[x]==0||col[y]==0) return i; &#125; return m*n-1; &#125;&#125;; 2023-12-02 题目传送门：1094. 拼车 - 力扣（LeetCode） 一道非常经典的差分数组题，先对上下车站点进行排序，然后维护一个以站点为索引的差分数组，前缀和代表车到达当前站点时，车上的乘客数，乘客数大于capacity即不行，否则就可以。 1234567891011121314151617181920212223class Solution &#123;public: bool carPooling(vector&lt;vector&lt;int&gt;&gt;&amp; trips, int capacity) &#123; vector&lt;int&gt; pass_now(1001,0); // 这里需要排一个序 sort(trips.begin(),trips.end(),[](vector&lt;int&gt; &amp;a,vector&lt;int&gt; &amp;b)&#123; if(a[1] == b[1])return a[2]&lt;b[2]; else return a[1]&lt;b[1]; &#125;); int max_dis = 0; for(auto &amp;trip:trips)&#123; pass_now[trip[1]] += trip[0]; pass_now[trip[2]] -= trip[0]; max_dis = max(max_dis,trip[2]); &#125; int now = 0; for(int i=0;i&lt;=max_dis;i++)&#123; now += pass_now[i]; if(now&gt;capacity)return false; &#125; return true; &#125;&#125;; 2023-12-03 题目传送门：1423. 可获得的最大点数 - 力扣（LeetCode） 脑筋急转弯题目，既然要让我们求两边取的最大，那么就是让我们反过来取中间的最小。这样就从取不连续的数变成取连续的了。 123456789101112131415161718class Solution &#123;public: int maxScore(vector&lt;int&gt;&amp; cardPoints, int k) &#123; int sum_point = 0,n = cardPoints.size(); for(auto &amp;cardPoint:cardPoints)sum_point+=cardPoint; int res = n-k; int minn,capa=0,now=0; for(int i=0;i&lt;n;++i)&#123; if (capa&lt;res) now += cardPoints[i], capa += 1; else &#123; if(i == res) minn = now; now = now - cardPoints[i-res] + cardPoints[i]; minn = min(minn,now); &#125; &#125; return sum_point - minn; &#125;&#125;; 2023-12-04 题目传送门：1038. 从二叉搜索树到更大和树 - 力扣（LeetCode） 今天的题还是有点意思，借机学习一下C++指针版本的二叉树如何书写, 题目要求我们将每个节点的值替换为值大于等于当前节点值的和，那么这个时候我们可以稍微思考一下二叉树的性质，不难发现，我们进行反向中序遍历时，生成的遍历序列就是从大到小的一个排序。那么我们按照这个顺序更新每一个节点即可。 123456789101112131415161718192021222324/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125; * &#125;; */class Solution &#123;public: int sumnow = 0; TreeNode* bstToGst(TreeNode* root) &#123; if (root!=nullptr) &#123; if (root-&gt;right!=nullptr) bstToGst(root-&gt;right); sumnow += root-&gt;val; root-&gt;val = sumnow; if (root-&gt;left!=nullptr) bstToGst(root-&gt;left); &#125; return root; &#125;&#125;; 2023-12-05 题目传送门：2477. 到达首都的最少油耗 - 力扣（LeetCode） 今天的题目还是有点意思，感觉是之前一个周赛没做出来的题目，因为题目中还是有一些细节需要注意，这里就稍微梳理一下。 首先学习了C++内嵌函数的写法。 模板写法大概如下所示： 1function&lt;int(int, int)&gt; dfs = [&amp;](int u,int fa) -&gt; int &#123; 上面的代码，定义了一个名为 dfs 的函数，它接受两个整数参数 u 和 fa，返回一个整数值。这个函数是一个匿名函数（即没有名称的函数），它是一个模板函数，模板参数 int(int, int) 表示该函数接受两个整数参数并返回一个整数值。 代码中的 -&gt; 符号表示函数的返回类型，它位于函数定义的末尾。int(int, int) 是函数的参数类型，int 表示返回值类型，int 和 int 分别表示两个参数的类型。 [&amp;](int u,int fa) -&gt; int 表示这是一个左值引用捕获的匿名函数，它捕获了函数外部的变量 u 和 fa 的引用。这使得在函数内部可以修改这些变量的值，而不会影响到函数外的变量。 总之，这段代码定义了一个名为 dfs 的匿名函数，它接受两个整数参数 u 和 fa，返回一个整数值。记住以后都这么写就对啦。(还有需要注意的就是，最后&#125;后需要加;) 接下来就是本题的一些解题感想了，因为是树形结构，所以不难想到会用递归这种算法，在本题中递归我们回溯的时候传递子树大小。同时我们定义一个变量用于存储最终的耗油量， 本题大致思路就是，对于当前节点在使用递归已知其子树大小后，再用耗油量变量加上当前节点指向其父亲节点那条边上的耗油量，该耗油量计算方式如下： oil=ceil((size+1)/seats)oil = ceil((size+1)/seats) oil=ceil((size+1)/seats) 其中size为当前节点子树大小，ceil是向上取整操作 最后代码如下： 123456789101112131415161718192021222324252627class Solution &#123;public: long long minimumFuelCost(vector&lt;vector&lt;int&gt;&gt;&amp; roads, int seats) &#123; vector&lt;vector&lt;int&gt;&gt; edge(roads.size()+1); for(auto &amp;road:roads) &#123; int u = road[0],v = road[1]; edge[u].push_back(v); edge[v].push_back(u); &#125; long long ans = 0; function&lt;int(int, int)&gt; dfs = [&amp;](int u,int fa) -&gt; int &#123; if (edge[u].size()==1&amp;&amp;u!=0)&#123; ans += 1; return 1; &#125; int size = 0; for (auto &amp;v:edge[u])&#123; if (v == fa) continue; size += dfs(v,u); &#125; if (u!=0) ans += (size) / seats + 1; // ceil((size+1)/seats) return size+1; &#125;; dfs(0,-1); return ans; &#125;&#125;; 2023-12-06 题目传送门：2646. 最小化旅行的价格总和 - 力扣（LeetCode） 今天的题目还是有点有意思，一道树形DP记录一下。首先我们需要抓住一个要点就是，因为是在树上，所以路径是唯一的。所以解本题的第一步是统计走完所有的trip时，树上的每个节点被走过多少次。第一步就是一个简单的dfs搜索，当搜索到正确的路径后，回溯时将路径上每个节点经过的次数+1。 经过第一步后，我们就可以重新计算每个点的代价，就是用原来每个点的价格乘上次数。然后就进入一个标准的树形DP染色问题了。定义dp数组dp[i][j]其中i代表的是当前节点，j取值范围是0，1。0代表的是当前节点不优惠，1代表的是当前节点染色。状态转移方程如下所示： dp[u][0]=(∑min(dp[v][0],dp[v][1]))+cost[u]dp[u][0] = (\\sum min(dp[v][0],dp[v][1]))+ cost[u] dp[u][0]=(∑min(dp[v][0],dp[v][1]))+cost[u] dp[u][1]=(∑dp[v][0])+cost[u]/2dp[u][1] = (\\sum dp[v][0]) + cost[u]/2 dp[u][1]=(∑dp[v][0])+cost[u]/2 其中v是u的儿子 更新就是和传统树形DP一样是在进行递归后回溯的时候更新。 最终代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: int minimumTotalPrice(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges, vector&lt;int&gt;&amp; price, vector&lt;vector&lt;int&gt;&gt;&amp; trips) &#123; vector&lt;int&gt; nums(n+1,0); vector&lt;vector&lt;int&gt;&gt; edge(n+1); for(auto &amp;tmp:edges)&#123; int u = tmp[0], v = tmp[1]; edge[u].push_back(v); edge[v].push_back(u); &#125; function&lt;bool(int,int,int)&gt; dfs = [&amp;](int u,int fa,int tar) &#123; if (u == tar)&#123; nums[u]++; return true; &#125; for(auto &amp;v:edge[u])&#123; if (v==fa) continue; if (dfs(v,u,tar)) &#123; nums[u]+=1; return true; &#125; &#125; return false; &#125;; for(auto &amp;trip:trips) &#123; dfs(trip[0],-1,trip[1]); &#125; vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(2,0)); function &lt;void(int,int)&gt; dfs_dp = [&amp;](int u,int fa) &#123; dp[u][0] = price[u]*nums[u]; dp[u][1] = price[u]*nums[u]/2; for(auto &amp;v:edge[u])&#123; if (v==fa) continue; dfs_dp(v,u); dp[u][0] += min(dp[v][0],dp[v][1]); dp[u][1] += dp[v][0]; &#125; &#125;; dfs_dp(0,-1); return min(dp[0][0],dp[0][1]); &#125;&#125;; 2023-12-07 题目传送门：1466. 重新规划路线 - 力扣（LeetCode） 此题就是一个非常简单的树的遍历题目，我们只需要判断遍历过程中，边的方向，然后决定是否要反转即可。 注意本题中pair相关的写法，压入vector的时候使用的是{}，for循环提取出来的时候使用的是[] 123456789101112131415161718192021class Solution &#123;public: int minReorder(int n, vector&lt;vector&lt;int&gt;&gt;&amp; connections) &#123; vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt; edge(n+1); for(auto &amp;c:connections)&#123; int a = c[0], b = c[1]; edge[a].push_back(&#123;b,1&#125;); edge[b].push_back(&#123;a,0&#125;); &#125; int ans = 0; function&lt;void(int,int)&gt; dfs = [&amp;](int u,int fa)&#123; for(auto &amp;[v,type]:edge[u])&#123; if(v==fa) continue; ans += type; dfs(v,u); &#125; &#125;; dfs(0,-1); return ans; &#125;&#125;; 2023-12-08 题目传送门：2008. 出租车的最大盈利 - 力扣（LeetCode） 今天的这道题首先读完题目后就可以猜出应该是一道DP题，然后我们在观察数据规模，行程数量最大为3e4，站点取值范围为1e5。所以我们设计的DP算法的时间复杂度只能是O(n)O(n)O(n)或O(nlogn)O(nlogn)O(nlogn) 方法一：动态规划+二分查找 因为这道题对顺序有一些要求，所以不难想到可能会使用排序或者二分这类算法。 当然我们还是先设计dp状态转移方程，方程如下所示： dp[i]=max(dp[i−1],dp[j]+val[i])dp[i] = max(dp[i-1],dp[j]+val[i]) dp[i]=max(dp[i−1],dp[j]+val[i]) 其中索引号代表的是每个人的编号（重新排序过的，依终点大小从小到大进行排序），val[i]val[i]val[i]代表的是当前序号为i的人能够赚的钱。 jjj的选择是采用二分查询终点的方法找到比iii的起点小的最大的jjj的终点。（学习了upperbound的书写方法，在C++教程合集中有记载） 因此这道题就比较好解决了： 先按照终点从小到大排序 遍历所有的人，遵循上面的转移方程进行状态转移 转移的过程中，进行二分寻找j 最终代码如下： 123456789101112131415161718class Solution &#123;public: long long maxTaxiEarnings(int n, vector&lt;vector&lt;int&gt;&gt;&amp; rides) &#123; sort(rides.begin(),rides.end(), [](const vector&lt;int&gt; &amp;a, const vector&lt;int&gt; &amp;b)&#123; if(a[1]==b[1])return a[0] &lt; b[0]; return a[1] &lt; b[1]; &#125;); int p_num = rides.size(); vector&lt;long long&gt; dp(p_num+1); for(int i=0;i&lt;p_num;++i)&#123; int j = upper_bound(rides.begin(), rides.begin() + i, rides[i][0], [](int x, const vector&lt;int&gt; &amp;r)&#123; return x &lt; r[1]; &#125;) - rides.begin(); dp[i+1] = max(dp[i], dp[j]+rides[i][1]-rides[i][0]+rides[i][2]); &#125; return dp[p_num]; &#125;&#125;; 方法二：动态规划+哈希表 方法一可能存在不直观（dp数组索引是人）以及算法时间复杂度稍高的问题。我们其实使用动态规划结合哈希表的方法可以将算法时间复杂度优化到O(n)O(n)O(n)并且转移方程也会显得更加直观。 我们使用一个哈希表rideMap[end]，记录终点为end的所有乘客信息，不同于之前的方法这里dp[i]代表到达第iii个地点时能获得的最大利润。因此初始情况就是dp[0]=0。对于每个地点i有两种可能的转移： iii地点没人下车，则dp[i]=dp[i−1]dp[i]=dp[i-1]dp[i]=dp[i−1] 有人下车，最大利润为dp[i]=max(dp[startj]+end[j]−start[j]+tip[j])dp[i]=max(dp[start_j]+end[j]-start[j]+tip[j])dp[i]=max(dp[startj​]+end[j]−start[j]+tip[j]) 理解完整个过程最终的代码就是： 1234567891011121314151617class Solution &#123;public: long long maxTaxiEarnings(int n, vector&lt;vector&lt;int&gt;&gt;&amp; rides) &#123; vector&lt;long long&gt; dp(n+1); unordered_map&lt;int, vector&lt;vector&lt;int&gt;&gt;&gt; rideMap; for(const auto &amp;ride: rides) &#123; rideMap[ride[1]].push_back(ride); &#125; for(int i = 1; i &lt;= n; ++i)&#123; dp[i] = dp[i-1]; for(const auto &amp;ride : rideMap[i]) &#123; dp[i] = max(dp[i], dp[ride[0]] + ride[1] - ride[0] + ride[2]); &#125; &#125; return dp[n]; &#125;&#125;; 2023-12-09 题目传送门：2048. 下一个更大的数值平衡数 - 力扣（LeetCode） 这个题打一个表，用一个可爱的二分就可以解决，（二分直接调用upper_bound函数） 123456789101112131415161718192021222324class Solution &#123;public: const vector&lt;int&gt; balance &#123; 1, 22, 122, 212, 221, 333, 1333, 3133, 3313, 3331, 4444, 14444, 22333, 23233, 23323, 23332, 32233, 32323, 32332, 33223, 33232, 33322, 41444, 44144, 44414, 44441, 55555, 122333, 123233, 123323, 123332, 132233, 132323, 132332, 133223, 133232, 133322, 155555, 212333, 213233, 213323, 213332, 221333, 223133, 223313, 223331, 224444, 231233, 231323, 231332, 232133, 232313, 232331, 233123, 233132, 233213, 233231, 233312, 233321, 242444, 244244, 244424, 244442, 312233, 312323, 312332, 313223, 313232, 313322, 321233, 321323, 321332, 322133, 322313, 322331, 323123, 323132, 323213, 323231, 323312, 323321, 331223, 331232, 331322, 332123, 332132, 332213, 332231, 332312, 332321, 333122, 333212, 333221, 422444, 424244, 424424, 424442, 442244, 442424, 442442, 444224, 444242, 444422, 515555, 551555, 555155, 555515, 555551, 666666, 1224444 &#125;; int nextBeautifulNumber(int n) &#123; return *upper_bound(balance.begin(), balance.end(), n); &#125;&#125;; 2023-12-10 题目传送门：70. 爬楼梯 - 力扣（LeetCode） 刚好和动态规划（基础版）第一题一样，直接抄过来即可。 记忆化搜索版： 12345678910111213class Solution &#123;public: int climbStairs(int n) &#123; vector&lt;int&gt; dp(n+1,-1); dp[0] = 1;dp[1] = 1; function&lt;int(int)&gt; dfs = [&amp;](int now)&#123; if (dp[now]!=-1) return dp[now]; dp[now-1] = dfs(now-1); dp[now-2] = dfs(now-2); return dp[now-1] + dp[now-2]; &#125;; return dfs(n); &#125;&#125;; 递推版： 1234567891011class Solution &#123;public: int climbStairs(int n) &#123; vector&lt;int&gt; dp(n+1,0); dp[0] = 1;dp[1] = 1; for(int i=2;i&lt;=n;++i)&#123; dp[i] = dp[i-1]+dp[i-2]; &#125; return dp[n]; &#125;&#125;; 2023-12-11 题目传送门：1631. 最小体力消耗路径 - 力扣（LeetCode） 这道题注意题目不要读错，其实就是一道最短路板子题，正好借助这个机会熟悉一下Dijkstra的C++写法。 最终代码如下所示： 123456789101112131415161718192021222324252627282930313233struct DIS &#123; int x, y, val; DIS(int _x,int _y, int _val): x(_x), y(_y), val(_val) &#123;&#125; bool operator&lt; (const DIS&amp; that) const&#123; return that.val &lt; val; &#125;&#125;;class Solution &#123;public: int minimumEffortPath(vector&lt;vector&lt;int&gt;&gt;&amp; heights) &#123; int m = heights.size(), n = heights[0].size(); vector&lt;vector&lt;bool&gt;&gt; vis(m,vector&lt;bool&gt;(n, false)); vector&lt;vector&lt;int&gt;&gt; dis(m, vector&lt;int&gt;(n, INT_MAX)); priority_queue&lt;DIS&gt; q; dis[0][0] = 0; q.emplace(0,0,0); vector&lt;vector&lt;int&gt;&gt; dirs = &#123;&#123;0,1&#125;,&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,-1&#125;&#125;; while(!q.empty())&#123; auto [x,y,val] = q.top();q.pop(); if (vis[x][y]) continue; vis[x][y] = true; for(auto &amp;dir:dirs)&#123; int nx = x + dir[0], ny = y + dir[1]; if(nx&gt;=0 &amp;&amp; nx&lt;m &amp;&amp; ny&gt;=0 &amp;&amp; ny&lt;n &amp;&amp; dis[nx][ny] &gt; max(dis[x][y],abs(heights[x][y]-heights[nx][ny])))&#123; dis[nx][ny] = max(dis[x][y],abs(heights[x][y]-heights[nx][ny])); q.emplace(nx,ny,dis[nx][ny]); &#125; &#125; &#125; return dis[m-1][n-1]; &#125;&#125;; 上述代码使用了struct这就不是很正统C++，所以我们当然也可以写成类的形式： 12345678910111213141516171819202122232425262728293031323334353637class DIS &#123;public: int x, y, val; // 默认为私有成员 // 构造函数 DIS(int _x, int _y, int _val) : x(_x), y(_y), val(_val) &#123;&#125; // 比较操作符 bool operator&lt; (const DIS&amp; that) const &#123; return that.val &lt; val; &#125;&#125;;class Solution &#123;public: int minimumEffortPath(vector&lt;vector&lt;int&gt;&gt;&amp; heights) &#123; int m = heights.size(), n = heights[0].size(); vector&lt;vector&lt;bool&gt;&gt; vis(m,vector&lt;bool&gt;(n, false)); vector&lt;vector&lt;int&gt;&gt; dis(m, vector&lt;int&gt;(n, INT_MAX)); priority_queue&lt;DIS&gt; q; dis[0][0] = 0; q.emplace(0,0,0); vector&lt;vector&lt;int&gt;&gt; dirs = &#123;&#123;0,1&#125;,&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,-1&#125;&#125;; while(!q.empty())&#123; auto [x,y,val] = q.top();q.pop(); if (vis[x][y]) continue; vis[x][y] = true; for(auto &amp;dir:dirs)&#123; int nx = x + dir[0], ny = y + dir[1]; if(nx&gt;=0 &amp;&amp; nx&lt;m &amp;&amp; ny&gt;=0 &amp;&amp; ny&lt;n &amp;&amp; dis[nx][ny] &gt; max(dis[x][y],abs(heights[x][y]-heights[nx][ny])))&#123; dis[nx][ny] = max(dis[x][y],abs(heights[x][y]-heights[nx][ny])); q.emplace(nx,ny,dis[nx][ny]); &#125; &#125; &#125; return dis[m-1][n-1]; &#125;&#125;; 当然不出意外， 还是struct的效率会高一些，也建议以后都写成struct，在里面重定义小于运算符。 其他注意事项 auto取top中的结构体值 1auto [x,y,val] = q.top();q.pop(); 重定义运算符两个const都不能省略 123bool operator&lt; (const DIS&amp; that) const &#123;\treturn that.val &lt; val;&#125; int类型的最大值可以直接写INT_MAX 2023-12-12※ 题目传送门：2454. 下一个更大元素 IV - 力扣（LeetCode） 此题需要用到单调栈的相关内容,单调栈基础详情见专项训练单调栈模块。 以下内容摘自灵神题解： 首先回想一下，怎么找下一个更大元素，即右侧最近的更大元素。 使用视频中讲的第二种做法，从左到右遍历数组，用一个（递减）单调栈sss维护遍历过的元素，如果当前元素 xxx 比栈顶大，那么栈顶的下一个更大元素就是 xxx，并弹出栈顶。 在这个做法中，栈顶元素弹出后，就没有用了。但对于本题，我们需要的正是弹出去的元素！ 再用一个（递减）单调栈 ttt 记录从 sss 中弹出去的元素。继续向后遍历，如果又找到一个元素 yyy 比 ttt 的栈顶大，那么栈顶的下下个更大元素就是 yyy，并弹出栈顶。 1234567891011121314151617181920class Solution &#123;public: vector&lt;int&gt; secondGreaterElement(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; ans(n, -1), s, t; for (int i = 0; i &lt; n; ++i) &#123; int x = nums[i]; while (!t.empty() &amp;&amp; nums[t.back()] &lt; x)&#123; ans[t.back()] = x; t.pop_back(); &#125; int j = s.size(); while(j &amp;&amp; nums[s[j-1]] &lt; x) j--; t.insert(t.end(), s.begin() + j, s.end()); // 把从 s 弹出的这一整段元素加到 t s.resize(j); // 弹出一整段元素 s.push_back(i); // 当前元素（的下标）加到 s 栈顶 &#125; return ans; &#125;&#125;; 2023-12-13 题目传送门：2697. 字典序最小回文串 - 力扣（LeetCode） 双指针签到题目，两个指针分别指向字符串头尾，标记的是回文串相对应的位置，遍历的过程中，left每向右移动一个位置，right就向左移动一个位置，这样会出现两种情况。 如果指向的字符一样，不做任何操作 如果指向的字母不同，将他们都变成这两个子母中较小的那个字母即可。 最后返回新的字符串。 最终代码如下所示： 12345678910111213class Solution &#123;public: string makeSmallestPalindrome(string s) &#123; int n = s.length(); for(int i=0;i&lt;n/2;++i)&#123; if (s[i]!=s[n-1-i])&#123; if (s[i] &lt; s[n-1-i]) s[n-1-i] =s[i]; else s[i] = s[n-1-i]; &#125; &#125; return s; &#125;&#125;; 2023-12-14 今天的这道题非常的有意思，读完题目后，大概就会有一个基本思路，就是能放上邮票的地方我们尽量放上邮票，最后我们看整个矩阵有没有空白的地方。 借用灵神的思路概括一下： 由于邮票可以互相重叠，贪心地想，能放邮票就放邮票。 遍历所有能放邮票的位置去放邮票。注意邮票不能覆盖被占据的格子，也不能出界。 放邮票的同时，记录每个空格子被多少张邮票覆盖。如果存在一个空格子没被邮票覆盖，则返回 false，否则返回 true。 其实把上面的大体思路想明白了，根据之前的做题经验，这道题使用差分也比较明显了，就是还有一些细节需要注意一下。 因为要使用差分，所以我们放邮票从右下角开始放，这样好使用差分O(1)O(1)O(1)判断能否将邮票放在这里。 放邮票我们也是采用一个差分矩阵来记录，最后求二维前缀和就可以还原最终的邮票放置方法。 注意：以后写这种差分的题目，下标代表的都是从1开始，因为必须要留一个0保证求前缀和这类操作的统一性。 最终代码如下所示： 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: bool possibleToStamp(vector&lt;vector&lt;int&gt;&gt;&amp; grid, int stampHeight, int stampWidth) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;vector&lt;int&gt;&gt; s(n+1,vector&lt;int&gt;(m+1,0)); for (int i=0; i&lt;n; ++i) &#123; for(int j=0; j&lt;m; ++j)&#123; s[i+1][j+1] = grid[i][j] + s[i][j+1] + s[i+1][j] - s[i][j]; &#125; &#125; vector&lt;vector&lt;int&gt;&gt; cov(n+2,vector&lt;int&gt;(m+2,0)); for (int i=stampHeight; i&lt;=n; ++i) &#123; for(int j=stampWidth; j&lt;=m; ++j)&#123; int i1 = i - stampHeight + 1; int j1 = j - stampWidth + 1; if (s[i][j] - s[i][j1-1] - s[i1-1][j] + s[i1-1][j1-1] == 0)&#123; cov[i1][j1] ++; cov[i+1][j1] -- ; cov[i1][j+1] --; cov[i+1][j+1] ++; &#125; &#125; &#125; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; cov[i][j] += cov[i-1][j] + cov[i][j-1] - cov[i-1][j-1]; if (grid[i-1][j-1] == 0 &amp;&amp; cov[i][j] == 0) return false; &#125; &#125; return true; &#125;&#125;; 2023-12-15 题目传送门：2415. 反转二叉树的奇数层 - 力扣（LeetCode） 此题考察的是二叉树的层序遍历，其实就是一种bfs吧，通过本题可以更熟悉一下C++中指针和vector相关内容。 最终代码如下所示： 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* reverseOddLevels(TreeNode* root) &#123; vector&lt;vector&lt;TreeNode*&gt;&gt; que(2); que[0].push_back(root); int now = 0; while (!que[now].empty())&#123; for(auto node:que[now])&#123; if(node-&gt;left) que[1-now].push_back(node-&gt;left); if(node-&gt;right) que[1-now].push_back(node-&gt;right); &#125; if (now) &#123; int siz = que[now].size(); for(int i=0;i&lt;siz/2;++i)&#123; int tmp = que[now][i]-&gt;val; que[now][i]-&gt;val = que[now][siz-1-i]-&gt;val; que[now][siz-1-i]-&gt;val = tmp; &#125; &#125; while(!que[now].empty()) que[now].pop_back(); now = 1-now; &#125; return root; &#125;&#125;; 2023-12-16 题目传送门：2276. 统计区间中的整数数目 - 力扣（LeetCode） 还没太整明白： 123456789101112131415161718192021class Solution &#123;public: vector&lt;int&gt; secondGreaterElement(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;int&gt; ans(n, -1), s, t; for (int i = 0; i &lt; n; ++i) &#123; int x = nums[i]; while (!t.empty() &amp;&amp; nums[t.back()] &lt; x)&#123; ans[t.back()] = x; t.pop_back(); &#125; int j = s.size(); while(j &amp;&amp; nums[s[j-1]] &lt; x) j--; t.insert(t.end(), s.begin() + j, s.end()); // 把从 s 弹出的这一整段元素加到 t s.resize(j); // 弹出一整段元素 s.push_back(i); // 当前元素（的下标）加到 s 栈顶 &#125; return ans; &#125;&#125;; 2023-12-17 题目传送门：746. 使用最小花费爬楼梯 - 力扣（LeetCode） 一道非常基础的DP题目，这里就使用记忆化搜索解决了，需要注意的是cost最后压入一个0，来保证所有的操作一致性（代表已经到最高点了，往上走的开销为0） 12345678910111213141516class Solution &#123;public: int minCostClimbingStairs(vector&lt;int&gt;&amp; cost) &#123; int n = cost.size(); cost.emplace_back(0); vector&lt;int&gt; dp(n+1, -1); dp[0] = cost[0]; dp[1] = cost[1]; function&lt;int(int)&gt; dfs = [&amp;](int now)&#123; if (dp[now]!=-1) return dp[now]; int a = dfs(now-1), b = dfs(now-2); dp[now] = cost[now] + min(a,b); return dp[now]; &#125;; return dfs(n); &#125;&#125;; 2023-12-18 题目传送门：162. 寻找峰值 - 力扣（LeetCode） 因为题目要求实现O(log⁡n)O(\\log n)O(logn)时间复杂度的算法，所以不难想到需要使用二分算法，二分的基础知识我们已经在二分专题板块进行了讲解。 本题二分的思想其实很简单，我们需要明白一个道理，就是峰值一定是比较大的值，所以每次二分的时候判断当前mid位置与他左右位置数的大小关系，然后往大的地方更新区间即可。 代码如下： 123456789101112131415161718class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; int len = nums.size(); int l = 0, r = len-1; function&lt;long long(int)&gt; getitem = [&amp;](int i)&#123; if (i&lt;=-1||i&gt;=len) return 2*(long long)(INT_MIN); return (long long)(nums[i]); &#125;; while (l &lt;= r) &#123; int mid = l + (r-l)/2; if (getitem(mid)&gt;getitem(mid-1)&amp;&amp;getitem(mid)&gt;getitem(mid+1)) return mid; if (getitem(mid)&lt;getitem(mid+1)) l = mid+1; else r = mid-1; &#125; return -123123; &#125;&#125;; 这里使用的是闭区间二分，需要注意的是0位置的左边以及len-1位置的右边定义的峰值大小都是2*INT_MIN,使用一个getitem函数来取值，其中需要考虑这种超出区间范围的问题。 其实上面的这种写法并不是很二分，可以再改进一下，如下所示（因为最后一定有峰顶，所以直接分为不可能为峰顶和可能为峰顶两类就可以判断出来谁是峰顶了）： 123456789101112131415161718class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; int len = nums.size(); int l = 0, r = len-1; function&lt;long long(int)&gt; getitem = [&amp;](int i)&#123; if (i&lt;=-1||i&gt;=len) return 2*(long long)(INT_MIN); return (long long)(nums[i]); &#125;; while (l &lt;= r) &#123; int mid = l + (r-l)/2; // if (getitem(mid)&gt;getitem(mid-1)&amp;&amp;getitem(mid)&gt;getitem(mid+1)) return mid; if (getitem(mid)&lt;getitem(mid+1)) l = mid+1; else r = mid-1; &#125; return l; &#125;&#125;; 可以仔细理解一下为啥把中间那个if注释掉。 最后返回lll是因为mid还是有可能是峰顶的，所以应该返回r+1r+1r+1，又因为最后终止的时候r+1=lr+1=lr+1=l，所以返回lll 2023-12-19 题目传送门：1901. 寻找峰值 II - 力扣（LeetCode） 同样也是二分，只不过是对每行的最大值进行二分，往mid前后行更大的最大值的地方更新区间。最终代码如下所示： 123456789101112131415161718class Solution &#123;public: int max_idx(vector&lt;int&gt; nums)&#123; return max_element(nums.begin(), nums.end()) - nums.begin(); &#125; vector&lt;int&gt; findPeakGrid(vector&lt;vector&lt;int&gt;&gt;&amp; mat) &#123; int n = mat.size(); int l = 0, r = n-1; while(l&lt;r)&#123; int mid = l + (r-l)/2; int j = max_idx(mat[mid]); if (mat[mid][j] &gt; mat[mid+1][j]) r = mid; else l = mid+1; &#125; int ans = l; return vector&lt;int&gt;&#123;ans,max_idx(mat[ans])&#125;; &#125;&#125;; 2023-12-20 题目传送门：2828. 判别首字母缩略词 - 力扣（LeetCode） 签到题，熟悉string的基本用法 12345678910class Solution &#123;public: bool isAcronym(vector&lt;string&gt;&amp; words, string s) &#123; if(words.size()!=s.size()) return false; for(int i=0;i&lt;words.size();++i)&#123; if (words[i][0]!=s[i]) return false; &#125; return true; &#125;&#125;; 2023-12-21 题目传送门：2866. 美丽塔 II - 力扣（LeetCode） 2023-12-22 题目传送门：1671. 得到山形数组的最少删除次数 - 力扣（LeetCode） 2023-12-23 题目传送门：1962. 移除石子使总数最小 - 力扣（LeetCode） 一道非常容易想的贪心题目，每次取最大减半即可。使用优先队列维护最大值。 12345678910111213141516171819class Solution &#123;public: int minStoneSum(vector&lt;int&gt;&amp; piles, int k) &#123; priority_queue&lt;int&gt; pq; int sum_pile = 0; for(auto &amp;pile:piles) &#123; pq.emplace(pile); sum_pile += pile; &#125; int ans = 0; for(int i=1;i&lt;=k;++i)&#123; int nw = pq.top(); ans += nw/2; pq.pop(); pq.emplace(nw-(nw/2)); &#125; return sum_pile - ans; &#125;&#125;; 2023-12-24 题目传送门：1954. 收集足够苹果的最小花园周长 - 力扣（LeetCode） 一道非常明显的二分题目，将给定x苹果总数计算公式推出即可，使用左闭右开二分查找： 12345678910111213141516class Solution &#123;public: long long minimumPerimeter(long long neededApples) &#123; function&lt;long long(long long)&gt; getApple = [&amp;](long long x)&#123; return (x*2+1)*x*(x+1)*2; &#125;; long long l = 0, r = 100000; while(l&lt;r) &#123; long long mid = l + (r-l)/2; long long nwapple = getApple(mid); if (nwapple &gt;= neededApples) r=mid; else l=mid+1; &#125; return l*8; &#125;&#125;; 2023-12-25 题目传送门：1276. 不浪费原料的汉堡制作方案 - 力扣（LeetCode） 再次复习一下二分闭区间写法。 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; numOfBurgers(int tomatoSlices, int cheeseSlices) &#123; function&lt;bool(int)&gt; search = [&amp;](int x)&#123; return x*4+(cheeseSlices-x) * 2 &gt;=tomatoSlices?true:false; &#125;; int l = 0, r = cheeseSlices; while(l &lt;= r)&#123; int mid = l+(r-l)/2; if (search(mid)) r = mid-1; else l = mid+1; &#125; return l*4+(cheeseSlices-l)*2 == tomatoSlices?vector&lt;int&gt;&#123;l,cheeseSlices-l&#125;:vector&lt;int&gt;&#123;&#125;; &#125;&#125;; 2023-12-26 题目传送门：1349. 参加考试的最大学生数 - 力扣（LeetCode） 2023-12-27 题目传送门：2660. 保龄球游戏的获胜者 - 力扣（LeetCode） 一道非常简单的签到题目： 12345678910111213141516class Solution &#123;public: int isWinner(vector&lt;int&gt;&amp; player1, vector&lt;int&gt;&amp; player2) &#123; int n = player1.size(); int p1_sum = player1[0], p2_sum = player2[0]; for(int i=1;i&lt;n;++i)&#123; if (player1[i-1]==10) p1_sum += player1[i]*2; else if (i&gt;1&amp;&amp;player1[i-2]==10) p1_sum += player1[i]*2; else p1_sum += player1[i]; if (player2[i-1]==10) p2_sum += player2[i]*2; else if (i&gt;1&amp;&amp;player2[i-2]==10) p2_sum += player2[i]*2; else p2_sum += player2[i]; &#125; return p1_sum == p2_sum?0:p1_sum&gt;p2_sum?1:2; &#125;&#125;; 2023-12-28 题目传送门：2735. 收集巧克力 - 力扣（LeetCode） 这道题题目描述有点歧义，需要结合样例理解题目意思，但是还是一道有点有意思的枚举。 理解清楚题意后我们不难发现，操作的次数是有限的，最多是等于种类数-1，然后我们观察数据范围&lt;1000，因此我们可以采用枚举操作数的方式解决这道题目，代码如下： 1234567891011121314151617class Solution &#123;public: long long minCost(vector&lt;int&gt;&amp; nums, int x) &#123; int len = nums.size(); vector&lt;int&gt;min_nw = nums; long long ans = 1000000000004; for(int i=0;i&lt;len;++i)&#123; long long tmp = (long long)i*x; for(int j=0;j&lt;len;++j)&#123; min_nw[j] = min(min_nw[j],nums[(j+i)%len]); tmp += min_nw[j]; &#125; ans = min(ans,tmp); &#125; return ans; &#125;&#125;; 2023-12-29 题目传送门：2706. 购买两块巧克力 - 力扣（LeetCode） 签到题，代码如下： 12345678class Solution &#123;public: int buyChoco(vector&lt;int&gt;&amp; prices, int money) &#123; sort(prices.begin(),prices.end()); int cost = prices[0] + prices[1]; return cost &gt; money?money:money-cost; &#125;&#125;; 2023-12-30 题目传送门：1185. 一周中的第几天 - 力扣（LeetCode） 模拟题，注意判断闰年以及闰年二月份的时候即可。 12345678910111213141516171819202122class Solution &#123;public: string dayOfTheWeek(int day, int month, int year) &#123; auto judge_year = [&amp;](int x)&#123; if(x % 400 == 0 || (x % 4 == 0 &amp;&amp; x % 100 != 0)) return true; return false; &#125;; vector&lt;string&gt; weekDay = &#123;&quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot;&#125;; vector&lt;int&gt; monthDay = &#123;31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31&#125;; int ans = day; // cal_year int del_year = year-1971; ans += (year-1971)*365; for(int i=1971;i&lt;year;++i)&#123; if (judge_year(i)) ans++; &#125; for(int i=1;i&lt;month;++i) ans += monthDay[i-1]; ans = (month&gt;2&amp;&amp;judge_year(year))?ans+1:ans; int week_day = (4+ans)%7; return weekDay[week_day]; &#125;&#125;; 注意上一题Sunday存的下标为0，因为这样处理方便一些。 2023-12-31 题目传送门：1154. 一年中的第几天 - 力扣（LeetCode） 上一道题的简单版。 12345678910111213141516171819class Solution &#123;public: int dayOfYear(string date) &#123; int year = stoi(date.substr(0, 4)); int month = stoi(date.substr(5, 2)); int day = stoi(date.substr(8, 2)); auto judge_year = [&amp;](int x) &#123; if (x%400==0||(x%4==0&amp;&amp;x%100!=0)) return true; return false; &#125;; vector&lt;int&gt; months = &#123;31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31&#125;; int ans = day; for (int i=1;i&lt;month;++i) &#123; ans += months[i-1]; &#125; return (month&gt;2&amp;&amp;judge_year(year))?ans+1:ans; &#125;&#125;;"},{"title":"2024年01月每日一题","path":"/wiki/LeetCode/每日一题/2024年01月每日一题.html","content":"2024-01-01 题目传送门：1599. 经营摩天轮的最大利润 - 力扣（LeetCode） 新年快乐🎇🎇🎇！！！，新年第一题就是之前做过的，模拟即可。首先先循环每个时间点，使用一个变量previ记录该时间点上一个时间点后还剩下没上摩天轮的顾客数量，然后当前时间点的顾客数就是之前剩下的加上新来的，这里大于4还是小于等于4分类讨论一下。最后如果最后一个时间点已经过了的话previ还有剩余，则继续转直到将previ转空，中间判断一下转多少次的时候能够有最大利润即可： Python代码如下： 123456789101112131415161718192021222324252627282930313233class Solution: def minOperationsMaxProfit(self, customers: List[int], boardingCost: int, runningCost: int) -&gt; int: res = 0 ans = [] serv = 0 for idx,customer in enumerate(customers): if customer + res &gt; 4: res += (customer-4) serv += 4 ans.append(serv * boardingCost - (idx+1) * runningCost) else: serv += (res+customer) res = 0 ans.append(serv * boardingCost - (idx+1) * runningCost) while res: idx += 1 if res &gt; 4: serv += 4 res -= 4 ans.append(serv * boardingCost - (idx+1) * runningCost) else: serv += res res = 0 ans.append(serv * boardingCost - (idx+1) * runningCost) now,loc = 0,-1 # return ans for i in range(len(ans)): if now &lt; ans[i]: now = ans[i] loc = i+1 return -1 if loc == -1 else loc C++代码如下： 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; public: int minOperationsMaxProfit(vector&lt;int&gt;&amp; customers, int boardingCost, int runningCost) &#123; int previ = 0; int ans = 0,nw = 0; int ans_cnt=-1,cnt=0; for (auto customer:customers) &#123; int tmp = customer + previ; cnt++; if (tmp &gt; 4) &#123; nw += 4*boardingCost - runningCost; previ = tmp - 4; if (nw &gt; ans) ans = nw,ans_cnt = cnt; ans = max(ans, nw); &#125; else &#123; nw += tmp*boardingCost - runningCost; previ = 0; if (nw &gt; ans) ans = nw,ans_cnt = cnt; &#125; &#125; while (previ) &#123; cnt++; if(previ &gt; 4) &#123; nw += 4*boardingCost - runningCost; previ -= 4; if (nw &gt; ans) ans = nw,ans_cnt = cnt; &#125; else &#123; nw += previ*boardingCost - runningCost; previ = 0; if (nw &gt; ans) ans = nw,ans_cnt = cnt; &#125; &#125; return ans_cnt; &#125; &#125;; 2024-01-02 题目传送门：466. 统计重复个数 - 力扣（LeetCode） 2024-01-03 题目传送门：2487. 从链表中移除节点 - 力扣（LeetCode） 一道链表的题目，充分暴露了我对C++一些特性以及指针相关内容还不是非常的熟练。 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* removeNodes(ListNode* head) &#123; ListNode* nw = head; vector&lt;int&gt; arr = &#123;nw-&gt;val&#125;; while (nw-&gt;next) &#123; nw = nw-&gt;next; arr.push_back(nw-&gt;val); &#125; stack&lt;int&gt; ans_arr; int siz = arr.size(); for (int i=siz-1;i&gt;=0;--i) &#123; if (ans_arr.size()==0||arr[i] &gt;= ans_arr.top()) &#123; ans_arr.push(arr[i]); &#125; &#125; ListNode *hhead = new ListNode(), *now = new ListNode(); now = hhead; while (!ans_arr.empty()) &#123; ListNode *tmp = new ListNode(); int nw_val = ans_arr.top(); ans_arr.pop(); tmp-&gt;val = nw_val; now-&gt;next = tmp; now = tmp; &#125; return hhead-&gt;next; &#125;&#125;; 注意需要非常注意的就是在初始化指针的时候一定要这么写ListNode *hhead = new ListNode(), *now = new ListNode(); 为什么不能ListNode *tmp; 这里的tmp是一个指向ListNode类型的指针。但是，这个指针在使用之前没有被分配任何内存。这意味着tmp指向的是一个未知的、未初始化的内存位置。当您尝试通过now-&gt;next = tmp;给now-&gt;next赋值时，您实际上是在让now-&gt;next指向一个不确定的内存位置。这是非常危险的，因为这块内存可能是空的，或者已经被其他数据占用，从而导致未定义行为或程序崩溃。 为什么不能ListNode* tmp = nullptr; 如果您将ListNode* tmp = nullptr;并在之后的代码中尝试通过tmp指针访问或修改数据，那么这同样会导致错误。原因如下： 空指针解引用: 当tmp被设置为nullptr时，它指向的是空地址。任何尝试通过空指针（nullptr）访问或修改数据的行为都会导致运行时错误，通常是段错误（segmentation fault）。在您的代码中，如果tmp是nullptr，那么像now-&gt;next = tmp;这样的赋值操作是安全的，但之后的tmp-&gt;val = nw_val;尝试解引用nullptr，这会导致错误。 未分配实际的节点: 在链表操作中，通常需要创建新的节点来存储数据。如果tmp只是一个空指针，那么它不能用于存储任何实际的数据。您需要为每个新节点分配内存来存储数据和指向下一个节点的指针。 总结 要解决这个问题，您需要在使用tmp之前为其分配内存，如下所示： 1ListNode* tmp = new ListNode(); // 分配内存并初始化 这样，tmp不再是一个空指针，而是指向一个新分配且已初始化的ListNode对象。这允许您安全地使用tmp来存储数据并将其加入到链表中。 2024-01-04 题目传送门：2397. 被列覆盖的最多行数 - 力扣（LeetCode） 一道暴力搜索板子题目。 1234567891011121314151617181920212223242526272829303132class Solution &#123;public: int maximumRows(vector&lt;vector&lt;int&gt;&gt;&amp; matrix, int numSelect) &#123; int ans = 0; int m = matrix.size(),n = matrix[0].size(); vector&lt;bool&gt; cols(n,false); auto judge = [&amp;](int x) &#123; int now_ans = 0; for(int i=0;i&lt;n;++i)&#123; if (matrix[x][i]==1 &amp;&amp; cols[i]==false) return false; &#125; return true; &#125;; function&lt;void(int,int)&gt; dfs = [&amp;](int idx,int pre) &#123; if (idx==numSelect) &#123; int nw = 0; for(int i=0;i&lt;m;++i)&#123; if (judge(i)) nw += 1; &#125; ans = max(ans,nw); return; &#125; for (int i=pre+1;i&lt;n;++i) &#123; cols[i] = true; dfs(idx+1, i); cols[i] = false; &#125; &#125;; dfs(0,-1); return ans; &#125;&#125;; 2024-01-05 题目传送门：1944. 队列中可以看到的人数 - 力扣（LeetCode） 在读完题目以及看完数据范围后不难想到应该用单调栈的方法解决这道题，反向遍历数组，单调栈维护一个顶小底大的栈，代表当前元素可能可以看见的右边的人。 我们只需要在每次在判断当前元素时，统计我们要进行多少次弹栈操作。就是我们能够看到的人的数量，但是最后还有一个地方需要注意一下，见下面红色。 注意这里还有一个细节，弹完栈后，需要判断当前栈是否为空，如果不为空，我们能看到的人还要+1（虽然比我们高但是能看到） 12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; canSeePersonsCount(vector&lt;int&gt;&amp; heights) &#123; stack&lt;int&gt; stk; int n = heights.size(); vector&lt;int&gt; ans(n,0); for(int i=n-1;i&gt;=0;--i)&#123; int tmp = 0; while(!stk.empty() &amp;&amp; heights[stk.top()] &lt; heights[i]) &#123; stk.pop(); tmp++; &#125; if (stk.empty()) ans[i] = tmp; else ans[i] = tmp+1; stk.push(i); &#125; return ans; &#125;&#125;; 2024-01-06 题目传送门：2807. 在链表中插入最大公约数 - 力扣（LeetCode） 一道非常简单的签到题，复习一下链表写法和gcd写法。 1234567891011121314151617181920212223242526272829/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* insertGreatestCommonDivisors(ListNode* head) &#123; function&lt;int(int,int)&gt; ggcd=[&amp;](int a,int b) &#123; return a%b==0?b:ggcd(b,a%b); &#125;; ListNode *nw = head; if (nw-&gt;next == nullptr) return head; ListNode *nwnxt = nw-&gt;next; while (nwnxt != nullptr) &#123; int val = ggcd(max(nw-&gt;val,nwnxt-&gt;val),min(nw-&gt;val,nwnxt-&gt;val)); ListNode *tmp = new ListNode(val, nwnxt); nw-&gt;next = tmp; nw = nwnxt; nwnxt = nwnxt-&gt;next; &#125; return head; &#125;&#125;; 2024-01-07 题目传送门：383. 赎金信 - 力扣（LeetCode） 建立26个桶，统计每个字符个数即可。签到题： 12345678910111213class Solution &#123;public: bool canConstruct(string ransomNote, string magazine) &#123; if (ransomNote.size() &gt; magazine.size()) return false; vector&lt;int&gt; cnt(26,0); for(auto note:magazine) cnt[note-&#x27;a&#x27;]++; for(auto note:ransomNote)&#123; cnt[note-&#x27;a&#x27;]--; if (cnt[note-&#x27;a&#x27;] &lt; 0) return false; &#125; return true; &#125;&#125;; 2024-01-08 题目传送门：447. 回旋镖的数量 - 力扣（LeetCode） 一道unordered_map哈希题，O(n2)O(n^2)O(n2)复杂度，对于每个位置，遍历所有位置，使用一个哈希表存储与该位置相距特定距离下有哪些位置（哈希表索引是距离），最后就可以根据这个哈希表计算以该点为中心的回旋镖数量。 代码如下： 12345678910111213141516171819class Solution &#123;public: int numberOfBoomerangs(vector&lt;vector&lt;int&gt;&gt;&amp; points) &#123; int ans = 0,n=points.size(); for(int i=0;i&lt;n;++i)&#123; unordered_map&lt;int,int&gt; cnt; for(int j=0;j&lt;n;++j)&#123; if (i == j) continue; int dis = (points[i][0]-points[j][0]) * (points[i][0]-points[j][0]) + (points[i][1]-points[j][1]) * (points[i][1]-points[j][1]); if(cnt.find(dis)==cnt.end()) cnt[dis] = 1; else cnt[dis] ++; &#125; for(auto ele:cnt) &#123; ans += ele.second * (ele.second-1); &#125; &#125; return ans; &#125;&#125;; 2024-01-09 题目传送门： 2024-01-14 题目传送门：83. 删除排序链表中的重复元素 - 力扣（LeetCode） 链表遍历签到题目，代码如下： 1234567891011121314151617181920212223/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; ListNode *nw = head; while(nw!= nullptr&amp;&amp; nw-&gt;next!= nullptr)&#123; ListNode *nxt = nw-&gt;next; while(nxt!= nullptr&amp;&amp;nw-&gt;val == nxt-&gt;val) nxt = nxt-&gt;next; nw-&gt;next = nxt; nw = nw-&gt;next; &#125; return head; &#125;&#125;; 2024-01-15 题目传送门：82. 删除排序链表中的重复元素 II - 力扣（LeetCode） 链表遍历题目，是昨天题目的加强版,下面给出只遍历一遍的做法，需要注意的细节是： 头节点也可能被删除，所以要开一个head之前的点。 需要判断下一个和下下个节点是否存在，只有都存在才可能出现相等的情况。 最后代码如下： 12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; ListNode *headpre = new ListNode(0, head); ListNode *nw = headpre; while (nw-&gt;next &amp;&amp; nw-&gt;next-&gt;next) &#123; if (nw-&gt;next-&gt;val == nw-&gt;next-&gt;next-&gt;val) &#123; int x = nw-&gt;next-&gt;val; while (nw-&gt;next &amp;&amp; nw-&gt;next-&gt;val == x) &#123; nw-&gt;next = nw-&gt;next-&gt;next; &#125; &#125; else &#123; nw = nw-&gt;next; &#125; &#125; return headpre-&gt;next; &#125;&#125;; 2024-01-16 题目传送门：2719. 统计整数数目 - 力扣（LeetCode） 一道数位DP的题目 2024-01-17 题目传送门：2744. 最大字符串配对数目 - 力扣（LeetCode） 双重循环直接解决问题。代码如下： 12345678910111213class Solution &#123;public: int maximumNumberOfStringPairs(vector&lt;string&gt;&amp; words) &#123; int n = words.size(); int ans = 0; for(int i=0;i&lt;n;++i)&#123; for(int j=i+1;j&lt;n;++j)&#123; if(words[i][0] == words[j][1] &amp;&amp; words[i][1] == words[j][0]) ans++; &#125; &#125; return ans; &#125;&#125;; 2024-01-20 题目传送门：2788. 按分隔符拆分字符串 - 力扣（LeetCode） 签到题（对于Python来说哈哈），C的话主要可能需要使用stringstream以及getline函数，具体见我的C整理。 最终代码如下： 12345678910111213141516class Solution &#123;public: vector&lt;string&gt; splitWordsBySeparator(vector&lt;string&gt;&amp; words, char separator) &#123; vector&lt;string&gt; res; for (string &amp;word : words) &#123; stringstream ss(word); string sub; while (getline(ss, sub, separator)) &#123; if (!sub.empty()) &#123; res.push_back(sub); &#125; &#125; &#125; return res; &#125;&#125;; 2024-01-21 题目传送门：410. 分割数组的最大值 - 力扣（LeetCode） 一道DP题目，设置dp二维数组dp[n+1][k+1]dp[n+1][k+1]dp[n+1][k+1]其中nnn表示数组长度，kkk表示分割次数。定义dp[i][j]dp[i][j]dp[i][j]的含义是从第1个数到第iii个数分成kkk段后的最大值。那么不难想到状态转移方程应该表示为： dp[j][i]=min⁡(dp[j][i],max⁡(dp[kk][i−1],nums_sum[j]−nums_sum[kk]))dp[j][i] = \\min(dp[j][i],\\max(dp[kk][i-1], nums\\_sum[j]-nums\\_sum[kk])) dp[j][i]=min(dp[j][i],max(dp[kk][i−1],nums_sum[j]−nums_sum[kk])) 其中nums_sumnums\\_sumnums_sum是前缀和数组, 因此nums_sum[j]nums\\_sum[j]nums_sum[j]表示的是前jjj个数的和。 接下来就是dp限制条件和初始状态的定义了，观察上述状态转移方程，不难得知方程想要正确运行需要j≥ij\\geq ij≥i且kk≥i−1kk\\geq i-1kk≥i−1 且j&gt;kkj &gt; kkj&gt;kk。这样如果采用递推的方法的话我们就可以确定循环的顺序了。 123for(int i=2; i&lt;=k; ++i)&#123;\tfor(int j=i; j&lt;=n; ++j)&#123; for(int kk=i-1; kk&lt;=j-1; ++kk) 最后就是初始条件的确定了，其实就是初始化dp[i][1]=nums_sum[i]dp[i][1] = nums\\_sum[i]dp[i][1]=nums_sum[i] ,然后循环就可以从第二个分割开始循环了。 注意dp数组最开始初始化一定是INT_MAX 123456789101112131415161718192021class Solution &#123;public: int splitArray(vector&lt;int&gt;&amp; nums, int k) &#123; int n = nums.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(k+1, INT_MAX)); vector&lt;int&gt; nums_sum(n+1,0); dp[0][1] = 0; for(int i=0;i&lt;n;++i) &#123; dp[i+1][1] = dp[i][1] + nums[i]; nums_sum[i+1] = nums_sum[i] + nums[i]; &#125; for(int i=2; i&lt;=k; ++i)&#123; for(int j=i; j&lt;=n; ++j)&#123; for(int kk=i-1; kk&lt;=j-1; ++kk)&#123; dp[j][i] = min(dp[j][i],max(dp[kk][i-1], nums_sum[j]-nums_sum[kk])); &#125; &#125; &#125; return dp[n][k]; &#125;&#125;; 记忆化搜索同理，逻辑反过来就行,会更简单易懂一些： 12345678910111213141516171819class Solution &#123;public: int splitArray(vector&lt;int&gt;&amp; nums, int k) &#123; int n = nums.size(); vector&lt;vector&lt;int&gt;&gt; dp(n + 1, vector&lt;int&gt;(k + 1, INT_MAX)); dp[0][1] = 0; for (int i = 1; i &lt;= n; ++i) dp[i][1] = dp[i - 1][1] + nums[i - 1]; function&lt;int(int, int)&gt; dfs = [&amp;](int u, int v) &#123; if (v == 1) return dp[u][1]; if (dp[u][v] != INT_MAX) return dp[u][v]; int tmp = INT_MAX; for (int i = v - 1; i &lt; u; ++i) &#123; tmp = min(tmp, max(dfs(i, v - 1), dp[u][1] - dp[i][1])); &#125; return dp[u][v] = tmp; &#125;; return dfs(n, k); &#125;&#125;; 还是建议使用递推，因为效率更高！！！ 2024-01-22 题目传送门：670. 最大交换 - 力扣（LeetCode） 这道题解决方法应该还是很多的，但是因为之前在学习单调栈，所以这里第一时间想到的是基于单调栈的解法。 主体思路很简单，就是我们倒着扫描这个数的每一位，维护一个单调栈，这样每次我们扫描的时候就可以知道当前第i个数是否有下一个比他大的数。如果有我们使用一个数组ans记录比他大的数的索引。最后正着扫一遍数组，如果碰到第一个第iii个数的ans[i]中存储了索引，则进行交换操作即可。 注意判断出栈的时候是&gt;&gt;&gt;不是&gt;=&gt;=&gt;=，这是因为当值一样时，换更后面的显然更优，最后判断当前数存不存索引的时候一定要判断当前数和栈底数的大小关系（因为有可能一样大，这时就不需要进行交换） 最后代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int maximumSwap(int num) &#123; string num_s = to_string(num); int n = num_s.size(); vector&lt;int&gt; stk; vector&lt;int&gt; ans(n,-1); for(int i=n-1;i&gt;=0;--i)&#123; while(!stk.empty() &amp;&amp; num_s[i]-&#x27;0&#x27; &gt; num_s[stk[stk.size()-1]] - &#x27;0&#x27;)stk.pop_back(); if (!stk.empty() &amp;&amp; num_s[i]-&#x27;0&#x27; &lt; num_s[stk[0]] - &#x27;0&#x27;) ans[i] = stk[0]; stk.emplace_back(i); &#125; for(int i=0;i&lt;n;++i)&#123; if (ans[i]!=-1) &#123; int tmp = num_s[ans[i]]-&#x27;0&#x27;; num_s[ans[i]] = num_s[i]; num_s[i] = tmp + &#x27;0&#x27;; break; &#125; &#125; return stoi(num_s); &#125;&#125;; 2023-01-23 题目传送门：2765. 最长交替子数组 - 力扣（LeetCode） 签到题目，代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int alternatingSubarray(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(),pre = nums[0]; int ans = -1; for(int i=1;i&lt;n;++i)&#123; if (nums[i] == pre+1) &#123; int tmp = 2; pre = nums[i]; int dir = -1; while (i+1&lt;n &amp;&amp; pre + dir == nums[i+1])&#123; dir *= -1; i += 1; pre = nums[i]; tmp ++; &#125; ans = max(ans,tmp); &#125; pre = nums[i]; &#125; return ans; &#125;&#125;; 2024-01-24 题目传送门：2865. 美丽塔 I - 力扣（LeetCode） 这道题同样也是一道单调栈的题目，还是有点意思。 1 2024-01-25 题目传送门：2859. 计算 K 置位下标对应元素的和 - 力扣（LeetCode） 一道位运算签到题目，统计二进制数有多少个置位，只需要&amp;1和右移即可。 123456789101112131415161718192021class Solution &#123;public: int sumIndicesWithKSetBits(vector&lt;int&gt;&amp; nums, int k) &#123; int n = nums.size(); function&lt;int(int)&gt;bitCount = [&amp;](int i)&#123; int nw = 0; while(i)&#123; nw += i&amp;1; i = i&gt;&gt;1; &#125; return nw; &#125;; int ans = 0; for(int i=0;i&lt;n;++i)&#123; if (bitCount(i)==k) &#123; ans += nums[i]; &#125; &#125; return ans; &#125;&#125;; 2024-01-26 题目传送门："},{"title":"2024年03月每日一题","path":"/wiki/LeetCode/每日一题/2024年03月每日一题.html","content":"2024-03-01 题目传送门：2369. 检查数组是否存在有效划分 - 力扣（LeetCode） 非常基础的DP，DP方程如下： 123if (nums[i-1] == nums[i-2]) dp[i] = dp[i]||dp[i-2];if (nums[i-1] == nums[i-2]&amp;&amp;nums[i-2] == nums[i-3]) dp[i] = dp[i]||dp[i-3];if (nums[i-1] == nums[i-2]+1 &amp;&amp; nums[i-2] == nums[i-3]+1) dp[i] = dp[i]||dp[i-3]; 其中dp[i]代表的是到第i个数前面的数能否被划分。初始条件dp[0]=true;dp[1]=false，然后dp[2]的取值，取决于第一个元素和第二个元素是否相等，从3开始递推。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: bool validPartition(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); if (n &lt; 2) return false; vector&lt;bool&gt; dp(n+1,false); dp[0] = true; dp[1] = false; if (nums[0]==nums[1]) dp[2] = true; for(int i=3; i&lt;=n; ++i) &#123; if (nums[i-1] == nums[i-2]) dp[i] = dp[i]||dp[i-2]; if (nums[i-1] == nums[i-2]&amp;&amp;nums[i-2] == nums[i-3]) dp[i] = dp[i]||dp[i-3]; if (nums[i-1] == nums[i-2]+1 &amp;&amp; nums[i-2] == nums[i-3]+1) dp[i] = dp[i]||dp[i-3]; &#125; return dp[n]; &#125;&#125;; 2024-03-02 题目传送门：2368. 受限条件下可到达节点的数目 - 力扣（LeetCode） DFS深搜，对于每个节点返回以当前节点为子树可以访问的节点数量。 实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int reachableNodes(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges, vector&lt;int&gt;&amp; restricted) &#123; vector&lt;bool&gt; vis(n, false); for(auto &amp;node:restricted) vis[node] = true; vector&lt;vector&lt;int&gt;&gt; es(n); for(auto &amp;edge:edges)&#123; es[edge[0]].push_back(edge[1]); es[edge[1]].push_back(edge[0]); &#125; function&lt;int(int,int)&gt; dfs = [&amp;](int u, int fa)&#123; int now = 1; for(auto &amp;v:es[u])&#123; if (vis[v]||v == fa) continue; now += dfs(v, u); &#125; return now; &#125;; return dfs(0, -1); &#125;&#125;; 2024-03-09 题目传送门：2386. 找出数组的第 K 大和 - 力扣（LeetCode） 2024-03-10 题目传送门：299. 猜数字游戏 - 力扣（LeetCode） 思路比较简单的一道题，其实我们只需要遍历，对于两个字符串相同位置如果一致不用管，不一样，分别存入对应的10个桶中（分别代表每个字符串中0~9的个数），最后比较对应位置桶的大小即可。 注意：C++字fw符常量使用''，字符串常量使用&quot;&quot; 代码如下： 12345678910111213141516171819class Solution &#123; public: string getHint(string secret, string guess) &#123; vector&lt;int&gt; buk_sec(10,0),buk_gue(10,0); int a = 0; for (int i=0;i&lt;secret.size();++i) &#123; if(secret[i]==guess[i]) a++; else &#123; buk_sec[secret[i]-&#x27;0&#x27;]++; buk_gue[guess[i]-&#x27;0&#x27;]++; &#125; &#125; int b = 0; for(int i=0;i&lt;10;++i)&#123; b += min(buk_sec[i],buk_gue[i]); &#125; return to_string(a)+&quot;A&quot;+to_string(b)+&quot;B&quot;; &#125; &#125;; 2024-03-11 题目传送门：2129. 将标题首字母大写 - 力扣（LeetCode） 这道题作为一道基础模拟题，整体思路并不复杂，我将思路分为两个阶段，第一阶段将每个单词第一个字母都大写，然后将所有非首字母的部分都小写。第二阶段判断每个单词的长度，如果长度小于等于2，则将该单词首字母小写。 注意这里转换大写和转换小写可以使用toupper和tolower函数 因此最终代码如下所示： 123456789101112131415161718192021222324252627282930class Solution &#123;public: string capitalizeTitle(string title) &#123; int n = title.size(); bool flag = true; // 先将所有单词的所有首字母大写 for(auto &amp;let:title)&#123; if (let == &#x27; &#x27;) &#123; flag = true; continue; &#125; if(flag == true)&#123; flag = false; let=toupper(let); &#125; else &#123; let=tolower(let); &#125; &#125; int len = title.size(); // 再判断所有单词是否满足小于等于2，如果满足就将最前面的字符变为小写 for(int i=0;i&lt;len;++i)&#123; if (title[i] == &#x27; &#x27;) continue; int tmp = i; while(tmp&lt;len&amp;&amp;title[tmp]!=&#x27; &#x27;) tmp++; if(tmp-i&lt;=2) title[i] = tolower(title[i]); i = tmp; &#125; return title; &#125;&#125;;"},{"title":"2024年05月每日一题","path":"/wiki/LeetCode/每日一题/2024年05月每日一题.html","content":"2024-05-05 题目传送门：1652. 拆炸弹 - 力扣（LeetCode） 模拟签到题，需要注意的是，C++负数取模还是负数！！！！！！！ 所以需要先转成正数再取模。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; decrypt(vector&lt;int&gt;&amp; code, int k) &#123; int n = code.size(); vector&lt;int&gt; ans(n,0); if (k==0) return ans; for(int i=0;i&lt;n;++i)&#123; for(int j=1;j&lt;=abs(k);++j)&#123; int nw = i+(k/abs(k))*j; while(nw &lt; 0) nw += n; ans[i] += code[nw%n]; &#125; &#125; return ans; &#125;&#125;; 2024-05-10 题目传送门：2960. 统计已测试设备 - 力扣（LeetCode） 模拟签到题 最后实现代码如下： 1234567891011121314class Solution &#123;public: int countTestedDevices(vector&lt;int&gt;&amp; batteryPercentages) &#123; int n = batteryPercentages.size(); int ans = 0; for(int i=0;i&lt;n;++i)&#123; if (batteryPercentages[i]) &#123; ans ++; for(int j=i+1;j&lt;n;++j) batteryPercentages[j] = max(batteryPercentages[j]-1,0); &#125; &#125; return ans; &#125;&#125;; 2024-05-24 题目传送门：1673. 找出最具竞争力的子序列 - 力扣（LeetCode） 单调栈题目，根据题目对竞争力的定义，我们可以发现越小的数字放置的位置越前，对应的子序列越具竞争力。我们可以用类似单调栈的思想尽量将更小的元素放到子序列的前面，令 numsnumsnums 的大小为 nnn，遍历数组nums\\textit{nums}nums，假设当前访问的下标为 iii，对数字 nums[i]\\textit{nums}[i]nums[i] 执行以下操作： 记栈中的元素数目为 mmm，我们不断地进行操作直到不满足条件：如果 m&gt;0m&gt;0m&gt;0 且 m+n−i&gt;km+n−i&gt;km+n−i&gt;k 且单调栈的栈顶元素大于 nums[i]\\textit{nums}[i]nums[i]，那么说明栈顶元素可以被当前数字 nums[i]\\textit{nums}[i]nums[i] 替换，弹出单调栈的栈顶元素。 将nums[i]nums[i]nums[i]压入栈中。 最后返回栈中自下而上的前 kkk 个元素为结果。 最后实现代码如下： 12345678910111213class Solution &#123;public: vector&lt;int&gt; mostCompetitive(vector&lt;int&gt;&amp; nums, int k) &#123; vector&lt;int&gt; stk; int n = nums.size(); for(int i=0; i&lt;n; ++i) &#123; while (!stk.empty() &amp;&amp; n-i+stk.size() &gt; k &amp;&amp; stk.back() &gt; nums[i]) stk.pop_back(); stk.push_back(nums[i]); &#125; stk.resize(k); return stk; &#125;&#125;; 2024-05-25 题目传送门：2903. 找出满足差值条件的下标 I - 力扣（LeetCode） 签到题目，两个循环结束战斗。 最后实现代码如下： 123456789class Solution &#123; public: vector&lt;int&gt; findIndices(vector&lt;int&gt;&amp; nums, int indexDifference, int valueDifference) &#123; int n = nums.size(); for(int i=0; i&lt;n;++i)&#123; for(int j=i;j&lt;n;++j)&#123; if (j-i&gt;=indexDifference &amp;&amp; abs(nums[j]-nums[i])&gt;=valueDifference) return vector&lt;int&gt;&#123;i, j&#125;; &#125; &#125; return vector&lt;int&gt;&#123;-1,-1&#125;; &#125;&#125;; 2024-05-26 题目传送门：1738. 找出第 K 大的异或坐标值 - 力扣（LeetCode） 前缀和+排序即可。 最后实现代码如下： 1234567891011121314151617181920212223242526class Solution &#123;public: int kthLargestValue(vector&lt;vector&lt;int&gt;&gt;&amp; matrix, int k) &#123; int n = matrix.size(), m = matrix[0].size(); vector&lt;vector&lt;int&gt;&gt; xor_all(n+1,vector&lt;int&gt;(m+1)); for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; xor_all[i][j] = xor_all[i][j-1] ^ matrix[i-1][j-1]; &#125; &#125; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; xor_all[i][j] = xor_all[i-1][j] ^ xor_all[i][j]; &#125; &#125; vector&lt;int&gt; nw; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; nw.push_back(xor_all[i][j]); &#125; &#125; sort(nw.begin(), nw.end()); int nn = nw.size(); return nw[nn-k]; &#125;&#125;; 2024-05-27 题目传送门：2028. 找出缺失的观测数据 - 力扣（LeetCode） 模拟签到题 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: vector&lt;int&gt; missingRolls(vector&lt;int&gt;&amp; rolls, int mean, int n) &#123; int nn = rolls.size(); int N = n + nn; int all_sum = mean*N; for(auto roll:rolls) &#123; all_sum -= roll; &#125; int nw_mean = all_sum / n; vector&lt;int&gt; ans; if (nw_mean &lt; 1 || nw_mean &gt; 6) return ans; int nw_rest = all_sum - (n*nw_mean); if (nw_rest &amp;&amp; nw_mean+1&gt;6) return ans; for(int i=1;i&lt;=n;++i) &#123; if(nw_rest)ans.push_back(nw_mean+1),nw_rest--; else ans.push_back(nw_mean); &#125; return ans; &#125;&#125;; 2024-05-28 题目传送门：2951. 找出峰值 - 力扣（LeetCode） 遍历签到题。 最后实现代码如下： 1234567891011class Solution &#123;public: vector&lt;int&gt; findPeaks(vector&lt;int&gt;&amp; mountain) &#123; vector&lt;int&gt; ans; int n = mountain.size(); for(int i=1;i&lt;n-1;++i)&#123; if(mountain[i] &gt; mountain[i-1] &amp;&amp; mountain[i] &gt; mountain[i+1]) ans.push_back(i); &#125; return ans; &#125;&#125;; 2024-05-31 题目传送门：2965. 找出缺失和重复的数字 - 力扣（LeetCode） 签到题 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; findMissingAndRepeatedValues(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(); int ans1,ans2; vector&lt;int&gt; buk(3000,0); for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;n;++j)&#123; int tmp = grid[i][j]; buk[tmp]++; if (buk[tmp] == 2) ans2 = tmp; &#125; &#125; for(int i=1;i&lt;=n*n;++i)&#123; if (!buk[i]) ans1 = i; &#125; return vector&lt;int&gt;&#123;ans2, ans1&#125;; &#125;&#125;;"},{"title":"2024年04月每日一题","path":"/wiki/LeetCode/每日一题/2024年04月每日一题.html","content":"2024-04-01 题目传送门：2810. 故障键盘 - 力扣（LeetCode） 一道双端队列数据结构题，我们换一种思路来想，如果遇上字符i，旋转之前的所有字符相当于换一个方向进行插入操作，因此我们使用一个双端队列来模拟这个操作。 最后实现代码如下： 1234567891011121314151617class Solution &#123;public: string finalString(string s) &#123; deque&lt;char&gt; q; bool head = false; for (auto ch:s)&#123; if(ch!=&#x27;i&#x27;)&#123; if (head) q.push_front(ch); else q.push_back(ch); &#125; else &#123; head = !head; &#125; &#125; string ans = (head ? string&#123;q.rbegin(), q.rend()&#125; : string&#123;q.begin(), q.end()&#125;); return ans; &#125;&#125;; 2024-04-02 题目传送门：894. 所有可能的真二叉树 - 力扣（LeetCode） 这道题暴露出自己对于分治算法还是有一些遗忘，需要多加练习。整体思路比较简单，在每层递归中，如果当前不剩余节点，则返回空数组（其实这个判断只会在第一次进行，后面保证左右节点分到的个数都为奇数，因为偶数肯定不满足条件），如果只剩1个，则自己当叶子节点，否则枚举左右两边节点数量。然后进行下一轮递归。下一层递归会返回一个vector，代表不同的左右子树。然后只需排列组会一下便可得到这一层的所有子树形状，封装成vector往上返回即可。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;TreeNode*&gt; allPossibleFBT(int n) &#123; vector&lt;TreeNode*&gt; ans; if (n % 2 == 0) return ans; if (n == 1) &#123; ans = &#123;new TreeNode(0)&#125;; return ans; &#125; for(int i=1; i&lt;n; i+=2)&#123; vector&lt;TreeNode*&gt; left_ans = allPossibleFBT(i); vector&lt;TreeNode*&gt; right_ans = allPossibleFBT(n-1-i); for(TreeNode* lnode:left_ans)&#123; for(TreeNode* rnode:right_ans)&#123; TreeNode *root = new TreeNode(0, lnode, rnode); ans.push_back(root); &#125; &#125; &#125; return ans; &#125;&#125;; 2024-04-03 题目传送门：1379. 找出克隆二叉树中的相同节点 - 力扣（LeetCode） 一道简单的遍历题目，我们只需要在递归遍历的时候让克隆树执行和原树一样的操作即可。停止条件就是当克隆树节点对应的原树位置的节点等于目标节点时返回克隆树节点指针。 最后实现代码如下： 123456789101112131415161718192021222324252627/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* getTargetCopy(TreeNode* original, TreeNode* cloned, TreeNode* target) &#123; if (target==original) return cloned; if (original-&gt;left!=nullptr)&#123; TreeNode* tmp = getTargetCopy(original-&gt;left,cloned-&gt;left, target); if(tmp != nullptr) return tmp; &#125; if (cloned-&gt;right!=nullptr)&#123; TreeNode* tmp = getTargetCopy(original-&gt;right,cloned-&gt;right, target); if(tmp != nullptr) return tmp; &#125; return static_cast&lt;TreeNode*&gt;(nullptr); &#125;&#125;; 2024-04-04 题目传送门：2192. 有向无环图中一个节点的所有祖先 - 力扣（LeetCode） 一道较为明显的拓扑排序板子题目，这里在合并祖先的时候使用了vector的insert方法。详情用法可见C++-&gt;vector-&gt;insert部分的介绍。 最后实现代码如下： 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; getAncestors(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123; vector&lt;int&gt; in(n,0); vector&lt;vector&lt;int&gt;&gt; anc(n); vector&lt;vector&lt;int&gt;&gt; e(n); for(auto &amp; edge:edges)&#123; in[edge[1]]++; e[edge[0]].push_back(edge[1]); &#125; queue&lt;int&gt; que; for(int i=0;i&lt;n;++i)&#123; if (in[i]==0) que.push(i); &#125; while (!que.empty()) &#123; int u = que.front(); que.pop(); for(auto &amp;v:e[u])&#123; anc[v].push_back(u); anc[v].insert(anc[v].end(),anc[u].begin(),anc[u].end()); sort(anc[v].begin(),anc[v].end()); auto nwend = unique(anc[v].begin(),anc[v].end()); anc[v].erase(nwend, anc[v].end()); in[v]--; if (in[v]==0) que.push(v); &#125; &#125; return anc; &#125;&#125;; 2024-04-05 题目传送门：1026. 节点与其祖先之间的最大差值 - 力扣（LeetCode） 自底向上进行DFS，对于当前节点，调用dfs后返回以所有儿子为根的子树的最大值和最小值，然后更新以当前节点为根的子树的最大值和最小值。在递归的过程中，维护最大差值。最大差值ansansans可以表述为： ans=max⁡(maxx,ans)ans = \\max(maxx,ans) ans=max(maxx,ans) maxx=max⁡(abs(当前节点值−当前节点为根子树最大值),abs(当前节点值−当前节点为根子树最小值))maxx = \\max(abs(当前节点值-当前节点为根子树最大值),abs(当前节点值-当前节点为根子树最小值)) maxx=max(abs(当前节点值−当前节点为根子树最大值),abs(当前节点值−当前节点为根子树最小值)) 其中maxx代表每个节点为根的子树的最大差值 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125; * &#125;; */class Solution &#123;public: int maxAncestorDiff(TreeNode* root) &#123; int maxx = 0; function&lt;pair&lt;int,int&gt;(TreeNode*)&gt; dfs = [&amp;](TreeNode* u) &#123; pair&lt;int,int&gt; tmp = make_pair(u-&gt;val,u-&gt;val); if (u-&gt;left!=nullptr) &#123; pair&lt;int,int&gt; temp = dfs(u-&gt;left); tmp.first = min(min(temp.first, temp.second), tmp.first); tmp.second= max(max(temp.first, temp.second),tmp.second); &#125; if (u-&gt;right!=nullptr) &#123; pair&lt;int,int&gt; temp = dfs(u-&gt;right); tmp.first = min(min(temp.first, temp.second), tmp.first); tmp.second= max(max(temp.first, temp.second),tmp.second); &#125; maxx = max(maxx,max(abs(tmp.first - u-&gt;val),abs(tmp.second - u-&gt;val))); return tmp; &#125;; dfs(root); return maxx; &#125;&#125;; 2024-04-06 题目传送门：1483. 树节点的第 K 个祖先 - 力扣（LeetCode） 本题目是一道使用倍增思想的DP题目。定义ancs[i][j]代表i节点的第2j2^j2j个祖先，然后进行刷表即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334class TreeAncestor &#123;public: vector&lt;vector&lt;int&gt;&gt; ancs; constexpr static int LOG = 16; TreeAncestor(int n, vector&lt;int&gt;&amp; parent) &#123; ancs = vector&lt;vector&lt;int&gt;&gt;(n,vector&lt;int&gt;(LOG, -1)); for (int i=0; i&lt;n; ++i) &#123; ancs[i][0] = parent[i]; &#125; for (int i=1;i&lt;LOG;++i) &#123; for (int j=0; j&lt;n; ++j)&#123; if (ancs[j][i-1] != -1)&#123; ancs[j][i] = ancs[ancs[j][i-1]][i-1]; &#125; &#125; &#125; &#125; int getKthAncestor(int node, int k) &#123; for(int i=LOG; i&gt;=0; --i)&#123; if ((k&gt;&gt;i)&amp;1) &#123; node = ancs[node][i]; if (node==-1) return -1; &#125; &#125; return node; &#125;&#125;;/** * Your TreeAncestor object will be instantiated and called as such: * TreeAncestor* obj = new TreeAncestor(n, parent); * int param_1 = obj-&gt;getKthAncestor(node,k); */ 2024-04-07 题目传送门：1600. 王位继承顺序 - 力扣（LeetCode） 这道题其实就是一个多叉树的前序遍历，但是因为使用的是字符串，所以需要使用到unordered_set以及unordered_map等相关STL（使用方法见C++文档）: 最后实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041class ThroneInheritance &#123;private: unordered_map&lt;string, vector&lt;string&gt;&gt; edges; unordered_set&lt;string&gt; dead; string king;public: ThroneInheritance(string kingName) &#123; king = kingName; &#125; void birth(string parentName, string childName) &#123; edges[parentName].push_back(childName); &#125; void death(string name) &#123; dead.insert(name); &#125; vector&lt;string&gt; getInheritanceOrder() &#123; vector&lt;string&gt; ans; function&lt;void(string)&gt; dfs = [&amp;](string u) &#123; if (!dead.count(u)) ans.push_back(u); if (edges.count(u)) &#123; for (auto v:edges[u])&#123; dfs(v); &#125; &#125; &#125;; dfs(king); return ans; &#125;&#125;;/** * Your ThroneInheritance object will be instantiated and called as such: * ThroneInheritance* obj = new ThroneInheritance(kingName); * obj-&gt;birth(parentName,childName); * obj-&gt;death(name); * vector&lt;string&gt; param_3 = obj-&gt;getInheritanceOrder(); */ 2024-04-08 题目传送门：2009. 使数组连续的最少操作数 - 力扣（LeetCode） 假设xxx是修改后的连续数字的最大值，则修改后的连续数字的范围为闭区间 [x−n+1,x][x−n+1,x][x−n+1,x]，其中 nnn 是 nums 的长度。在修改前，对于已经在 [x−n+1,x][x−n+1,x][x−n+1,x] 中的数，我们无需修改。那么，xxx 取多少，可以使无需修改的数最多呢？ 由于元素的位置不影响答案，且要求所有元素互不相同，我们可以将 nums 从小到大排序，并去掉重复元素。设 a 为nums 排序去重后的数组。将 a[i] 画在一条数轴上，本题相当于有一个长度为 n 的滑动窗口，我们需要计算窗口内最多可以包含多少个数轴上的点。 最后实现代码如下： 1234567891011121314class Solution &#123;public: int minOperations(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(),nums.end()); int n = nums.size(); int now_n = unique(nums.begin(), nums.end()) - nums.begin(); int ans = 0, r = 0; for (int l=0; l&lt;now_n; ++l) &#123; while (r&lt;now_n &amp;&amp; nums[r]-nums[l] &lt; n) r++; ans = max(r-l, ans); &#125; return n - ans; &#125;&#125;; 2024-04-09 题目传送门：2529. 正整数和负整数的最大计数 - 力扣（LeetCode） 纯签到题。 最后实现代码如下： 1234567891011class Solution &#123;public: int maximumCount(vector&lt;int&gt;&amp; nums) &#123; int a = 0,b = 0; for(auto&amp;num:nums)&#123; if (num&gt;0) a++; if (num&lt;0) b++; &#125; return max(a, b); &#125;&#125;; 2024-04-10 题目传送门：1702. 修改后的最大二进制字符串 - 力扣（LeetCode） 这道题题解建议看灵神的推理方法，讲解的十分透彻。 题解传送门：贪心，简洁写法（Python/Java/C++/Go/JS/Rust） 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: string maximumBinaryString(string binary) &#123; int n = binary.size(); int j = 0; for (int i = 0; i &lt; n; i++) &#123; if (binary[i] == &#x27;0&#x27;) &#123; while (j &lt;= i || (j &lt; n &amp;&amp; binary[j] == &#x27;1&#x27;)) &#123; j++; &#125; if (j &lt; n) &#123; binary[j] = &#x27;1&#x27;; binary[i] = &#x27;1&#x27;; binary[i + 1] = &#x27;0&#x27;; &#125; &#125; &#125; return binary; &#125;&#125;; 2024-04-11 题目传送门：1766. 互质树 - 力扣（LeetCode） 这道题就是一道DFS题目，因为取值非常小，所以可以先进行预处理，先把50以内所有互质的数预处理出来。由于根节点到任意节点的搜索路径就是该节点的祖先节点的集合，所以搜索路径上最近的与其互质的就是答案。在搜索遍历树处理答案的时候，有许多信息可以复用。 最后实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Solution &#123;private: vector&lt;vector&lt;int&gt;&gt; gcds; vector&lt;vector&lt;int&gt;&gt; tmp; vector&lt;vector&lt;int&gt;&gt; g; vector&lt;int&gt; dep; vector&lt;int&gt; ans;public: void dfs(vector&lt;int&gt; &amp;nums, int x, int depth) &#123; dep[x] = depth; for (int val : gcds[nums[x]]) &#123; if (tmp[val].empty()) &#123; continue; &#125; int las = tmp[val].back(); if (ans[x] == -1 || dep[las] &gt; dep[ans[x]]) &#123; ans[x] = las; &#125; &#125; tmp[nums[x]].push_back(x); for(int val : g[x]) &#123; if (dep[val] == -1) &#123; // 被访问过的点dep不为-1 dfs(nums, val, depth + 1); &#125; &#125; tmp[nums[x]].pop_back(); &#125; vector&lt;int&gt; getCoprimes(vector&lt;int&gt;&amp; nums, vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123; int n = nums.size(); // 初始化 gcds.resize(51); tmp.resize(51); ans.resize(n, -1); dep.resize(n, -1); g.resize(n); for (int i = 1; i &lt;= 50; i++) &#123; for (int j = 1; j &lt;= 50; j++) &#123; if (gcd(i, j) == 1) &#123; gcds[i].push_back(j); &#125; &#125; &#125; for (const auto &amp;val : edges) &#123; g[val[0]].push_back(val[1]); g[val[1]].push_back(val[0]); &#125; dfs(nums, 0, 1); return ans; &#125;&#125;; 2024-04-12 题目传送门：2923. 找到冠军 I - 力扣（LeetCode） 有点像拓扑排序第一步，统计每个节点的入度。这里我们设定A比B强，表示有一条边由A指向B。最后我们返回入度为0的节点即可。 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int findChampion(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(); vector&lt;int&gt; rank(n,0); for(int i=0; i&lt;n; ++i)&#123; for(int j=0; j&lt;n; ++j)&#123; if (grid[i][j] == 1) &#123; rank[j] ++; &#125; &#125; &#125; for (int i=0; i&lt;n; ++i)&#123; if (rank[i]==0) return i; &#125; return -1; &#125;&#125;; 2024-04-13 题目传送门：2924. 找到冠军 II - 力扣（LeetCode） 与昨天的题目思路完全一样，最后实现代码如下： 123456789101112131415class Solution &#123;public: int findChampion(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges) &#123; vector&lt;int&gt; ins(n, 0); for(auto &amp;edge:edges) &#123; ins[edge[1]]++; &#125; int cnt = 0, ans = -1; for(int i=0;i&lt;n;++i) &#123; if (ins[i]==0) cnt++,ans=i; &#125; if (cnt == 1) return ans; return -1; &#125;&#125;; 2024-04-14 题目传送门：705. 设计哈希集合 - 力扣（LeetCode） STL unordered_set 练习题，最后实现代码如下： 1234567891011121314151617class MyHashSet &#123;private: unordered_set&lt;int&gt; myset;public: MyHashSet() &#123;&#125; void add(int key) &#123;myset.insert(key);&#125; void remove(int key) &#123;myset.erase(key);&#125; bool contains(int key) &#123;return myset.find(key) != myset.end();&#125;&#125;;/** * Your MyHashSet object will be instantiated and called as such: * MyHashSet* obj = new MyHashSet(); * obj-&gt;add(key); * obj-&gt;remove(key); * bool param_3 = obj-&gt;contains(key); */ 2024-04-15 题目传送门：706. 设计哈希映射 - 力扣（LeetCode） STL unordered_map 练习题，最后实现代码如下： 1234567891011121314151617181920class MyHashMap &#123;private: unordered_map&lt;int, int&gt; mp;public: void put(int key, int value) &#123;mp[key] = value;&#125; int get(int key) &#123; if (mp.find(key)!=mp.end()) return mp[key]; return -1; &#125; void remove(int key) &#123;mp.erase(key);&#125;&#125;;/** * Your MyHashMap object will be instantiated and called as such: * MyHashMap* obj = new MyHashMap(); * obj-&gt;put(key,value); * int param_2 = obj-&gt;get(key); * obj-&gt;remove(key); */ 2024-04-16 题目传送门：924. 尽量减少恶意软件的传播 - 力扣（LeetCode） 一道细节较多的DFS/并查集题目，其实就是找init中存在的最大连通块（第一次读题都快了以为是找图中的割点），只不过找到了需要注意： 如果一个连通块中，只有一个initial中的节点被感染，则将他移除这个连通块不会感染。但是如果有2个及以上的节点被感染，则删掉一个以后这个连通块仍然会被感染。所以具体来说我们需要找的是只有一个initial节点在连通块中的最大连通块。（判断两个点是否在一个连通块中可以通过并查集或者unordered_set） 还有需要注意的是，存在特殊情况找不到一个点在一个连通块中（每个连通块都存在2个及以上的initial中的点），此时返回下标最小的initial点即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031class Solution &#123;public: int minMalwareSpread(vector&lt;vector&lt;int&gt;&gt;&amp; graph, vector&lt;int&gt;&amp; initial) &#123; int n = graph.size(), m = initial.size(); sort(initial.begin(),initial.end()); // 排序方便选最小，这样如果都在一个连通块中也可很快选出最小的 vector&lt;bool&gt; vis(n, false); unordered_set&lt;int&gt; se; for (auto init:initial) se.insert(init); function&lt;pair&lt;int, bool&gt;(int)&gt; dfs = [&amp;](int u)&#123; int num = 1; bool flag = false; for(int i=0;i&lt;n;++i)&#123; if (graph[u][i] &amp;&amp; !vis[i]) &#123; vis[i] = true; if (se.find(i)!=se.end()) flag = true; pair&lt;int,bool&gt; tmp = dfs(i); num += tmp.first; flag |= tmp.second; &#125; &#125; return make_pair(num, flag); &#125;; int ans=initial[0],nw=0; for (auto init:initial) &#123; if (vis[init]) continue; vis[init] = true; pair&lt;int,bool&gt; tmp = dfs(init); if (tmp.second!=true &amp;&amp; (tmp.first &gt; nw || (tmp.first == nw &amp;&amp; init &lt; ans))) nw=tmp.first, ans=init; &#125; return ans; &#125;&#125;; 2024-04-17 题目传送门：928. 尽量减少恶意软件的传播 II - 力扣（LeetCode） 此题为上一道题目的衍生版本，最后实现代码如下： 1234567891011121314151617class Solution &#123;public: int minMalwareSpread(vector&lt;vector&lt;int&gt;&gt;&amp; graph, vector&lt;int&gt;&amp; initial) &#123; int n = graph.size(); vector&lt;int&gt; init_size(n); for (auto init:initial) init_size[init] = 1; vector&lt;int&gt; ufa(n); iota(ufa.begin(), ufa.end(), 0); for (int u=0; u&lt;n; ++u)&#123; if(init_size[u]==1) continue; for(int v=0; v&lt;n; ++v)&#123; if(init_size[v]==1) continue; if (graph[u][v] == 1) &#123;merge(uf, u, v);&#125; &#125; &#125; &#125;&#125;; 2024-04-18 题目传送门：2007. 从双倍数组中还原原数组 - 力扣（LeetCode） STL map使用题，使用一个map记录当前键值以及当前键值所出现的次数，遍历map是按照键值从小到大顺序的，对于当前键值，如果*2后的键值出现次数大于等于当前键值数量，则将当前键值加入答案队列中（数量保持一致），否则则证明不存在这个数列，返回空。注意0进行特殊判定一下。 最后实现代码如下： 1234567891011121314151617181920212223242526class Solution &#123;public: vector&lt;int&gt; findOriginalArray(vector&lt;int&gt;&amp; changed) &#123; sort(changed.begin(), changed.end()); map&lt;int, int&gt; mp; vector&lt;int&gt; ans=&#123;&#125;; for(auto &amp;i:changed) &#123; if (mp.find(i)==mp.end()) mp[i] = 1; else mp[i]++; &#125; for(auto it = mp.begin();it != mp.end(); ++it)&#123; if (it-&gt;first == 0 &amp;&amp; it-&gt;second%2) return vector&lt;int&gt;&#123;&#125;; if (it-&gt;second == 0) continue; auto it2 = mp.find(it-&gt;first*2); if (it2!=mp.end() &amp;&amp; it2-&gt;second &gt;= it-&gt;second) &#123; if (it-&gt;first == 0)it-&gt;second /= 2; for(int i=1;i&lt;=it-&gt;second;++i) ans.push_back(it-&gt;first); it2-&gt;second -= it-&gt;second; it-&gt;second = 0; &#125; else &#123; return vector&lt;int&gt;&#123;&#125;; &#125; &#125; return ans; &#125;&#125;; 2024-04-19 题目传送门：1883. 准时抵达会议现场的最小跳过休息次数 - 力扣（LeetCode） 读完题目后，观察数据范围不难判断此题应该是一道dp，设dp[i][j]代表到达第i个地方使用j次跳过所需要的最小时间（i&gt;j）。可以非常简单写出状态转移方程： 12double tmp_t = (double)dist[i-1]/speed;dp[i][j] = min(ceil(dp[i-1][j]) + tmp_t, dp[i-1][j-1]+tmp_t) 下面再来讨论一下边界以及初始化条件，首先为了方便起见，将所有dp数组初始化成一个非常大的int值，dp[0][0] = 0, 当j=0时，不能使用上面的状态转移方程(存在j-1)项，因此可以写为 1dp[i][0] = ceil(dp[i-1][0]-EPS)+tmp_t; 以上便是这道DP的核心部分，但是当我提交后发现一直过不了。这是因为： 浮点数运算的细节（摘自LeetCode官方题解） 根据 IEEE 754 标准，浮点数在计算机中存储的精度是有限的，而本题中我们不可避免的会使用「浮点数运算」以及「向上取整」运算，如果强行忽略产生的计算误差，会得到错误的结果。 举一个简单的例子，假设使用的语言中「向上取整」运算的函数为 ceil，下面的运算： 1ceil(8.0 + 1.0 / 3 + 1.0 / 3 + 1.0 / 3) 应当是 9，而计算机会给出 10。这是因为浮点数误差导致： 18.0 + 1.0 / 3 + 1.0 / 3 + 1.0 / 3 计算出的结果约为： 19.000000000000002 向上取整后会得到 10，产生了错误的答案。 因此我们引入常量 EPS 表示一个极小值，例如 10−710^{-7}10−7 。在进行「向上取整」运算前，我们将待取整的浮点数减去 EPS 再进行取整，就可以避免上述的误差。 同时，我们需要说明 EPS 不会引入其它的问题。在本题中速度最大为 10610^{6}106，时间与速度成反比，那么 EPS 的粒度只需要高于时间的精度10−610^{-6}10−6即可，取10−710^{-7}10−7或 110−810^{-8}10−8都是可行的。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;private: // 可忽略误差 static constexpr double EPS = 1e-7;public: int minSkips(vector&lt;int&gt;&amp; dist, int speed, int hoursBefore) &#123; int n = dist.size(); vector&lt;vector&lt;double&gt;&gt; dp(1002,vector&lt;double&gt;(1002,1e8+7)); dp[0][0] = 0; for(int i=1;i&lt;=n;++i)&#123; double tmp_t = (double)dist[i-1]/speed; dp[i][0] = ceil(dp[i-1][0]-EPS)+tmp_t; for(int j=1;j&lt;i;++j)&#123; dp[i][j] = min(ceil(dp[i-1][j]-EPS)+tmp_t,dp[i-1][j-1]+tmp_t); &#125; &#125; for(int i=0;i&lt;n;++i)&#123; if (dp[n][i] &lt;= hoursBefore) return i; &#125; return -1; &#125;&#125;; 就是因为加了EPS才AC了，我们C++真是太神奇辣！！！ 2024-04-20 题目传送门：39. 组合总和 - 力扣（LeetCode） 打暴搜即可，超过target进行剪枝。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) &#123; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; tmp; int n = candidates.size(); function&lt;void(int,int)&gt; dfs = [&amp;](int loc, int nw_sum) &#123; if (nw_sum &gt; target) return; if (nw_sum == target) &#123; ans.push_back(tmp); return; &#125; for(int i=loc;i&lt;n;++i)&#123; tmp.push_back(candidates[i]); dfs(i, nw_sum+candidates[i]); tmp.pop_back(); &#125; &#125;; dfs(0, 0); return ans; &#125;&#125;; 2024-04-21 题目传送门：216. 组合总和 III - 力扣（LeetCode） 和昨天题目差不多大暴搜，多了一个剪枝条件加上即可。 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; tmp; function&lt;void(int,int,int)&gt; dfs = [&amp;](int cnt,int loc,int nw_sum) &#123; if (nw_sum &gt; n || cnt &gt; k) return; if (nw_sum == n &amp;&amp; cnt == k)&#123; ans.push_back(tmp);return; &#125; for(int i=loc;i&lt;=9;++i)&#123; tmp.push_back(i); dfs(cnt+1,i+1,nw_sum+i); tmp.pop_back(); &#125; &#125;; dfs(0,1,0); return ans; &#125;&#125;; 2024-04-22 题目传送门：377. 组合总和 Ⅳ - 力扣（LeetCode） 刚看到这个题还没反应过来是DP，直接一发暴搜，但是超时了，让我仔细思考了一下，发现其实就是一个和爬楼梯一样的简单DP。 但是需要注意的是，可能会存在溢出的情况，所以我们的dp数组开成unsigned类型 最后实现代码如下： 123456789101112131415class Solution &#123;public: int combinationSum4(vector&lt;int&gt;&amp; nums, int target) &#123; vector&lt;unsigned&gt; dp(target+3,0); dp[0] = 1; for(int i=1;i&lt;=target;++i)&#123; for(auto num:nums)&#123; if (num &lt;= i)&#123; dp[i] += dp[i-num]; &#125; &#125; &#125; return dp[target]; &#125;&#125;; 2024-04-23 题目传送门：1052. 爱生气的书店老板 - 力扣（LeetCode） 滑动窗口一下，很简单的啦！ 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int maxSatisfied(vector&lt;int&gt;&amp; customers, vector&lt;int&gt;&amp; grumpy, int minutes) &#123; int init=0,n=customers.size(); for(int i=0; i&lt;n; ++i) if (!grumpy[i]) init += customers[i]; int tmp = 0; for(int i=0; i&lt;minutes; ++i) if(grumpy[i]) tmp += customers[i]; int ans = tmp; for(int i=1;i&lt;=n-minutes;++i)&#123; if (grumpy[i-1]) tmp -= customers[i-1]; if (grumpy[i-1+minutes]) tmp += customers[i-1+minutes]; if (tmp &gt; ans) ans = tmp; &#125; return init + ans; &#125;&#125;; 2024-04-24 题目传送门：2385. 感染二叉树需要的总时间 - 力扣（LeetCode） 我首先想到的办法是做两次DFS，第一遍建图，第二遍以给定节点为根，算最长时间。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x) : val(x), left(nullptr), right(nullptr) &#123;&#125; * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) &#123;&#125; * &#125;; */class Solution &#123; public: int amountOfTime(TreeNode* root, int start) &#123; vector&lt;vector&lt;int&gt;&gt; edges(100003); function&lt;void(TreeNode*)&gt; dfs = [&amp;](TreeNode* u) &#123; if(u-&gt;left!=nullptr)&#123; edges[u-&gt;val].emplace_back(u-&gt;left-&gt;val); edges[u-&gt;left-&gt;val].emplace_back(u-&gt;val); dfs(u-&gt;left); &#125; if(u-&gt;right!=nullptr)&#123; edges[u-&gt;val].emplace_back(u-&gt;right-&gt;val); edges[u-&gt;right-&gt;val].emplace_back(u-&gt;val); dfs(u-&gt;right); &#125; &#125;; dfs(root); function&lt;int(int,int)&gt; dfs2 = [&amp;](int u,int fa)&#123; int tmp=0; for (auto v:edges[u]) &#123; if (v==fa) continue; tmp = max(tmp, dfs2(v,u)); &#125; return 1+tmp; &#125;; return dfs2(start,-1)-1; &#125;&#125;; 但是上面这个代码太极限了，卡着时限交了三次评测机过了一次（笑）： 于是需要再想想有没有改进方法。 2024-04-25 题目传送门：2739. 总行驶距离 - 力扣（LeetCode） 签到题目，最后实现代码如下： 1234567891011class Solution &#123;public: int distanceTraveled(int mainTank, int additionalTank) &#123; int ans = 0; while (mainTank &gt;=5 ) &#123; ans += 50; mainTank -= 5; if (additionalTank) mainTank++,additionalTank--; &#125; return ans + mainTank*10; &#125;&#125;; 2024-04-26 题目传送门：1146. 快照数组 - 力扣（LeetCode） 此题如果想到了是二分+哈希表就比较容易了，二分还是比较好想，观察数据范围为50000，便可知每次进行snap操作，对整个数组进行复制是不现实的。 对于此题，可以采用unordered_set对历史修改进行维护： 1unordered_map&lt;int, vector&lt;pair&lt;int, int&gt;&gt;&gt; history; 其中，map的索引代表的是位置，pair的第一个值记录对当前位置进行修改时的snap_id,第二个值记录对当前位置修改时，更改的值val。后续进行get操作时，只需要对pair第一个进行二分即可（找小于等于给定snap_id的最后一个更改值） 最后实现代码如下： 1234567891011121314151617181920212223242526272829303132333435class SnapshotArray &#123;private: int cnt = 0; unordered_map&lt;int, vector&lt;pair&lt;int, int&gt;&gt;&gt; history; public: SnapshotArray(int length) &#123;&#125; void set(int index, int val) &#123; history[index].emplace_back(cnt, val); &#125; int snap() &#123; return cnt++; &#125; int get(int index, int snap_id) &#123; auto&amp; tmp = history[index]; int l = 0, r = tmp.size(); while (l &lt; r) &#123; int mid = l + ((r-l)&gt;&gt;1); if (tmp[mid].first &gt; snap_id) r = mid; else l = mid+1; &#125; if ((l-1)&lt;0) return 0; return tmp[l-1].second; &#125;&#125;;/** * Your SnapshotArray object will be instantiated and called as such: * SnapshotArray* obj = new SnapshotArray(length); * obj-&gt;set(index,val); * int param_2 = obj-&gt;snap(); * int param_3 = obj-&gt;get(index,snap_id); */ 2024-04-27 题目传送门：2639. 查询网格图中每一列的宽度 - 力扣（LeetCode） 签到题目，最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: vector&lt;int&gt; findColumnWidth(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;int&gt; res(m); for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; int x = grid[i][j]; int length = 0; if (x &lt;= 0) &#123; length = 1; &#125; while (x != 0) &#123; length += 1; x /= 10; &#125; res[j] = max(res[j], length); &#125; &#125; return res; &#125;&#125;; 2024-04-28 题目传送门：1017. 负二进制转换 - 力扣（LeetCode） 这里需要使用到我们小学所学进制转换的知识，让我们来回顾一下： 负二进制表示的定义 负二进制数表示形式使用基数 -2。对于一个整数 nnn，我们希望将其表示为负二进制形式： n=a0⋅(−2)0+a1⋅(−2)1+a2⋅(−2)2+…n = a_0 \\cdot (-2)^0 + a_1 \\cdot (-2)^1 + a_2 \\cdot (-2)^2 + \\ldots n=a0​⋅(−2)0+a1​⋅(−2)1+a2​⋅(−2)2+… 其中 aia_iai​ 是二进制位，取值只能为0或1。 取模的意义 当我们说对2取模时，我们的目标是找到当前位 aia_iai​ 的值，并调整 nnn 以继续处理下一位。 转换过程 假设我们在某一步计算中，当前的值为 nnn。 确定当前位的值： ai=nmod 2a_i = n \\mod 2 ai​=nmod2 然而，对于负数取模，可能会得到负数结果。为确保 aia_iai​ 是0或1，我们使用如下公式： ai=(nmod 2+2)mod 2a_i = (n\\mod 2 + 2) \\mod 2 ai​=(nmod2+2)mod2 这个公式确保了 aia_iai​ 结果为0或1，无论 nnn 是正数还是负数。（非常重要！！！） 调整nnn的值： 由于我们已经确定了当前位 aia_iai​ 的值，我们需要调整 nnn 以便处理下一位。调整方法是： n=n−ain = n - a_i n=n−ai​ 这一步确保我们已经扣除了当前位的影响。 处理下一位： 我们需要将 nnn 除以基数 -2，以继续处理下一位： n=n−2n = \\frac{n}{-2} n=−2n​ 通过这种方式，调整后的 nnn 准备好处理下一位。 数学解释 假设我们在第 kkk 步，有当前值 nkn_knk​： nk=ak⋅(−2)k+ak+1⋅(−2)k+1+ak+2⋅(−2)k+2+…n_k = a_k \\cdot (-2)^k + a_{k+1} \\cdot (-2)^{k+1} + a_{k+2} \\cdot (-2)^{k+2} + \\ldots nk​=ak​⋅(−2)k+ak+1​⋅(−2)k+1+ak+2​⋅(−2)k+2+… 通过上述步骤，我们确定当前位 aka_kak​： ak=(nkmod 2+2)mod 2a_k = (n_k \\mod 2 + 2) \\mod 2 ak​=(nk​mod2+2)mod2 然后，我们调整 nnn 以扣除当前位： nk′=nk−akn_k&#x27; = n_k - a_k nk′​=nk​−ak​ 最后，我们将调整后的 nk′n_k&#x27;nk′​ 除以 -2，以继续处理下一位： nk+1=nk′−2n_{k+1} = \\frac{n_k&#x27;}{-2} nk+1​=−2nk′​​ 代码中的体现 将上述逻辑表示为代码,最后实现代码如下： 123456789101112131415class Solution &#123;public: string baseNeg2(int n) &#123; if (n == 0 || n == 1) return to_string(n); string res; while (n != 0) &#123; int rem = (n % 2 + 2) % 2; // 确定当前位的值 res.push_back(&#x27;0&#x27; + rem); // 将当前位添加到结果字符串 n -= rem; // 调整 n 的值 n /= -2; // 准备处理下一位 &#125; reverse(res.begin(), res.end()); // 反转结果字符串 return res; &#125;&#125;; 总结 通过对2取模并进行调整，我们可以确保每一位 aia_iai​ 是0或1，并且通过除以-2继续处理下一位。这种方法利用了数学上的取模和调整，使得负二进制转换过程简洁而高效。 2024-04-29 题目传送门：1329. 将矩阵按对角线排序 - 力扣（LeetCode） 模拟一下，对每个对角线都来排个序就可以了，非常的简单。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; diagonalSort(vector&lt;vector&lt;int&gt;&gt;&amp; mat) &#123; int n = mat.size(), m = mat[0].size(); vector&lt;int&gt; tmp; for(int i=0;i&lt;n;++i)&#123; int cnt = 0; while(i+cnt &lt; n &amp;&amp; cnt &lt; m) tmp.emplace_back(mat[i+cnt][cnt]),cnt++; sort(tmp.begin(),tmp.end()); cnt -= 1; while(cnt &gt;= 0) mat[i+cnt][cnt] = tmp[cnt],cnt--,tmp.pop_back(); &#125; for(int i=0;i&lt;m;++i)&#123; int cnt = 0; while(cnt &lt; n &amp;&amp; cnt+i &lt; m) tmp.emplace_back(mat[cnt][cnt+i]),cnt++; sort(tmp.begin(),tmp.end()); cnt -= 1; while(cnt &gt;= 0) mat[cnt][cnt+i] = tmp[cnt],cnt--,tmp.pop_back();; &#125; return mat; &#125;&#125;; 2024-04-30 题目传送门：2798. 满足目标工作时长的员工数目 - 力扣（LeetCode） 签到题目，循环一遍比大小即可，最后实现代码如下： 123456789101112class Solution &#123;public: int numberOfEmployeesWhoMetTarget(vector&lt;int&gt;&amp; hours, int target) &#123; int ans = 0; for (int i = 0; i &lt; hours.size(); i++) &#123; if (hours[i] &gt;= target) &#123; ans++; &#125; &#125; return ans; &#125;&#125;;"},{"title":"2024年06月每日一题","path":"/wiki/LeetCode/每日一题/2024年06月每日一题.html","content":"2024-06-01 题目传送门：2928. 给小朋友们分糖果 I - 力扣（LeetCode） 签到题目，使用暴搜即可（后来发现只有三个人，直接两个循环嵌套也可以哈哈）。 六一儿童节快乐🎇🎉😊 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int distributeCandies(int n, int limit) &#123; int ans = 0; function&lt;void(int, int)&gt; dfs =[&amp;](int cnt,int res) &#123; if(cnt == 3 &amp;&amp; res == 0) &#123; ans ++; return; &#125; if(cnt == 2)&#123; if(res&lt;=limit) dfs(cnt+1,0); &#125; else &#123; int tmp = min(res,limit); for(int i=0;i&lt;=tmp;++i)&#123; dfs(cnt+1,res-i); &#125; &#125; &#125;; dfs(0,n); return ans; &#125;&#125;; 2024-06-02 题目传送门：575. 分糖果 - 力扣（LeetCode） 签到题目，不再赘述。 最后实现代码如下： 12345678910111213class Solution &#123;public: int distributeCandies(vector&lt;int&gt;&amp; candyType) &#123; sort(candyType.begin(), candyType.end()); int n = candyType.size(); int m = 1; for(int i=1; i&lt;n; ++i) &#123; if (candyType[i] == candyType[i-1]) continue; m++; &#125; return min(m, n/2); &#125;&#125;; 2024-06-03 题目传送门：1103. 分糖果 II - 力扣（LeetCode） 签到题目，直接大模拟或者二分求出每次最大分配值以后再模拟都可以，这里我选择使用二分。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123;public: vector&lt;int&gt; distributeCandies(int candies, int num_people) &#123; function&lt;bool(int,int)&gt; addup = [&amp;](int x,int tar)&#123; int xx = 0; for(int i=1;i&lt;=x;++i)&#123; xx += i; if (xx &gt; tar) return false; &#125; return true; &#125;; int l=1, r=candies/num_people+1000; while (l&lt;r) &#123; int mid = l + ((r-l)&gt;&gt;1); if (addup(mid,candies)) l = mid+1; else r = mid; &#125; l -= 1; int res=0; for(int i=1;i&lt;=l;++i)&#123; res += i; &#125; res = candies - res; vector&lt;int&gt; ans; for(int i=1;i&lt;=num_people;++i)&#123; int tmp = i; int nans = 0; while(tmp&lt;=l) &#123; nans += tmp; tmp += num_people; &#125; if (tmp == l+1) nans += res; ans.push_back(nans); &#125; return ans; &#125;&#125;; 2024-06-04 题目传送门：3067. 在带权树网络中统计可连接服务器对数目 - 力扣（LeetCode） 这道题是一道较为典型的搜索回溯的题目，递归函数每次返回当前节点下（包含当前节点）满足距离整除signalSpeed的节点数量。 需要注意的是，在回溯的时候最后判断一下当前递归函数，是不是在递归根节点就可以了（最后一步操作和其余节点的操作不太一样）。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: vector&lt;int&gt; countPairsOfConnectableServers(vector&lt;vector&lt;int&gt;&gt;&amp; edges, int signalSpeed) &#123; int n = edges.size()+1; vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt;es(n); for(auto &amp;edge : edges) &#123; es[edge[0]].emplace_back(edge[1],edge[2]); es[edge[1]].emplace_back(edge[0],edge[2]); &#125; vector&lt;int&gt; final_ans; function&lt;int(int, int, int)&gt; dfs = [&amp;](int u, int fa, int nw_dis) &#123; int nw = 0; if (fa != -1) &#123; if (nw_dis%signalSpeed==0) nw += 1; for (auto &amp;tmp : es[u]) &#123; if (tmp.first == fa) continue; nw += dfs(tmp.first,u,nw_dis+tmp.second); &#125; &#125; else &#123; vector&lt;int&gt; ans; for (auto &amp;tmp : es[u]) &#123; if (tmp.first == fa) continue; ans.push_back(dfs(tmp.first,u,nw_dis+tmp.second)); &#125; int nn = ans.size(); for(int i=0;i&lt;nn;++i)&#123; for(int j=i+1;j&lt;nn;++j)&#123; nw += ans[i] * ans[j]; &#125; &#125; &#125; return nw; &#125;; for (int i=0;i&lt;n;++i) &#123; final_ans.push_back(dfs(i,-1,0)); &#125; return final_ans; &#125;&#125;; 2024-06-05 题目传送门：3072. 将元素分配到两个数组中 II - 力扣（LeetCode） 离散化+树状数组 首先对队列中的值进行一个离散化操作(使用unordered_map),然后分别对两个队列维护两个树状数组进行查询和更新即可（前缀和查询+单点更新） 注意：需要注意的是题目要求判断的是严格大于，因此需要进行一定的操作将后缀和转化为前缀和；还有就是树状数组的下标从1开始。 最后实现代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution &#123;public: vector&lt;int&gt; resultArray(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); function&lt;int(int)&gt; lowbit = [&amp;](int x)&#123; return x &amp; (-x); &#125;; function&lt;void(vector&lt;int&gt;&amp;,int)&gt; updateBIT = [&amp;](vector&lt;int&gt;&amp; bit, int a)&#123; int n = bit.size(); while(a&lt;n)&#123; bit[a]++; a += lowbit(a); &#125; &#125;; function&lt;int(vector&lt;int&gt;&amp;,int)&gt; queryBIT = [&amp;](vector&lt;int&gt;&amp; bit,int a)&#123; int sum = 0; while(a&gt;0)&#123; sum += bit[a]; a -= lowbit(a); &#125; return sum; &#125;; vector&lt;int&gt; bit1(100002, 0), bit2(100002, 0); vector&lt;int&gt; arr1, arr2, ans, nnums = nums; unordered_map&lt;int,int&gt; mp; sort(nnums.begin(), nnums.end()); int cnt = 0, prev = -1; for(auto num : nnums) &#123; if (num != prev) cnt++; mp[num] = cnt; prev = num; &#125; arr1.push_back(0); updateBIT(bit1, cnt+1-mp[nums[0]]); arr2.push_back(1); updateBIT(bit2, cnt+1-mp[nums[1]]); for(int i=2;i&lt;n;++i)&#123; int q1 = queryBIT(bit1, cnt-mp[nums[i]]); int q2 = queryBIT(bit2, cnt-mp[nums[i]]); if (q1 &gt; q2) updateBIT(bit1, cnt+1-mp[nums[i]]),arr1.push_back(i); else if (q1 &lt; q2) updateBIT(bit2, cnt+1-mp[nums[i]]),arr2.push_back(i); else &#123; if (arr1.size() &lt;= arr2.size()) updateBIT(bit1, cnt+1-mp[nums[i]]),arr1.push_back(i); else updateBIT(bit2, cnt+1-mp[nums[i]]),arr2.push_back(i); &#125; &#125; for(int arr:arr1)&#123;ans.push_back(nums[arr]);&#125; for(int arr:arr2)&#123;ans.push_back(nums[arr]);&#125; return ans; &#125;&#125;; 2024-06-06 题目传送门：2938. 区分黑球与白球 - 力扣（LeetCode） 此题可以简单的抽象为一道贪心题目，「将所有的黑球都移动到右侧，将所有的白球都移动到左侧」，即0都在1的左侧。不难想到，对于每个白球需要移动多少次，取决于他前面有多少个黑球。因此遍历队列进行贪心即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: long long minimumSteps(string s) &#123; int n = s.size(); long long ans=0; int tmp_blk = 0; for(int i=0;i&lt;n;++i)&#123; if(s[i]==&#x27;0&#x27;) ans += tmp_blk; else tmp_blk++; &#125; return ans; &#125;&#125;; 2024-06-07 题目传送门：3038. 相同分数的最大操作数目 I - 力扣（LeetCode） 签到遍历题。 最后实现代码如下： 1234567891011121314class Solution &#123;public: int maxOperations(vector&lt;int&gt;&amp; nums) &#123; int n=nums.size(),poi = 0,pre=-1,ans=0; while(poi+2&lt;=nums.size())&#123; int tmp = nums[poi] + nums[poi+1]; if(poi==0) pre = tmp,ans++; else if(pre == tmp) ans++; else break; poi += 2; &#125; return ans; &#125;&#125;; 2024-06-08 题目传送门：3040. 相同分数的最大操作数目 II - 力扣（LeetCode） 刚做没有仔细看题目，以为是一个大暴搜，然后毫无意外的TLE了： 123456789101112131415class Solution &#123;public: int maxOperations(vector&lt;int&gt;&amp; nums) &#123; int ans = 0, n = nums.size(); function&lt;void(int,int,int,int)&gt; dfs = [&amp;](int l, int r, int sum_num, int tmp_ans) &#123; ans = max(ans, tmp_ans); if (r-l+1&lt;2) return; if (nums[l]+nums[l+1] == sum_num || (l == 0 &amp;&amp; r == n-1)) dfs(l+2, r, nums[l]+nums[l+1], tmp_ans+1); if (nums[r]+nums[r-1] == sum_num || (l == 0 &amp;&amp; r == n-1)) dfs(l, r-2, nums[r]+nums[r-1], tmp_ans+1); if (nums[l] + nums[r] == sum_num || (l == 0 &amp;&amp; r == n-1)) dfs(l+1,r-1,nums[r]+nums[l], tmp_ans+1); &#125;; dfs(0,n-1,-1,0); return ans; &#125;&#125;; 仔细思考后，发现其实target只会存在三种情况： nums[0] + nums[1] nums[0] + nums[n-1] nums[n-2] + nums[n-1] 枚举这三种情况即可，因此可以使用记忆化搜索解决 最后实现代码如下： 123456789101112131415161718192021222324class Solution &#123;public: int maxOperations(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(), ans = 0; vector&lt;vector&lt;int&gt;&gt; dp(n,vector&lt;int&gt;(n, -1)); function&lt;int(int,int,int)&gt; dfs = [&amp;](int l, int r, int sum_num)&#123; // ans = max(ans, tmp_ans); if (r-l+1&lt;2) return 0; if (dp[l][r]!=-1) return dp[l][r]; int tmp = 0; if (nums[l]+nums[r]==sum_num) tmp = max(tmp, 1+dfs(l+1, r-1, sum_num)); if (nums[l]+nums[l+1]==sum_num) tmp = max(tmp, 1+dfs(l+2, r, sum_num)); if (nums[r]+nums[r-1]==sum_num) tmp = max(tmp, 1+dfs(l, r-2, sum_num)); dp[l][r] = tmp; return dp[l][r]; &#125;; ans = max(ans, 1+dfs(2, n-1, nums[0]+nums[1])); dp = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(n, -1)); ans = max(ans, 1+dfs(1, n-2, nums[0]+nums[n-1])); dp = vector&lt;vector&lt;int&gt;&gt;(n, vector&lt;int&gt;(n, -1)); ans = max(ans, 1+dfs(0, n-3, nums[n-2]+nums[n-1])); return ans; &#125;&#125;; 2024-06-09 题目传送门：312. 戳气球 - 力扣（LeetCode） 最后实现代码如下： 1 2024-06-10 题目传送门：881. 救生艇 - 力扣（LeetCode） 贪心，因为每条船最多做两个人，所以最优策略是看能否将当前剩余人群中最重的人和最轻的人塞入一条船中，如果不行则最重的人需要单独一条船，对队列排序后进行双指针求解即可。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int numRescueBoats(vector&lt;int&gt;&amp; people, int limit) &#123; int n = people.size(); sort(people.begin(), people.end()); int l = 0, r=n-1, ans = 0; while (l&lt;=r) &#123; if (people[l]+people[r] &lt;= limit) &#123; l ++; &#125; r -- ; ans ++; &#125; return ans; &#125;&#125;; 2024-06-11 题目传送门：419. 甲板上的战舰 - 力扣（LeetCode） 遍历扫描，因为战舰只能水平或竖直放置，按照从左到右从上到下的顺序，统计战舰最左端和最上端部分的个数即可。 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: int countBattleships(vector&lt;vector&lt;char&gt;&gt;&amp; board) &#123; int m = board.size(), n = board[0].size(); int ans = 0; function&lt;bool(int,int)&gt; judge = [&amp;](int x,int y)&#123; if (board[x][y]!=&#x27;X&#x27;) return false; if (x-1&gt;=0 &amp;&amp; board[x-1][y]==&#x27;X&#x27;) return false; if (y-1&gt;=0 &amp;&amp; board[x][y-1]==&#x27;X&#x27;) return false; return true; &#125;; for(int i=0;i&lt;m;++i)&#123; for(int j=0;j&lt;n;++j)&#123; if (judge(i, j)) ans++; &#125; &#125; return ans; &#125;&#125;; 2024-06-12 题目传送门：2806. 取整购买后的账户余额 - 力扣（LeetCode） if语句签到题。 最后实现代码如下： 12345678class Solution &#123;public: int accountBalanceAfterPurchase(int purchaseAmount) &#123; int cost = purchaseAmount / 10 * 10; if (purchaseAmount % 10 &gt;= 5) cost += 10; return 100 - cost; &#125;&#125;; 2024-06-13 题目传送门： 最后实现代码如下： 1 2024-06-14 题目传送门：2786. 访问数组中的位置使分数最大 - 力扣（LeetCode） 简单的动态规划，状态转移是对当前的下标i： 选择一个之前的子序列，最后一个数和当前下标i奇偶性相同 选择一个之前的子序列，最后一个数和当前下标i奇偶性不同 最后实现代码如下（使用堆优化，因为我定义的dp数组是以当前数为结尾（必须选当前数）最大的子序列）： 1234567891011121314151617181920212223class Solution &#123;public: long long maxScore(vector&lt;int&gt;&amp; nums, int x) &#123; long long n = nums.size(); vector&lt;long long&gt; dp(n+1,0); dp[0] = nums[0]; priority_queue&lt;long long&gt; pq[2]; pq[nums[0]%2].push(nums[0]); for (long long i=1;i&lt;n;++i) &#123; long long tmp_mod = nums[i] % 2; long long ans1 = -1e7, ans2 = -1e7; if (!pq[tmp_mod].empty()) ans1 = pq[tmp_mod].top() ; if (!pq[1-tmp_mod].empty()) ans2 = pq[1-tmp_mod].top(); dp[i] = max(ans1, ans2-x) + nums[i]; pq[tmp_mod].push(dp[i]); &#125; long long ans = dp[0]; for(long long i=1;i&lt;n;++i) &#123; ans = max(ans, dp[i]); &#125; return ans; &#125;&#125;; 2024-06-15 题目传送门：2779. 数组的最大美丽值 - 力扣（LeetCode） 差分数组，此题相当于问给定n个区间，被覆盖最多元素的次数是多少，初始化一个全0的差分数组，对于每一个区间，区间最开始的元素对应位置在差分数组中+1，最后元素对应位置的下一个位置在差分数组中-1。最后对差分数组求前缀和就可知每个元素被覆盖的次数了，找出最大即可。 最后实现代码如下： 1234567891011121314151617class Solution &#123;public: int maximumBeauty(vector&lt;int&gt;&amp; nums, int k) &#123; vector&lt;int&gt; cnts(200003,0); for (auto &amp;num : nums) &#123; cnts[max(0,num-k)]++; cnts[num+k+1]--; &#125; int max_ele = *max_element(nums.begin(),nums.end()); int ans = cnts[0], nw = cnts[0]; for(int i=1; i&lt;=max_ele+k; ++i) &#123; nw += cnts[i]; ans = max(ans,nw); &#125; return ans; &#125;&#125;; 2024-06-16 题目传送门：521. 最长特殊序列 Ⅰ - 力扣（LeetCode） 今天的签到题还有点意思，有点脑筋急转弯的感觉，题面描述那么多，其实只需要判断两个字符串是不是相同即可。相同返回-1，不相同返回最长字符串长度即可。 最后实现代码如下： 123456789class Solution &#123;public: int findLUSlength(string a, string b) &#123; if (a.compare(b) == 0) &#123; return -1; &#125; return max(a.size(), b.size()); &#125;&#125;; 2024-06-17 题目传送门：522. 最长特殊序列 II - 力扣（LeetCode） 最后实现代码如下： 1 2024-06-18 题目传送门：2288. 价格减免 - 力扣（LeetCode） 模拟题，先使用空格对字符串进行划分，然后再判断一个字符串中除了$其余是否都是数字，如果是转为int进行计算后，加入答案中去。 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: string discountPrices(string sentence, int discount) &#123; double dis = 1 - (discount / 100.0); stringstream ss(sentence); string ans, w; while (ss &gt;&gt; w)&#123; // 按照空格进行分割 if(!ans.empty()) &#123; ans += &#x27; &#x27;; &#125; if (w.length() &gt; 1 &amp;&amp; w[0] == &#x27;$&#x27; &amp;&amp; all_of(w.begin()+1, w.end(), ::isdigit)) &#123; stringstream s; s &lt;&lt; fixed &lt;&lt; setprecision(2) &lt;&lt; &#x27;$&#x27; &lt;&lt; stoll(w.substr(1)) * dis; ans += s.str(); &#125; else &#123; ans += w; &#125; &#125; return ans; &#125;&#125;; 2024-06-19 题目传送门：2713. 矩阵中严格递增的单元格数 - 力扣（LeetCode） 此题好像在蓝桥杯上遇见过 最后实现代码如下： 1 2024-06-20 题目传送门：2748. 美丽下标对的数目 - 力扣（LeetCode） 遍历签到题 最后实现代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int countBeautifulPairs(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); function&lt;int(int, int)&gt; gcd = [&amp;](int a,int b)&#123; if (a%b==0) return b; return gcd(b, a%b); &#125;; function&lt;int(int)&gt; rnum = [&amp;](int x)&#123; while(x&gt;9) x /= 10; return x; &#125;; int ans = 0; for(int i=0;i&lt;n;++i)&#123; for(int j=i+1;j&lt;n;++j)&#123; int maxx = max(rnum(nums[i]), nums[j]%10); int minn = min(rnum(nums[i]), nums[j]%10); if (gcd(maxx, minn)==1) ans++; &#125; &#125; return ans; &#125;&#125;; 2024-06-21 题目传送门：LCP 61. 气温变化趋势 - 力扣（LeetCode） 签到题，遍历即可。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int temperatureTrend(vector&lt;int&gt;&amp; temperatureA, vector&lt;int&gt;&amp; temperatureB) &#123; int ans=0, n=temperatureA.size(),tmp=0; for(int i=1; i&lt;n; ++i) &#123; if (((temperatureA[i]&gt;temperatureA[i-1])&amp;&amp;(temperatureB[i]&gt;temperatureB[i-1]))|| ((temperatureA[i]&lt;temperatureA[i-1])&amp;&amp;(temperatureB[i]&lt;temperatureB[i-1]))|| ((temperatureA[i]==temperatureA[i-1])&amp;&amp;(temperatureB[i]==temperatureB[i-1])))&#123; tmp++; ans = max(ans, tmp); &#125; else &#123; tmp = 0; &#125; &#125; return ans; &#125;&#125;; 2024-06-22 题目传送门：2663. 字典序最小的美丽字符串 - 力扣（LeetCode） 模拟+贪心，为了字典序尽可能的小，应当从最右边开始检查当前字符能否更改，若能够更改则接着填充后续字符，如果能填充完毕则成功，返回最终的字符串，否则继续搜索。 问题： 更改字符串中的字符，是不是只往上加一个就行了。 是，根据贪心算法，可以证明，只加一个具有最优解。 最后实现代码如下： 1234567891011121314151617181920212223242526272829class Solution &#123; public: string smallestBeautifulString(string s, int k) &#123; int n = s.size(); function&lt;char(int)&gt; judge = [&amp;](int loc)&#123; for (int i=s[loc]-&#x27;a&#x27;+2;i&lt;=k;++i) &#123; if (loc==0) return char(i-1+&#x27;a&#x27;); if (loc==1&amp;&amp;s[loc-1]!=i+&#x27;a&#x27;-1) return char(i-1+&#x27;a&#x27;); if (loc&gt;=2&amp;&amp;s[loc-1]!=i+&#x27;a&#x27;-1&amp;&amp;s[loc-2]!=i+&#x27;a&#x27;-1) return char(i-1+&#x27;a&#x27;); &#125; return &#x27;!&#x27;; &#125;; string ans = &quot;&quot;; for (int i=n-1;i&gt;=0;--i) &#123; char nw = judge(i); if (nw != &#x27;!&#x27;) &#123; string tmp = s.substr(0, i); tmp += nw; bool flag=true; for(int j=i+1; j&lt;n; ++j)&#123; bool nw_flag = false; for(int c=1;c&lt;=k;++c)&#123; if (c==tmp[j-1]-&#x27;a&#x27;+1) continue; if(j-2&gt;=0&amp;&amp;tmp[j-2]-&#x27;a&#x27;+1==c) continue; tmp += char(c-1+&#x27;a&#x27;); nw_flag = true; break; &#125; if (!nw_flag) &#123;flag=false;break;&#125; &#125; if (flag) return tmp; &#125; &#125; return &quot;&quot;; &#125;&#125;; 2024-06-23 题目传送门：520. 检测大写字母 - 力扣（LeetCode） 一行代码解决战斗！ 最后实现代码如下： 123456class Solution &#123;public: bool detectCapitalUse(string word) &#123; return (all_of(word.begin(), word.end(), [](unsigned char c) &#123;return islower(c);&#125;)) || (all_of(word.begin(), word.end(), [](unsigned char c) &#123;return isupper(c);&#125;)) || (isupper(word[0]) &amp;&amp; all_of(word.begin()+1, word.end(), [](unsigned char c) &#123;return islower(c);&#125;)); &#125;&#125;; 2024-06-24 题目传送门：503. 下一个更大元素 II - 力扣（LeetCode） 单调栈，注意，由于队列首位相接，可以通过将队列复制一份，使其长度加倍。这样对于前一份队列便可以和自己之前的元素进行比较，近似实现了队列首位相接的功能。 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; nextGreaterElements(vector&lt;int&gt;&amp; nums) &#123; int max_ele = *max_element(nums.begin(), nums.end()); int n = nums.size(); vector&lt;int&gt; ans(n,-1); vector&lt;pair&lt;int,int&gt;&gt; stk; for(int i=0;i&lt;n*2-1;++i)&#123; int nw = nums[i%n]; int tmp_n = stk.size(); while (!stk.empty() &amp;&amp; stk[tmp_n-1].second &lt; nw &amp;&amp; ans[stk[tmp_n-1].first] == -1)&#123; ans[stk[tmp_n-1].first] = nw; stk.pop_back(); tmp_n--; &#125; if (i &lt; n &amp;&amp; max_ele!=nums[i]) stk.emplace_back(i, nums[i]); &#125; return ans; &#125;&#125;; 2024-06-25 题目传送门：2732. 找到矩阵中的好子集 - 力扣（LeetCode） 可以通过严格证明得知，在n&lt;=5的情况下，只需要考虑1，2行的情况。 严格证明过程见：严格证明+三种计算方法（Python/Java/C++/Go） 得知上述性质后，哈希表解决即可。（因为最多五列，所以哈希表也不大） 最后实现代码如下： 123456789101112131415161718192021222324252627282930class Solution &#123;public: vector&lt;int&gt; goodSubsetofBinaryMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; vector&lt;int&gt; ans; unordered_map&lt;int, int&gt; mp; int m = grid.size(); int n = grid[0].size(); for(int i=0; i&lt;m; ++i)&#123; int st = 0; for(int j=0; j&lt;n; ++j)&#123; st |= (grid[i][j] &lt;&lt; j); &#125; mp[st] = i; &#125; if (mp.count(0)) &#123; ans.push_back(mp[0]); return ans; &#125; for (auto [x, i]: mp) &#123; for (auto [y, j]: mp) &#123; if (!(x &amp; y)) &#123; return &#123;min(i, j), max(i, j)&#125;; &#125; &#125; &#125; return ans; &#125;&#125;; 2024-06-26 题目传送门：2741. 特别的排列 - 力扣（LeetCode） 状压DP板子题，需要注意的是vector不要一上来就开的很大，会报错😭。 最后实现代码如下： 1234567891011121314151617181920212223242526272829class Solution &#123;public: int specialPerm(vector&lt;int&gt;&amp; nums) &#123; // sort(nums.begin(), nums.end()); int n = nums.size(); int maxx = pow(2, n); vector&lt;vector&lt;int&gt;&gt; dp(maxx,vector&lt;int&gt;(n,0)); int MOD = 1e9 + 7; for(int i=0;i&lt;n;++i) dp[1&lt;&lt;i][i] = 1; for(int i=1;i&lt;maxx;++i)&#123; for(int j=0;j&lt;n;++j)&#123; if (!(i&gt;&gt;j)&amp;1) continue; for(int k=0;k&lt;n;++k)&#123; if (j==k) continue; if(!(1&amp;(i&gt;&gt;k))) continue; if (max(nums[j],nums[k]) % min(nums[j],nums[k])) continue; int ii = i ^ (1&lt;&lt;j); dp[i][j] = (dp[i][j] + dp[ii][k]) % MOD; &#125; &#125; &#125; int ans = 0; for(int i=0;i&lt;n;++i) ans = (ans + dp[maxx-1][i]) % MOD; return ans; &#125;&#125;; 2024-06-27 题目传送门：2734. 执行子串操作后的字典序最小字符串 - 力扣（LeetCode） 贪心，从左到右遍历，如果不是a就让他变小，这样连续的改一串就可以。需要注意的特殊情况是，如果字符串为全a，此时将最后一个字符改为z即可。这种特殊情况特判一下就好。 最后实现代码如下： 123456789101112class Solution &#123;public: string smallestString(string s) &#123; int n = s.size(); int cnt = 0; bool flag = false; while(cnt&lt;n&amp;&amp;s[cnt] == &#x27;a&#x27;) cnt++; while(cnt&lt;n&amp;&amp;s[cnt]!=&#x27;a&#x27;) flag=true,s[cnt]=s[cnt]-1,cnt++; if (!flag) s[n-1] = &#x27;z&#x27;; return s; &#125;&#125;; 2024-06-28 题目传送门：2742. 给墙壁刷油漆 - 力扣（LeetCode） 此题为01背包问题的至少变形， 详细讲解见OI-Knowledge背包问题部分最后的变形3章节。 最后实现代码如下： 123456789101112131415class Solution &#123;public: int paintWalls(vector&lt;int&gt; &amp;cost, vector&lt;int&gt; &amp;time) &#123; int n = cost.size(); vector&lt;int&gt; dp(n + 1, INT_MAX / 2); // 防止加法溢出 dp[0] = 0; for (int i = 0; i &lt; n; i++) &#123; int nw_cost = cost[i], nw_time = time[i]+1; for(int j=n;j&gt;=0;--j)&#123; dp[j] = min(dp[j], dp[max(0, j-nw_time)]+nw_cost); &#125; &#125; return dp[n]; &#125;&#125;; 2024-06-29 题目传送门：2710. 移除字符串中的尾随零 - 力扣（LeetCode） C++ string中substr练习题。 最后实现代码如下： 1234567891011class Solution &#123;public: string removeTrailingZeros(string num) &#123; int n = num.size(); int i = 0; while (num[n-1-i]==&#x27;0&#x27;) &#123; i++; &#125; return num.substr(0,n-i); &#125;&#125;; 2024-06-30 题目传送门：494. 目标和 - 力扣（LeetCode） 01背包变体，题目要求在每个数前面添加+或者-号，其实可以看作是初始都是减号，选出一些数变为+号，这就是01背包的思想了。只不过最后要求不像01背包是至多而是恰好需要注意。 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int findTargetSumWays(vector&lt;int&gt;&amp; nums, int target) &#123; if (target &lt; 0) target = -target; int sum_nums = 0; for(auto num : nums) sum_nums += num; if ((sum_nums+target)%2) return 0; int nw_target = (sum_nums+target)/2; vector&lt;int&gt; dp(nw_target+1, 0); dp[0] = 1; for(auto num : nums)&#123; for(int i=nw_target;i&gt;=0;--i)&#123; if(i&gt;=num) dp[i] += dp[i-num]; &#125; &#125; return dp[nw_target]; &#125;&#125;;"},{"title":"2024年08月每日一题","path":"/wiki/LeetCode/每日一题/2024年08月每日一题.html","content":"2024-08-08 题目传送门：3131. 找出与数组相加的整数 I - 力扣（LeetCode） 签到题，排序后第一个相减即可。 最后实现代码如下： 12345678class Solution &#123;public: int addedInteger(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; sort(nums1.begin(), nums1.end()); sort(nums2.begin(), nums2.end()); return nums2[0] - nums1[0]; &#125;&#125;; 2024-08-09 题目传送门：3132. 找出与数组相加的整数 II - 力扣（LeetCode） 上一题的难度增强版，但是由于数组长度较小，直接枚举被删除的数即可 最后实现代码如下： 123456789101112131415161718192021222324252627class Solution &#123;public: int minimumAddedInteger(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; sort(nums1.begin(), nums1.end()); sort(nums2.begin(), nums2.end()); int n = nums2.size(); function&lt;int(int,int)&gt; judge = [&amp;](int ex1,int ex2)&#123; int nw = 0; while(nw==ex1||nw==ex2) nw++; int x = nums2[0] - nums1[nw]; for(int i=0;i&lt;n;++i)&#123; while (i+nw==ex1||i+nw==ex2) nw++; if(nums2[i]-nums1[nw+i]!=x) return 12312312; &#125; return x; &#125;; int ans = INT_MAX; for(int i=0;i&lt;n+2;++i)&#123; for(int j=i+1;j&lt;n+2;++j)&#123; int x = judge(i,j); if (x!=12312312) ans = min(ans, x); &#125; &#125; return ans; &#125;&#125;;"},{"title":"2024年07月每日一题","path":"/wiki/LeetCode/每日一题/2024年07月每日一题.html","content":"2024-07-01 题目传送门：2065. 最大化一张图中的路径价值 - 力扣（LeetCode） 这道题看上去非常的吓人，但是我们注意一下数据范围便可以发现，由于每条边的时间以及总的时间限制最小为10最大为100，这说明我们至多只会经过图上的 10 条边。并且由于图中每个节点的度数都不超过 4，因此我们可以枚举（暴搜）所有从节点 0 开始的路径。暴搜终止条件就是超时了。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: int maximalPathQuality(vector&lt;int&gt;&amp; values, vector&lt;vector&lt;int&gt;&gt;&amp; edges, int maxTime) &#123; int n = values.size(); vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt; g(n); for (auto edge : edges) &#123; g[edge[0]].emplace_back(edge[1], edge[2]); g[edge[1]].emplace_back(edge[0], edge[2]); &#125; vector&lt;bool&gt; vis(n, false); vis[0] = true; int ans = 0; function&lt;void(int, int, int)&gt; dfs = [&amp;](int u, int time, int value)&#123; if(u == 0) &#123; ans = max(ans, value); &#125; for(auto [v, dist]:g[u])&#123; if (time + dist &lt;= maxTime) &#123; if(!vis[v]) &#123; vis[v] = true; dfs(v, time+dist, value+values[v]); vis[v] = false; &#125; else &#123; dfs(v, time+dist,value); &#125; &#125; &#125; &#125;; dfs(0, 0, values[0]); return ans; &#125;&#125;; 2024-07-02 题目传送门：3115. 质数的最大距离 - 力扣（LeetCode） 正好借此题复习一下欧拉筛。因为数的范围最大只到100，因此预处理一下100内的质数然后分别找出队列左边第一个质数的下标和右边第一个质数的下标即可。 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int maximumPrimeDifference(vector&lt;int&gt;&amp; nums) &#123; int MAX=100, n = nums.size(); vector&lt;bool&gt; vis(101, false); vector&lt;int&gt; primes; for(int i=2;i&lt;=MAX;++i)&#123; if (!vis[i]) primes.push_back(i); for(auto prime : primes)&#123; if (i * prime &gt; MAX) break; vis[i * prime] = true; if (i % prime == 0) break; &#125; &#125; vis[0] = true; vis[1] = true; int i=0,j=n-1; while(vis[nums[i]]) i++; while(vis[nums[j]]) j--; return j-i; &#125;&#125;; 2024-07-03 题目传送门：3099. 哈沙德数 - 力扣（LeetCode） 快乐签到题 最后实现代码如下： 12345678class Solution &#123;public: int sumOfTheDigitsOfHarshadNumber(int x) &#123; int ans = 0, xx = x; while (x) ans += x%10, x /= 10; return xx%ans==0?ans:-1; &#125;&#125;; 2024-07-04 题目传送门：3086. 拾起 K 个 1 需要的最少行动次数 - 力扣（LeetCode） 贪心+二分 最后实现代码如下： 1 2024-07-05 题目传送门：3033. 修改矩阵 - 力扣（LeetCode） 签到题喜加一 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; modifiedMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; matrix) &#123; int m = matrix.size(), n = matrix[0].size(); vector&lt;int&gt;col_max = matrix[0]; for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;m;++j)&#123; col_max[i] = max(col_max[i], matrix[j][i]); &#125; &#125; for(int i=0;i&lt;m;++i)&#123; for(int j=0;j&lt;n;++j)&#123; if(matrix[i][j]==-1) matrix[i][j] = col_max[j]; &#125; &#125; return matrix; &#125;&#125;; 2024-07-06 题目传送门：3101. 交替子数组计数 - 力扣（LeetCode） 可以把他当成一个DP来处理，开一个和数组一样长的DP数组，dp每一项记录以该项结尾能构成的子数组数。 最后实现代码如下： 123456789101112131415class Solution &#123;public: long long countAlternatingSubarrays(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); vector&lt;long long&gt;dp(n+2, 0); dp[1] = 1; for(int i=2;i&lt;=n;++i)&#123; if (nums[i-1] ^ nums[i-2]) dp[i] = 1 + dp[i-1]; else dp[i] = 1; &#125; long long ans=0; for(int i=1;i&lt;=n;++i) ans += dp[i]; return ans; &#125;&#125;; 仔细观察上述代码，我们发现dp数组也可以被我们优化掉： 优化后实现代码如下： 12345678910111213class Solution &#123;public: long long countAlternatingSubarrays(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); long long cnt = 0, ans = 0; for(int i=1;i&lt;=n;++i)&#123; if (i!=1&amp;&amp;nums[i-1] ^ nums[i-2]) cnt++; else cnt = 1; ans += cnt; &#125; return ans; &#125;&#125;; 其实就是一道遍历题 2024-07-07 题目传送门：1958. 检查操作是否合法 - 力扣（LeetCode） 大模拟题目，遍历8个方向，只有第一个和当前颜色不同在继续判断，最后记得判断最后一个块的颜色和传入颜色是否相同即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728class Solution &#123;public: bool checkMove(vector&lt;vector&lt;char&gt;&gt;&amp; board, int rMove, int cMove, char color) &#123; int n = board.size(), m = board[0].size(); function&lt;bool(int, int)&gt; judge = [&amp;](int x,int y)&#123; if (x&lt;0||y&lt;0||x&gt;=n||y&gt;=m) return false; return true; &#125;; function&lt;bool(char, int)&gt; judge_col = [&amp;](char nw_color,int rev)&#123; if (rev&amp;&amp;(nw_color!=color)) &#123; if (nw_color==&#x27;B&#x27;&amp;&amp;color==&#x27;W&#x27;) return true; if (nw_color==&#x27;W&#x27;&amp;&amp;color==&#x27;B&#x27;) return true; return false; &#125; if (!rev&amp;&amp;(nw_color==color)) return true; return false; &#125;; vector&lt;vector&lt;int&gt;&gt; dirs = &#123;&#123;-1,0&#125;,&#123;1,0&#125;,&#123;0,-1&#125;,&#123;0,1&#125;,&#123;1,1&#125;,&#123;-1,-1&#125;,&#123;-1,1&#125;,&#123;1,-1&#125;&#125;; for (auto dir : dirs)&#123; if(judge(rMove+dir[0],cMove+dir[1])&amp;&amp;judge_col(board[rMove+dir[0]][cMove+dir[1]],1))&#123; int cnt = 1; while(judge(rMove+dir[0]*cnt,cMove+dir[1]*cnt)&amp;&amp;judge_col(board[rMove+dir[0]*cnt][cMove+dir[1]*cnt],1)) cnt++; if (judge(rMove+dir[0]*cnt,cMove+dir[1]*cnt)&amp;&amp;judge_col(board[rMove+dir[0]*cnt][cMove+dir[1]*cnt],0)) return true; &#125; &#125; return false; &#125;&#125;; 2024-07-08 题目传送门：724. 寻找数组的中心下标 - 力扣（LeetCode） 快乐签到题，遍历一遍即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: int pivotIndex(vector&lt;int&gt;&amp; nums) &#123; int sum_all = 0, n = nums.size(); for(int i=0;i&lt;n;++i) sum_all += nums[i]; int sum_now=0; for(int i=0; i&lt;n; ++i) &#123; if (sum_now == sum_all-nums[i]-sum_now) return i; sum_now += nums[i]; &#125; return -1; &#125;&#125;; 2024-07-09 题目传送门：3102. 最小化曼哈顿距离 - 力扣（LeetCode） 需要使用技巧：曼哈顿距离转切比雪夫距离，详情可见灵神的题解：【图解】曼哈顿距离转切比雪夫距离（Python/Java/C++/Go） 转换公式如下： ∣x1−x2∣+∣y1−y2∣=max⁡(∣x1′−x2′∣,∣y1′−y2′∣)\\left| x_1 - x_2 \\right| + \\left| y_1 - y_2 \\right| = \\max \\left( \\left| x_1&#x27; - x_2&#x27; \\right|, \\left| y_1&#x27; - y_2&#x27; \\right| \\right) ∣x1​−x2​∣+∣y1​−y2​∣=max(∣x1′​−x2′​∣,∣y1′​−y2′​∣) 其中左侧为曼哈顿距离，右侧为切比雪夫距离，坐标变换关系如下所示： (x′,y′)=(x+y,y−x)\\left( x&#x27;, y&#x27; \\right) = \\left( x + y, y - x \\right) (x′,y′)=(x+y,y−x) 做了上面的变换后，我们的任务就很简单了。可以定义两个有序集合，分别维护n个数转换后横坐标和纵坐标的有序排列，然后枚举被删除的数，从集合中先删除他们的横纵坐标后，计算最大的距离，然后再复原。 最后实现代码如下： 1234567891011121314151617181920212223242526class Solution &#123;public: int minimumDistance(vector&lt;vector&lt;int&gt;&gt;&amp; points) &#123; multiset&lt;int&gt; xs, ys; for (auto&amp; p : points) &#123; xs.insert(p[0] + p[1]); ys.insert(p[1] - p[0]); &#125; int ans = INT_MAX; // 枚举被移除的点 for (auto&amp; p : points) &#123; int x = p[0] + p[1], y = p[1] - p[0]; xs.erase(xs.find(x)); // 移除一个 x ys.erase(ys.find(y)); // 移除一个 y int dx = *xs.rbegin() - *xs.begin(); // 最大x-最小x int dy = *ys.rbegin() - *ys.begin(); // 最大y-最小y ans = min(ans, max(dx, dy)); xs.insert(x); ys.insert(y); &#125; return ans; &#125;&#125;; 这种方法写法上最为简单，当然我们也可以维护每个坐标的最大次大以及最小次小值，一共8个值。 2024-07-10 题目传送门：2970. 统计移除递增子数组的数目 I - 力扣（LeetCode） 咋一看以为很难，但是看到数据范围是50，果断进行枚举子数组的初始和结束端点即可。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: int incremovableSubarrayCount(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); function&lt;bool(int, int)&gt; judge = [&amp;](int x,int y)&#123; int prev = 0; for(int i=0;i&lt;n;++i)&#123; if (i&gt;=x&amp;&amp;i&lt;=y) continue; if (nums[i] &lt;= prev) return false; prev = nums[i]; &#125; return true; &#125;; int ans = 0; for(int i=0;i&lt;n;++i)&#123; for(int j=i;j&lt;n;++j)&#123; ans = judge(i,j)?ans+1:ans; &#125; &#125; return ans; &#125;&#125;; 2024-07-11 题目传送门：2972. 统计移除递增子数组的数目 II - 力扣（LeetCode） 一道非常有意思的双指针题目，首先我们先找到能被删掉的最短后缀，删掉一个子数组相当于，将原数列分为了一个前缀和后缀，定义l为剩余前缀的最后一个数,j为剩余后缀开始的第一个数。 由于我们已经找到了最短后缀，因此l前面的数是具有一定单调性质的。而对于j（最开始就是最后一个数）他的停止条件是如果当前j大于j+1，因此j也有一定的单调性，我们遍历j的时候,只需比较j位置的数是否大于l位置的数，如果不是则将l--，找到第一个小于j位置的l，那么l前面的所有前缀肯定都满足条件，答案加上现目前前缀数总和(l+2)即可。因为j也有单调性的缘故，倒着遍历j时之前被更新l--掉的地方就不用看了。肯定不满足当前j的条件。 本人语文水平有限，更为清晰的题解请看：灵茶山艾府：双指针，O(n) 时间 O(1) 空间（Python/Java/C++/C/Go/JS/Rust） 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: long long incremovableSubarrayCount(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); int l=0; for(int i=1;i&lt;n;++i)&#123; if (nums[i] &gt; nums[i-1])&#123; l++; &#125; else break; &#125; if(l==n-1) return (long long) n*(n+1)/2; long long ans = l+2; for(int j=n-1;j &gt;= 0&amp;&amp;(j == n - 1|| nums[j] &lt; nums[j + 1]);--j)&#123; // 枚举后缀开始的第一个数 while( l&gt;=0 &amp;&amp; nums[l]&gt;= nums[j]) l--; ans += l+2; &#125; return ans; &#125;&#125;; 2024-07-12 题目传送门：2974. 最小数字游戏 - 力扣（LeetCode） 签到题，模拟题 最后实现代码如下： 12345678910111213class Solution &#123;public: vector&lt;int&gt; numberGame(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); sort(nums.begin(), nums.end()); vector&lt;int&gt; arr; for(int i=0;i&lt;n/2;++i)&#123; arr.push_back(nums[i*2+1]); arr.push_back(nums[i*2]); &#125; return arr; &#125;&#125;; 2024-07-13 题目传送门：3011. 判断一个数组是否可以变为有序 - 力扣（LeetCode） 模拟题，根据1的个数将数组划分为几段区间，然后维护每一个区间的最大最小值，如果每两个相邻区间都是下一个区间的最小值大于上一个区间的最大值则满足条件返回true，否则不满足条件返回false。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: bool canSortArray(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); function&lt;int(int)&gt; judge = [&amp;](int u)&#123; int cnt = 0; while(u)&#123; cnt += (u&amp;1); u = u&gt;&gt;1; &#125; return cnt; &#125;; vector&lt;int&gt; arr; for(int i=0;i&lt;n;++i) arr.push_back(judge(nums[i])); vector&lt;pair&lt;int,int&gt;&gt; pairs; int prev = -1,minn,maxx; for(int i=0;i&lt;n;++i) &#123; if(prev == arr[i])&#123; maxx = max(nums[i],maxx); minn = min(nums[i],minn); &#125; else &#123; if(i!=0) pairs.emplace_back(minn,maxx); prev = arr[i]; maxx = nums[i]; minn = nums[i]; &#125; &#125; pairs.emplace_back(minn,maxx); int m = pairs.size(); for(int i=1;i&lt;m;++i)&#123; if (pairs[i-1].second &gt; pairs[i].first) return false; &#125; return true; &#125;&#125;; 2024-07-14 题目传送门：807. 保持城市天际线 - 力扣（LeetCode） 维护每行每列的最大值就可以了。 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int maxIncreaseKeepingSkyline(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;int&gt; rows(n,0); vector&lt;int&gt; cols(m,0); for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;m;++j)&#123; rows[i] = max(rows[i],grid[i][j]); cols[j] = max(cols[j],grid[i][j]); &#125; &#125; int ans = 0; for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;m;++j)&#123; ans += min(rows[i],cols[j]) - grid[i][j]; &#125; &#125; return ans; &#125;&#125;; 2024-07-15 题目传送门：721. 账户合并 - 力扣（LeetCode） 本质这道题可以看成一个连通块问题（求有多少连通水洼），因此可以使用dfs解决问题。这道题主要难点在于使用哈希表将字符串映射成int类型对问题进行简化，细节操作见下面的代码实现。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: vector&lt;vector&lt;string&gt;&gt; accountsMerge(vector&lt;vector&lt;string&gt;&gt;&amp; accounts) &#123; int n = accounts.size(); unordered_map&lt;string, vector&lt;int&gt;&gt; accounts_idx; for(int i=0;i&lt;n;++i)&#123; for(int j=1;j&lt;accounts[i].size();++j)&#123; accounts_idx[accounts[i][j]].push_back(i); &#125; &#125; vector&lt;bool&gt; vis(n,false); unordered_set&lt;string&gt; email_set; function&lt;void(int)&gt; dfs = [&amp;](int i)&#123; vis[i] = true; for(int k = 1;k &lt; accounts[i].size();++k)&#123; string email = accounts[i][k]; if (email_set.contains(email)) continue; email_set.insert(email); for (int j : accounts_idx[email]) &#123; // 遍历所有包含该邮箱地址的账户下标 j if (!vis[j]) &#123; // j 没有访问过 dfs(j); &#125; &#125; &#125; &#125;; vector&lt;vector&lt;string&gt;&gt; ans; for(int i=0; i&lt;vis.size(); ++i)&#123; if (vis[i]) continue; email_set.clear(); dfs(i); vector&lt;string&gt; res = &#123;accounts[i][0]&#125;; res.insert(res.end(), email_set.begin(), email_set.end()); sort(res.begin()+1, res.end()); ans.push_back(res); &#125; return ans; &#125;&#125;; 2024-07-16 题目传送门：2956. 找到两个数组中的公共元素 - 力扣（LeetCode） 签到题，但是可以学习一下如何使用vector初始化unordered_set类型的变量。 最后实现代码如下： 123456789101112131415class Solution &#123;public: vector&lt;int&gt; findIntersectionValues(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; unordered_set&lt;int&gt; nums1_set(nums1.begin(), nums1.end()); unordered_set&lt;int&gt; nums2_set(nums2.begin(), nums2.end()); int ans1 = 0,ans2 = 0; for(const int &amp;num : nums1)&#123; if (nums2_set.find(num)!=nums2_set.end())ans1++; &#125; for(const int &amp;num : nums2)&#123; if (nums1_set.find(num)!=nums1_set.end())ans2++; &#125; return vector&lt;int&gt;&#123;ans1,ans2&#125;; &#125;&#125;; 2024-07-17 题目传送门：2959. 关闭分部的可行集合数目 - 力扣（LeetCode） 由于数据范围非常小，可以枚举所有关闭分部的情况，然后对于每种关闭方案，使用Floyd算法每次获得全局最短路信息，判断是否满足条件即可。 最后实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123;public: int numberOfSets(int n, int maxDistance, vector&lt;vector&lt;int&gt;&gt;&amp; roads) &#123; int max_stats = 1&lt;&lt;n, ans = 0; vector&lt;vector&lt;int&gt;&gt; dis(n,vector&lt;int&gt;(n, 1e9)); for(int stat=0;stat&lt;max_stats;++stat)&#123; for(int i=0;i&lt;n;++i) dis[i][i] = 0; for(auto road : roads) &#123; if (((stat&gt;&gt;road[0])&amp;1)==0||((stat&gt;&gt;road[1])&amp;1)==0) continue; dis[road[0]][road[1]] = min(dis[road[0]][road[1]], road[2]); dis[road[1]][road[0]] = min(dis[road[1]][road[0]], road[2]); &#125; // Floyd for(int k=0;k&lt;n;++k)&#123; if ((stat&gt;&gt;k)&amp;1==0) continue; for(int i=0;i&lt;n;++i)&#123; if ((stat&gt;&gt;i)&amp;1==0) continue; for(int j=0;j&lt;n;++j)&#123; if ((stat&gt;&gt;j)&amp;1==0) continue; dis[i][j] = min(dis[i][j], dis[i][k]+dis[k][j]); &#125; &#125; &#125; bool flag = true; for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;n;++j)&#123; if(dis[i][j] &gt; maxDistance &amp;&amp; (stat&gt;&gt;i)&amp;1 &amp;&amp; (stat&gt;&gt;j)&amp;1)&#123; flag = false; break; &#125; &#125; if (!flag) break; &#125; if (flag) ans++; &#125; return ans; &#125;&#125;; 但是由于每次在每种情况中进行数组的初始化实在是太浪费时间了，所以进行如下优化，在外面先创建好一个数组， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123;public: int numberOfSets(int n, int maxDistance, vector&lt;vector&lt;int&gt;&gt;&amp; roads) &#123; vector&lt;vector&lt;int&gt;&gt; g(n, vector&lt;int&gt;(n, INT_MAX / 2)); // 防止加法溢出 for (int i = 0; i &lt; n; i++) &#123; g[i][i] = 0; // 也可以不写，下面判断 maxDistance 时要保证 j != i &#125; for (auto&amp; e: roads) &#123; int x = e[0], y = e[1], wt = e[2]; g[x][y] = min(g[x][y], wt); g[y][x] = min(g[y][x], wt); &#125; vector&lt;vector&lt;int&gt;&gt; f(n); auto check = [&amp;](int s) -&gt; bool &#123; for (int i = 0; i &lt; n; i++) &#123; if ((s &gt;&gt; i) &amp; 1) &#123; f[i] = g[i]; &#125; &#125; // Floyd 算法（只考虑在 s 中的节点） for (int k = 0; k &lt; n; k++) &#123; if (((s &gt;&gt; k) &amp; 1) == 0) continue; for (int i = 0; i &lt; n; i++) &#123; if (((s &gt;&gt; i) &amp; 1) == 0) continue; for (int j = 0; j &lt; n; j++) &#123; f[i][j] = min(f[i][j], f[i][k] + f[k][j]); &#125; &#125; &#125; // 判断保留的节点之间的最短路是否均不超过 maxDistance for (int i = 0; i &lt; n; i++) &#123; if (((s &gt;&gt; i) &amp; 1) == 0) continue; for (int j = 0; j &lt; n; j++) &#123; if ((s &gt;&gt; j) &amp; 1 &amp;&amp; f[i][j] &gt; maxDistance) &#123; return false; &#125; &#125; &#125; return true; &#125;; int ans = 0; for (int s = 0; s &lt; (1 &lt;&lt; n); s++) &#123; // 枚举子集 ans += check(s); &#125; return ans; &#125;&#125;; 2024-07-18 题目传送门：3112. 访问消失节点的最少时间 - 力扣（LeetCode） Dijkstra板子题，正好重新进行复习一下，只需要在更新的时候判断一下当前更新方案是否合法（有可能需要更新的点已经消失了，此时就不用更新） 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: vector&lt;int&gt; minimumTime(int n, vector&lt;vector&lt;int&gt;&gt;&amp; edges, vector&lt;int&gt;&amp; disappear) &#123; vector&lt;vector&lt;pair&lt;int,int&gt;&gt;&gt; g(n); for(auto edge: edges)&#123; g[edge[0]].emplace_back(edge[1],edge[2]); g[edge[1]].emplace_back(edge[0],edge[2]); &#125; vector&lt;int&gt; dis(n,INT_MAX/2); priority_queue&lt;pair&lt;int,int&gt;, vector&lt;pair&lt;int,int&gt;&gt;, greater&lt;&gt;&gt; pq; vector&lt;bool&gt; vis(n, false); dis[0] = 0; pq.push(make_pair(dis[0],0)); while (!pq.empty()) &#123; int u = pq.top().second; pq.pop(); if (vis[u]) continue; vis[u] = true; // if (dis[u] &gt; disappear[u]) continue; for(auto tmp : g[u])&#123; int val = tmp.second, v = tmp.first; if (dis[v] &gt; dis[u] + val &amp;&amp; disappear[v] &gt; dis[u]+val) &#123; dis[v] = dis[u]+val; pq.push(make_pair(dis[v],v)); &#125; &#125; &#125; for(int i=0;i&lt;n;++i)&#123; if (dis[i]==INT_MAX/2) dis[i] = -1; &#125; return dis; &#125;&#125;; 2024-07-19 题目传送门：3096. 得到更多分数的最少关卡数目 - 力扣（LeetCode） 模拟题吧，先算出获胜条件的阈值，然后从第一位开始往后遍历，过程中统计Alice的得分，大于等于阈值返回当前是第几轮游戏，如果数组遍历完了依然不满足条件返回-1表示无法获胜。 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: int minimumLevels(vector&lt;int&gt;&amp; possible) &#123; int n = possible.size(),sumpos=0; for(int i=0;i&lt;n;++i) &#123; if (possible[i]==0) sumpos -= 1; else sumpos += 1; &#125; int thre; if (sumpos &gt;= 0) thre = sumpos / 2 + 1; else thre = (sumpos-1) / 2 + 1; int tmp = possible[0]==1?1:-1, ans = 1; for(int i=1;i&lt;n;++i)&#123; if (tmp&gt;=thre) return ans; ans++; tmp += possible[i]==1?1:-1; &#125; return -1; &#125;&#125;; 2024-07-20 题目传送门：2850. 将石头分散到网格图的最少移动次数 - 力扣（LeetCode） 由于数据范围很小，可以使用枚举法解决此题，首先先遍历所有格子，对于为0的地方，将格子坐标存入less数组，对于大于1的地方，将格子坐标存入more数组，每超过1一次就存一次坐标进去（比如当前位置是3，则存两次该位置的坐标到more数组中）。最终我们会得到一个长度相同的more和less数组。 然后此时我们只需要枚举其中一个数组的全排列，计算不同方案所使用的步数，返回最小步数即可。C中对vector数组枚举全排列的函数是next_permutation,具体关于该函数的介绍见C专栏中的next_permutation部分。 本题最后实现代码如下： 1234567891011121314151617181920212223242526class Solution &#123;public: int minimumMoves(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; vector&lt;pair&lt;int,int&gt;&gt; less, more; for(int i=0;i&lt;3;++i)&#123; for(int j=0;j&lt;3;++j)&#123; if (grid[i][j]&gt;1) &#123; for(int k=2; k&lt;=grid[i][j]; ++k)&#123; more.emplace_back(i, j); &#125; &#125; else if (grid[i][j] == 0) &#123; less.emplace_back(i, j); &#125; &#125; &#125; int ans = INT_MAX; do &#123; int steps = 0; for(int i=0;i&lt;more.size();++i)&#123; steps += abs(more[i].first - less[i].first) + abs(more[i].second - less[i].second); &#125; ans = min(ans, steps); &#125; while (next_permutation(more.begin(), more.end())); return ans; &#125;&#125;; 2024-07-21 题目传送门：1186. 删除一次得到子数组最大和 - 力扣（LeetCode） 该题目为经常练习的动态规划中的最大子数组和（最大子段和） 类题目，相比就是读了一个选择出来子数组后可以从中删除一个元素，那么我们可以在最大子数组和（最大子段和） 的常规解放上多一个状态维度，设计DP数组dp[n][2]含义如下，dp[i][0]表示以当前位置i的数结尾且未删除任何数可以达到的最大子数组和。dp[i][1]表示以当前位置i的数结尾且删除1个数可以达到的最大子数组和。这样定义后不难写出状态转移方程如下： 12dp[i][0] = max(0, dp[i-1][0]) + arr[i-1];dp[i][1] = max(0, max(dp[i-1][1], dp[i-2][0])) + arr[i-1]; 从i=2开始循环 初始条件就是dp[1][0] = arr[0]; dp[1][1] = arr[0]; 最后实现代码如下： 123456789101112131415class Solution &#123;public: int maximumSum(vector&lt;int&gt;&amp; arr) &#123; int n = arr.size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1, vector&lt;int&gt;(2,0)); dp[1][0] = arr[0]; dp[1][1] = arr[0]; for(int i=2;i&lt;=n;++i)&#123; dp[i][0] = max(0, dp[i-1][0]) + arr[i-1]; dp[i][1] = max(0, max(dp[i-1][1], dp[i-2][0])) + arr[i-1]; &#125; int ans = dp[1][0]; for(int i=1;i&lt;=n;++i) ans = max(ans, max(dp[i][0], dp[i][1])); return ans; &#125;&#125;; 2024-07-22 题目传送门：2101. 引爆最多的炸弹 - 力扣（LeetCode） 首先根据炸弹爆炸半径建立一个有向图，然后对于每个点都DFS一遍，找到以当前点起爆能引爆的最大炸弹数，最后返回最大引爆数即可。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233class Solution &#123;public: int maximumDetonation(vector&lt;vector&lt;int&gt;&gt;&amp; bombs) &#123; int n = bombs.size(); vector&lt;vector&lt;int&gt;&gt; edges(n); for(int i=0;i&lt;n;++i) &#123; int xi = bombs[i][0], yi = bombs[i][1], vali = bombs[i][2]; for(int j=0;j&lt;n;++j)&#123; if(i==j) continue; int xj = bombs[j][0], yj = bombs[j][1]; if (sqrt(double(xi-xj)*double(xi-xj)+double(yi-yj)*double(yi-yj)) &lt;= (double)vali) &#123; edges[i].push_back(j); &#125; &#125; &#125; int ans = 1; vector&lt;bool&gt; vis(n); function&lt;int(int)&gt; dfs = [&amp;](int u)&#123; int tmp=1; vis[u] = true; for(auto v : edges[u]) &#123; if(!vis[v]) tmp += dfs(v); &#125; return tmp; &#125;; for(int i=0;i&lt;n;++i)&#123; for(int j=0;j&lt;n;++j) vis[j]=false; ans = max(ans, dfs(i)); &#125; return ans; &#125;&#125;; 2024-07-23 题目传送门： 最后实现代码如下： 1 2024-07-24 题目传送门：2766. 重新放置石块 - 力扣（LeetCode） 一道非常标准的C++ set 题，主要考察： set初始化 set与vector互相转化 set基础操作（增删查） 详细知识点见C++专题，这里不再赘述 最后实现代码如下： 123456789101112class Solution &#123;public: vector&lt;int&gt; relocateMarbles(vector&lt;int&gt;&amp; nums, vector&lt;int&gt;&amp; moveFrom, vector&lt;int&gt;&amp; moveTo) &#123; set&lt;int&gt; se(nums.begin(), nums.end()); int n = moveFrom.size(); for(int i=0;i&lt;n;++i)&#123; if (se.find(moveFrom[i])==se.end()) continue; se.erase(moveFrom[i]); se.insert(moveTo[i]); &#125; return vector&lt;int&gt;(se.begin(),se.end()); &#125;&#125;; 2024-07-25 题目传送门：2844. 生成特殊数字的最少操作 - 力扣（LeetCode） 观察数据范围可以看到数字最长为100位，所以此题简单贪心即可。从最后一位倒着遍历，碰到5就找最近的2，7；碰到0就找最近的0，5。在遍历的过程中维护操作最小值即可。 注意：一个0也是能够被25整除的 最后实现代码如下： 12345678910111213141516171819202122232425class Solution &#123;public: int minimumOperations(string num) &#123; int n = num.size(); int ans = n; bool flag0 = false; for(int i=n-1;i&gt;=0;--i)&#123; if (num[i]==&#x27;0&#x27;) &#123; flag0 = true; int tmp = n-1-i; int j=i-1; while (j&gt;=0 &amp;&amp; num[j]!=&#x27;0&#x27; &amp;&amp; num[j]!=&#x27;5&#x27;)j--; if (j!=-1) &#123;tmp += (i-1-j); ans = min(ans, tmp);&#125; &#125; if (num[i]==&#x27;5&#x27;) &#123; int tmp = n-1-i; int j=i-1; while (j&gt;=0 &amp;&amp; num[j]!=&#x27;2&#x27; &amp;&amp; num[j]!=&#x27;7&#x27;)j--; if (j!=-1) &#123;tmp += (i-1-j); ans = min(ans, tmp);&#125; &#125; &#125; if (ans == n &amp;&amp; flag0) ans--; return ans; &#125;&#125;; 2024-07-26 题目传送门：2740. 找出分区值 - 力扣（LeetCode） 读完题后，仔细理解一下不难发现就是找排序后相邻元素之间差值的最小。 最后实现代码如下： 123456789class Solution &#123;public: int findValueOfPartition(vector&lt;int&gt;&amp; nums) &#123; sort(nums.begin(), nums.end()); int ans = nums[1] - nums[0], n = nums.size(); for(int i=1;i&lt;n;++i) ans = min(ans, nums[i]-nums[i-1]); return ans; &#125;&#125;; 2024-07-27 题目传送门：3106. 满足距离约束且字典序最小的字符串 - 力扣（LeetCode） 贪心，从最前面开始变，尽量让前面的字符接近a。 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: string getSmallestString(string s, int k) &#123; function&lt;int(char, char)&gt; dist = [&amp;](char a, char b)&#123; int dis = abs(a-b); return min(dis,26-dis); &#125;; for(char&amp; c : s)&#123; if (dist(c,&#x27;a&#x27;) &lt;=k ) &#123; k -= dist(c,&#x27;a&#x27;); c = &#x27;a&#x27;; &#125; else &#123; char b = &#x27;a&#x27;; while(dist(c,b) &gt; k) b++; c = char(b); break; &#125; &#125; return s; &#125;&#125;; 2024-07-28 题目传送门：699. 掉落的方块 - 力扣（LeetCode） 由于方块数量本身很少，所以可以采用枚举的方式进行最大高度更新，每次枚举到第i个方块，就遍历他前面i-1个方块，看当前方块是否落在前面方块上面，如果是则更新最大高度。依次遍历完所有的方块可得最终答案，时间复杂度为O(n2)O(n^2)O(n2)。 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: vector&lt;int&gt; fallingSquares(vector&lt;vector&lt;int&gt;&gt;&amp; positions) &#123; int n = positions.size(); vector&lt;int&gt; heights(n); for(int i=0;i&lt;n;++i)&#123; int left1 = positions[i][0], right1 = positions[i][0] + positions[i][1] - 1; heights[i] = positions[i][1]; for(int j=0;j&lt;i;++j)&#123; int left2 = positions[j][0], right2 = positions[j][0] + positions[j][1] - 1; if (right1 &gt;= left2 &amp;&amp; right2 &gt;= left1) heights[i] = max(heights[i], heights[j] + positions[i][1]); &#125; &#125; for (int i = 1; i &lt; n; i++) &#123; heights[i] = max(heights[i], heights[i - 1]); &#125; return heights; &#125;&#125;; 2024-07-29 题目传送门：682. 棒球比赛 - 力扣（LeetCode） 模拟题，C需要使用stoi用于字符串转整型，具体用法详见C专题。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int calPoints(vector&lt;string&gt;&amp; operations) &#123; vector&lt;int&gt; que; int n = 0; for(string operation : operations)&#123; if (operation == &quot;C&quot;) que.pop_back(),n--; else if (operation == &quot;D&quot;) que.push_back(que[n-1]*2),n++; else if (operation == &quot;+&quot;) que.push_back(que[n-1]+que[n-2]),n++; else que.push_back(stoi(operation)),n++; &#125; int ans = 0; for(int i=0;i&lt;n;++i) ans += que[i]; return ans; &#125;&#125;; 2024-07-30 题目传送门：2961. 双模幂运算 - 力扣（LeetCode） 快速幂板子题，快速幂算法详解见OI-Knowledge专题数学章节。 最后实现代码如下： 1234567891011121314151617class Solution &#123;public: vector&lt;int&gt; getGoodIndices(vector&lt;vector&lt;int&gt;&gt;&amp; variables, int target) &#123; function&lt;int(int,int,int)&gt; ksm = [&amp;](int a,int b,int m)&#123; if (b == 0) return 1; int nw = ksm(a,b/2,m)%m; if (b%2) return nw*nw%m*a%m; return nw*nw%m; &#125;; int n = variables.size(); vector&lt;int&gt; ans; for(int i=0;i&lt;n;++i)&#123; if (ksm(ksm(variables[i][0],variables[i][1],10),variables[i][2],variables[i][3])==target) ans.push_back(i); &#125; return ans; &#125;&#125;; 2024-07-31 题目传送门：3111. 覆盖所有点的最少矩形数目 - 力扣（LeetCode） 贪心，排序后从小到大进行覆盖即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: int minRectanglesToCoverPoints(vector&lt;vector&lt;int&gt;&gt;&amp; points, int w) &#123; sort(points.begin(), points.end()); int ans = 0, n = points.size(); int prev = points[0][0]-1; for(int i=0;i&lt;n;++i)&#123; if (points[i][0]&lt;=prev) continue; ans+=1; prev = points[i][0]+w; &#125; return ans; &#125;&#125;;"},{"title":"2024年10月每日一题","path":"/wiki/LeetCode/每日一题/2024年10月每日一题.html","content":"2024-10-01 题目传送门： 最后实现代码如下： 1 2024-10-02 题目传送门：1928. 规定时间内到达终点的最小花费 - 力扣（LeetCode） 最后实现代码如下： 1"},{"title":"2024年11月每日一题","path":"/wiki/LeetCode/每日一题/2024年11月每日一题.html","content":"2024-11-06 题目传送门：3254. 长度为 K 的子数组的能量值 I - 力扣（LeetCode） 最后实现代码如下： 1"},{"title":"2025年01月每日一题","path":"/wiki/LeetCode/每日一题/2025年01月每日一题.html","content":"2025-01-01 题目传送门：3280. 将日期转换为二进制表示 - 力扣（LeetCode） 模拟题，使用Python解决会更加方便一些，先将对应位置的字符串转成int，然后再做转二进制的操作，最后在将其转成string类型。 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: string binary(int x) &#123; string s; while (x) &#123; s.push_back(&#x27;0&#x27; + (x &amp; 1)); x &gt;&gt;= 1; &#125; reverse(s.begin(), s.end()); return s; &#125; string convertDateToBinary(string date) &#123; int year = stoi(date.substr(0, 4)); int month = stoi(date.substr(5, 2)); int day = stoi(date.substr(8, 2)); return binary(year) + &quot;-&quot; + binary(month) + &quot;-&quot; + binary(day); &#125;&#125;; 2025-01-02 题目传送门： 最后实现代码如下： 1"},{"title":"2025年02月每日一题","path":"/wiki/LeetCode/每日一题/2025年02月每日一题.html","content":"2025-02-01 题目传送门：81. 搜索旋转排序数组 II - 力扣（LeetCode） 这道题中，数组本身不是有序的，进行旋转后只保证了数组的局部是有序的，这还能进行二分查找吗？答案是可以的。 可以发现的是，我们将数组从中间分开成左右两部分的时候，一定有一部分的数组是有序的。拿示例来看，我们从 6 这个位置分开以后数组变成了 [4,5,6][4, 5, 6][4,5,6] 和 [7,0,1,2][7, 0, 1, 2][7,0,1,2] 两个部分，其中左边 [4,5,6][4, 5, 6][4,5,6] 这个部分的数组是有序的，其他也是如此。 这启示我们可以在常规二分查找的时候查看当前 mid 为分割位置分割出来的两个部分 [l,mid][l, mid][l,mid] 和 [mid+1,r][mid + 1, r][mid+1,r] 哪个部分是有序的，并根据有序的那个部分确定我们该如何改变二分查找的上下界，因为我们能够根据有序的那部分判断出 target 在不在这个部分： 对于数组中有重复元素的情况，二分查找时可能会有 a[l]=a[mid]=a[r]a[l]=a[mid]=a[r]a[l]=a[mid]=a[r]，此时无法判断区间 [l,mid][l,mid][l,mid] 和区间 [mid+1,r][mid+1,r][mid+1,r] 哪个是有序的。 例如 nums=[3,1,2,3,3,3,3]nums=[3,1,2,3,3,3,3]nums=[3,1,2,3,3,3,3]，target=2target=2target=2，首次二分时无法判断区间 [0,3][0,3][0,3] 和区间 [4,6][4,6][4,6] 哪个是有序的。 对于这种情况，我们只能将当前二分区间的左边界加一，右边界减一，然后在新区间上继续二分查找。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: bool search(vector&lt;int&gt; &amp;nums, int target) &#123; int n = nums.size(); if (n == 0) &#123; return false; &#125; if (n == 1) &#123; return nums[0] == target; &#125; int l = 0, r = n - 1; while (l &lt;= r) &#123; int mid = (l + r) / 2; if (nums[mid] == target) &#123; return true; &#125; if (nums[l] == nums[mid] &amp;&amp; nums[mid] == nums[r]) &#123; ++l; --r; &#125; else if (nums[l] &lt;= nums[mid]) &#123; if (nums[l] &lt;= target &amp;&amp; target &lt; nums[mid]) &#123; r = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; else &#123; if (nums[mid] &lt; target &amp;&amp; target &lt;= nums[n - 1]) &#123; l = mid + 1; &#125; else &#123; r = mid - 1; &#125; &#125; &#125; return false; &#125;&#125;; 2025-02-02 题目传送门：598. 区间加法 II - 力扣（LeetCode） 就是找所有操作中的最小行号和最小列号相乘就可以了。 最后实现代码如下： 123456789101112class Solution &#123;public: int maxCount(int m, int n, vector&lt;vector&lt;int&gt;&gt;&amp; ops) &#123; if (ops.size()==0) return m*n; int minR = ops[0][0], minC = ops[0][1]; for (auto &amp; op : ops) &#123; minR = min(minR, op[0]); minC = min(minC, op[1]); &#125; return minR * minC; &#125;&#125;; 2025-02-03 题目传送门：680. 验证回文串 II - 力扣（LeetCode） 双指针即可 最后实现代码如下： 12345678910111213141516171819202122232425class Solution &#123;public: bool checkPalindrome(const string&amp; s, int low, int high) &#123; for (int i = low, j = high; i &lt; j; ++i, --j) &#123; if (s[i] != s[j]) &#123; return false; &#125; &#125; return true; &#125; bool validPalindrome(string s) &#123; int low = 0, high = s.size() - 1; while (low &lt; high) &#123; char c1 = s[low], c2 = s[high]; if (c1 == c2) &#123; ++low; --high; &#125; else &#123; return checkPalindrome(s, low, high - 1) || checkPalindrome(s, low + 1, high); &#125; &#125; return true; &#125;&#125;; 2025-02-04 题目传送门：922. 按奇偶排序数组 II - 力扣（LeetCode） 分类排序完成后，再合并即可。 最后实现代码如下： 12345678910111213141516class Solution &#123;public: vector&lt;int&gt; sortArrayByParityII(vector&lt;int&gt;&amp; nums) &#123; vector&lt;int&gt; odd, even; for (auto &amp;num : nums) &#123; if (num % 2) odd.emplace_back(num); else even.emplace_back(num); &#125; vector&lt;int&gt; ans; for(int i=0;i&lt;odd.size();++i)&#123; ans.emplace_back(even[i]); ans.emplace_back(odd[i]); &#125; return ans; &#125;&#125;; 2025-02-05 题目传送门：90. 子集 II - 力扣（LeetCode） 暴搜+集合set熟练运用 最后实现代码如下： 123456789101112131415161718192021class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); set&lt;vector&lt;int&gt;&gt; se; for(int i=0;i&lt;(1&lt;&lt;n);++i)&#123; vector&lt;int&gt; tmp; int nw = i; for(int j=0;j&lt;n;++j)&#123; if ((nw&gt;&gt;j)&amp;1) tmp.push_back(nums[j]); &#125; sort(tmp.begin(), tmp.end()); se.insert(tmp); &#125; vector&lt;vector&lt;int&gt;&gt; ans; for(auto &amp; itm : se)&#123; ans.push_back(itm); &#125; return ans; &#125;&#125;; 2025-02-06 题目传送门：47. 全排列 II - 力扣（LeetCode） 同上，暴搜+集合set熟练运用 最后实现代码如下： 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; set&lt;vector&lt;int&gt;&gt; se; int n = nums.size(); vector&lt;bool&gt; flag(n,false); vector&lt;int&gt; tmp; function&lt;void(int)&gt; func = [&amp;](int cnt)&#123; if (cnt == n) &#123; se.insert(tmp); return; &#125; for(int i=0;i&lt;n;++i)&#123; if (!flag[i]) &#123; tmp.push_back(nums[i]); flag[i] = true; func(cnt+1); tmp.pop_back(); flag[i] = false; &#125; &#125; &#125;; func(0); vector&lt;vector&lt;int&gt;&gt; ans; for (auto &amp; itm : se) &#123; ans.push_back(itm); &#125; return ans; &#125;&#125;; 但是上面这种做法效率会非常低。 2025-02-07 题目传送门：59. 螺旋矩阵 II - 力扣（LeetCode） 大模拟 最后实现代码如下： 12345678910111213141516171819202122class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; generateMatrix(int n) &#123; int maxNum = n * n; int curNum = 1; vector&lt;vector&lt;int&gt;&gt; matrix(n, vector&lt;int&gt;(n)); int row = 0, column = 0; vector&lt;vector&lt;int&gt;&gt; directions = &#123;&#123;0, 1&#125;, &#123;1, 0&#125;, &#123;0, -1&#125;, &#123;-1, 0&#125;&#125;; // 右下左上 int directionIndex = 0; while (curNum &lt;= maxNum) &#123; matrix[row][column] = curNum; curNum++; int nextRow = row + directions[directionIndex][0], nextColumn = column + directions[directionIndex][1]; if (nextRow &lt; 0 || nextRow &gt;= n || nextColumn &lt; 0 || nextColumn &gt;= n || matrix[nextRow][nextColumn] != 0) &#123; directionIndex = (directionIndex + 1) % 4; // 顺时针旋转至下一个方向 &#125; row = row + directions[directionIndex][0]; column = column + directions[directionIndex][1]; &#125; return matrix; &#125;&#125;; 2025-02-08 题目传送门：63. 不同路径 II - 力扣（LeetCode） 简单的动态规划问题 最后实现代码如下： 12345678910111213141516class Solution &#123;public: int uniquePathsWithObstacles(vector&lt;vector&lt;int&gt;&gt;&amp; obstacleGrid) &#123; int n = obstacleGrid.size(), m = obstacleGrid[0].size(); vector&lt;vector&lt;int&gt;&gt; dp(n+1,vector&lt;int&gt;(m+1,0)); // if (obstacleGrid[0][0]) return 0; dp[1][1] = obstacleGrid[0][0]==0?1:0; for(int i=1;i&lt;=n;++i)&#123; for(int j=1;j&lt;=m;++j)&#123; if (obstacleGrid[i-1][j-1]) continue; dp[i][j] += dp[i-1][j] + dp[i][j-1]; &#125; &#125; return dp[n][m]; &#125;&#125;; 2025-02-09 题目传送门：80. 删除有序数组中的重复项 II - 力扣（LeetCode） 双指针即可。 最后实现代码如下： 123456789101112131415161718class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; int n = nums.size(); if (n &lt;= 2) &#123; return n; &#125; int slow = 2, fast = 2; while (fast &lt; n) &#123; if (nums[slow - 2] != nums[fast]) &#123; nums[slow] = nums[fast]; ++slow; &#125; ++fast; &#125; return slow; &#125;&#125;; 2025-02-10 题目传送门： 最后实现代码如下： 1 2025-02-11 题目传送门： 最后实现代码如下： 1 2025-02-12 题目传送门：1760. 袋子里最少数目的球 - 力扣（LeetCode） 二分一下就可以啦 最后实现代码如下： 12345678910111213141516171819class Solution &#123;public: int minimumSize(vector&lt;int&gt;&amp; nums, int maxOperations) &#123; function&lt;bool(int)&gt; judge = [&amp;](int ball)&#123; int nw = 0; for(auto num : nums)&#123; nw += (num-1) / ball; &#125; return nw &lt;= maxOperations ? true : false; &#125;; int l = 1, r=1e9+1; while(l&lt;r)&#123; int mid = (r-l)/2 + l; if (judge(mid)) r = mid; else l = mid+1; &#125; return l; &#125;&#125;; 2025-02-13 题目传送门：1742. 盒子中小球的最大数量 - 力扣（LeetCode） 模拟题 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: int countBalls(int lowLimit, int highLimit) &#123; function&lt;int(int)&gt; func = [&amp;](int cnt)&#123; int ans = 0; while(cnt)&#123; ans += (cnt%10); cnt /= 10; &#125; return ans; &#125;; vector&lt;int&gt; buk(100,0); for(int i=lowLimit;i&lt;=highLimit;++i)&#123; buk[func(i)]++; &#125; int ans = buk[func(lowLimit)]; for(int i=0;i&lt;99;++i) ans = max(ans,buk[i]); return ans; &#125;&#125;; 2025-02-14 题目传送门：1552. 两球之间的磁力 - 力扣（LeetCode） 二分一下啦 最后实现代码如下： 1234567891011121314151617181920212223class Solution &#123;public: int maxDistance(vector&lt;int&gt;&amp; position, int m) &#123; sort(position.begin(), position.end()); function&lt;bool(int)&gt; judge = [&amp;](int interval)&#123; int pre = position[0], cnt = 1; for(int i = 1; i &lt; position.size(); ++i) &#123; if (position[i] - pre &gt;= interval) &#123; pre = position[i]; cnt += 1; &#125; &#125; return cnt &gt;= m; &#125;; int l = 1, r = 1e9+1; while (l &lt; r) &#123; int mid = l + (r - l) / 2; if (judge(mid)) l =mid+1; else r = mid; &#125; return l-1; &#125;&#125;; 2025-02-15 题目传送门：1706. 球会落何处 - 力扣（LeetCode） 模拟题，简单模拟下就好。 最后实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution &#123;public: vector&lt;int&gt; findBall(vector&lt;vector&lt;int&gt;&gt;&amp; grid) &#123; int n = grid.size(), m = grid[0].size(); vector&lt;int&gt; ans; for(int i=0; i&lt;m; ++i)&#123; int nw = i; bool flag = false; for(int j=0;j&lt;n;++j)&#123; if(grid[j][nw]==1)&#123; // 如果当前块是朝右 if (nw!=m-1) &#123; if (grid[j][nw+1]==1)&#123; nw++; &#125; else &#123; flag = true;break; // if(j!=n-1) &#123;flag = true;break;&#125; // nw++; &#125; &#125; else &#123; // 如果是最后一列 flag = true;break; // if(j!=n-1) &#123;flag = true;break;&#125; // nw++; &#125; &#125; else &#123; if (nw!=0) &#123; if (grid[j][nw-1]==-1)&#123; nw--; &#125; else &#123; flag = true;break; // if(j!=n-1) &#123;flag=true; break;&#125; // nw--; &#125; &#125; else &#123; flag = true;break; // if(j!=n-1) &#123;flag=true; break;&#125; // nw--; &#125; &#125; &#125; if (!flag) ans.push_back(nw); else ans.push_back(-1); &#125; return ans; &#125;&#125;; 2025-02-16 题目传送门：1299. 将每个元素替换为右侧最大元素 - 力扣（LeetCode） 模拟简单题啦，遍历一遍即可。 最后实现代码如下： 1234567891011class Solution &#123;public: vector&lt;int&gt; replaceElements(vector&lt;int&gt;&amp; arr) &#123; int n = arr.size(); vector&lt;int&gt; ans(n,-1); for(int i=n-2;i&gt;=0;--i)&#123; ans[i] = max(arr[i+1],ans[i+1]); &#125; return ans; &#125;&#125;; 2025-02-17 题目传送门：1287. 有序数组中出现次数超过25%的元素 - 力扣（LeetCode） 排序后，计数判断即可～ 最后实现代码如下： 1234567891011121314151617181920class Solution &#123;public: int findSpecialInteger(vector&lt;int&gt;&amp; arr) &#123; sort(arr.begin(), arr.end()); int n = arr.size(); int prev = -1; int cnt = 1; for(int i=0; i&lt;n; ++i)&#123; if (arr[i]!=prev) &#123; cnt = 1; prev = arr[i]; &#125; else &#123; cnt++; &#125; if (cnt*4 &gt; n) return prev; &#125; return -1; &#125;&#125;; 2025-02-18 题目传送门：2080. 区间内查询数字的频率 - 力扣（LeetCode） 哈希表+二分查找，对于每个值都建立一个列表，按照顺序维护他出现的位置，然后对这个列表进行upper_bound-lower_bound即可。 最后实现代码如下： 123456789101112131415161718192021class RangeFreqQuery &#123;public: unordered_map&lt;int,vector&lt;int&gt;&gt; mp; RangeFreqQuery(vector&lt;int&gt;&amp; arr) &#123; int n = arr.size(); for (int i=0;i&lt;n;++i)&#123; mp[arr[i]].push_back(i); &#125; &#125; int query(int left, int right, int value) &#123; vector&lt;int&gt;&amp; tmp = mp[value]; return upper_bound(tmp.begin(),tmp.end(),right) - lower_bound(tmp.begin(),tmp.end(),left); &#125;&#125;;/** * Your RangeFreqQuery object will be instantiated and called as such: * RangeFreqQuery* obj = new RangeFreqQuery(arr); * int param_1 = obj-&gt;query(left,right,value); */ 2025-02-19 题目传送门：624. 数组列表中的最大距离 - 力扣（LeetCode） 对于每个数组，维护一个最大值列表和最小值列表（用列表形式存储每个数组中的最大值和最小值）。 然后找出这个列表中的最大值、次大值以及最小值、次小值。 如果最大值和最小值不在一个数组中的话，返回他们两个的差。 如果在的话，就返回max(最大值-次小值，次大值-最小值) 最后实现代码如下： 12345678910111213141516171819202122232425class Solution &#123;public: int maxDistance(vector&lt;vector&lt;int&gt;&gt;&amp; arrays) &#123; int n = arrays.size(); vector&lt;int&gt; maxx, minn; for (auto arr : arrays) &#123; int nn = arr.size(); minn.push_back(arr[ 0 ]); maxx.push_back(arr[nn-1]); &#125; int max_num1=maxx[0],max_loc1=0,max_num2=maxx[0],max_loc2=0; int min_num1=minn[0],min_loc1=0,min_num2=minn[0],min_loc2=0; for( int i = 0; i &lt; n ; ++ i ) &#123; if (maxx[i] &gt; max_num1) max_num1 = maxx[i], max_loc1 = i; if (minn[i] &lt; min_num1) min_num1 = minn[i], min_loc1 = i; &#125; if (max_loc1 == 0 &amp;&amp; n &gt; 1) max_loc2 = 1, max_num2 = maxx[1]; if (min_loc1 == 0 &amp;&amp; n &gt; 1) min_loc2 = 1, min_num2 = minn[1]; for( int i = 0; i &lt; n ; ++ i ) &#123; if (maxx[i] &gt; max_num2 &amp;&amp; i != max_loc1) max_num2 = maxx[i], max_loc2 = i; if (minn[i] &lt; min_num2 &amp;&amp; i != min_loc1) min_num2 = minn[i], min_loc2 = i; &#125; return max_loc1 != min_loc1 ? max_num1-min_num1 : max(max_num1-min_num2, max_num2-min_num1); &#125;&#125;; 2025-02-20 题目传送门：2595. 奇偶位数 - 力扣（LeetCode） 模2除2，做统计即可。 最后实现代码如下： 12345678910111213class Solution &#123;public: vector&lt;int&gt; evenOddBit(int n) &#123; vector&lt;int&gt; ans(2,0); int cnt = 0; while ( n ) &#123; ans[cnt%2] += n%2; n /= 2; cnt++; &#125; return ans; &#125;&#125;;"},{"title":"Katex公式配置","path":"/wiki/Others/Stellar/Katex公式配置.html","content":"想在Stellar主题中使用katex书写公式，就需要更改渲染器，使用如下代码卸载博客中原生的渲染器hexo-renderer-marked，下载新渲染器hexo-renderer-markdown-it以及katex支持@traptitech/markdown-it-katex： 123npm un hexo-renderer-marked --save npm i hexo-renderer-markdown-it --savenpm i @traptitech/markdown-it-katex --save 但是更改渲染器后会出现headerlink无法渲染出来的问题，此时需要安装一个包markdown-it-anchor用于headerlink的渲染。 1npm install markdown-it-anchor --save 下载完成后在_config.yml中添加如下设置代码： 12345678910111213141516171819202122232425262728293031markdown: preset: &#x27;default&#x27; render: html: true xhtmlOut: true breaks: true langPrefix: &#x27;language-&#x27; linkify: true typographer: true quotes: &#x27;“”‘’&#x27; plugins: - &#x27;markdown-it-footnote&#x27; # 角标 - &#x27;markdown-it-ins&#x27; # 下划线 - &#x27;markdown-it-mark&#x27; # 高亮 - &#x27;markdown-it-sub&#x27; # 分子式 - &#x27;markdown-it-sup&#x27; # 上角标 - plugin: name: &#x27;@traptitech/markdown-it-katex&#x27; options: # see https://katex.org/docs/options.html blockClass: &quot;math-block&quot; strict: false throwOnError: false errorColor: &quot;#cc0000&quot; - markdown-it-anchor anchors: level: 2 collisionSuffix: &#x27;&#x27; permalink: true permalinkClass: &#x27;headerlink&#x27; permalinkBefore: true permalinkSymbol: &#x27;&#x27; 在主题配置文件_config.stellar.yml中添加如下代码： 1234567plugins: katex: enable: true min_css: &lt;link rel=&quot;stylesheet&quot; href=&quot;https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css&quot; integrity=&quot;sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0&quot; crossorigin=&quot;anonymous&quot;&gt; min_js: &lt;script defer src=&quot;https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js&quot; integrity=&quot;sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt; auto_render_min_js: &lt;script defer src=&quot;https://gcore.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js&quot; integrity=&quot;sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05&quot; crossorigin=&quot;anonymous&quot;onload=&quot;renderMathInElement(document.body);&quot;&gt;&lt;/script&gt; 此时公式与headerlink便能够正常显示了。"},{"title":"实现博客专栏/专题","path":"/wiki/Others/Stellar/topic.html","content":"如果你使用过 Stellar 的 wiki 系统，那么专栏就非常容易了，相当于一个简化版的 wiki 系统，区别是： 无需「上架」动作 文章创建于 blog/source/_posts 文件夹内 按照时间排序，默认最新的排最上面 页面布局类似于普通文章 基本流程 1. 创建一个专栏 在 blog/source/_data/ 文件夹中创建一个 topic 文件夹，在其中放入各个专栏的描述文件，文件名就是项目的 id： blog/source/_data/topic/id.yml1234name: Stellar # 在面包屑导航上会显示较短的名字title: Stellar - 每个人的独立博客 # 在列表页会显示完整的专栏标题description: 关于搭建独立博客相关的知识和经验分享，以及 Stellar 的高级用法、版本更新相关的注意事项。order_by: -date # 默认是按发布日期倒序排序 2. 发布文章 在此专栏文章的 md 文件的 front-matter 部分指定所属的专栏 id （即上一步创建的文件名 id.yml） blog/source/_posts/20240114.md123456---title: 这是文章标题topic: id # 这是专栏id，对应 blog/source/_data/topic/id.yml---文章正文 这个功能的定位是什么？ 相比分类功能，它更偏向于一个更加有前后关系的文章集合，类似于文档的分页，但是相比文档，它又像文章一样持续增加新页面，一般以时间为排序依据。比分类更加结构化，比文档更加自动化，可以根据自己的需求选择使用不同的功能。 https://xaoxuu.com/blog/20240203/https://xaoxuu.com/blog/20240203/"},{"title":"如何使用文档系统","path":"/wiki/Others/Stellar/如何使用文档系统.html","content":"Stellar 独创了其它 Hexo 主题所没有的 Wiki 文档系统，可以自动找到一个项目的所有文档分页，生成一个目录树，还可以手动指定顺序、标题、分组，而非依赖文件路径、文件名来排序和显示。 基本流程 1/3 创建项目描述文件在 blog/source/_data/ 文件夹中创建一个 wiki 文件夹，在其中放入各个项目的文档。以 Stellar 项目为例，文件名就是项目的 id：blog/source/_data/wiki/hexo-stellar.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748name: Stellartitle: Stellar - 每个人的独立博客subtitle: &#x27;每个人的独立博客 | Designed by xaoxuu&#x27;tags: 博客主题icon: /assets/wiki/stellar/icon.svgcover: /assets/wiki/stellar/icon.svgdescription: Stellar 是一个内置文档系统的简约商务风 Hexo 主题，支持丰富的标签和动态数据组件。repo: xaoxuu/hexo-theme-stellarsearch: filter: /wiki/stellar/ placeholder: 在 Stellar 中搜索...leftbar: - tree - timeline_stellar_releases - relatedcomment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;comments: service: giscus giscus: data-repo: xaoxuu/hexo-theme-stellar data-mapping: number data-term: 226base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index - examples - releases &#x27;基本使用&#x27;: - theme-settings - pages - sidebar - tag-plugins - tag-plugins/express - tag-plugins/data - tag-plugins/container - comments &#x27;文档系统&#x27;: - wiki-settings &#x27;进阶玩法&#x27;: - widgets - advanced-settings - notes - fcircle &#x27;技术支持&#x27;: - articles - todo - contributors2/3 设置布局模板和项目名称在此文档项目的 md 文件的 front-matter 部分指定所属的项目 id （即上一步创建的文件名 id.yml）blog/source/wiki/stellar/index.md1234---wiki: hexo-stellar # 这是项目id，对应 /data/wiki/hexo-stellar.ymltitle: 这是分页标题---3/3 将此项目「上架」在 blog/source/_data/ 文件夹中创建一个 wiki.yml 文件，在其中写入需要显示的项目 id：blog/source/_data/wiki.yml12- hexo-stellar- 其它项目这样在项目列表（wiki）页面就可以看到刚刚创建的项目了。 项目分页索引 指定项目所在文件夹和目录树： blog/source/_data/wiki/hexo-stellar.yml1234567891011121314151617181920212223242526base_dir: /wiki/stellar/tree: &#x27;快速开始&#x27;: - index # 会被关联到 /wiki/stellar/index.md - examples # 会被关联到 /wiki/stellar/examples.md - releases &#x27;基本使用&#x27;: - theme-settings - pages - sidebar - tag-plugins - tag-plugins/express - tag-plugins/data - tag-plugins/container - comments &#x27;文档系统&#x27;: - wiki-settings &#x27;进阶玩法&#x27;: - widgets - advanced-settings - notes - fcircle &#x27;技术支持&#x27;: - articles - todo - contributors 如果目录树不需要分组，可以这样写： blog/source/_data/wiki/hexo-stellar.yml12345base_dir: /wiki/stellar/tree: - index # 会被关联到 /wiki/stellar/index.md - examples # 会被关联到 /wiki/stellar/examples.md - ... 是否显示封面 项目可以显示一个全屏封面，封面占据一个屏幕的高度，会居中依次显示项目的 logo、标题、描述。开启项目封面方法如下： blog/source/_data/wiki/hexo-stellar.yml12cover: /assets/wiki/stellar/icon.svgcoverpage: true # 默认是 true 如果 logo 中已经包含了项目标题，可以这样设置不显示项目标题： blog/source/_data/wiki/hexo-stellar.yml1coverpage: [logo, description] 项目文档标签 如果您有很多项目，有些项目是有相关性的，可以相同的 tags 值： blog/source/_data/wiki/hexo-stellar.yml1tags: 博客主题 也可以设置多个 tags 值： blog/source/_data/wiki/hexo-stellar.yml1tags: [博客主题, 开源项目] 项目的 GitHub 仓库信息 设置了 repo 值就会在右上角显示项目仓库的相关链接： blog/source/_data/wiki/hexo-stellar.yml1repo: xaoxuu/hexo-theme-stellar 项目评论设置 如果希望项目的所有分页使用相同的评论数据，可以在这里覆盖评论配置： blog/source/_data/wiki/hexo-stellar.yml123456comment_title: &#x27;评论区仅供交流，有问题请提 [issue](https://github.com/xaoxuu/hexo-theme-stellar/issues) 反馈。&#x27;comments: giscus: data-repo: xaoxuu/hexo-theme-stellar data-mapping: number data-term: 226 侧边栏组件 如果您希望自定义某个项目的侧边栏组件，可以设置 sidebar 值： 可以覆盖组件： blog/source/_data/wiki/hexo-stellar.yml1234leftbar: - tree - timeline_stellar_releases - related todo 在目录树中隐藏某篇文章 可以在 front-matter 中不设置 title 标题，或者将 title 改为 seo_title： blog/source/xxx/xxx.md1title: 原本的标题 todo 显示许可协议 沿用主题配置文件中设置的： blog/source/_data/wiki/hexo-stellar.yml1license: true 也可以指定协议内容： blog/source/_data/wiki/hexo-stellar.yml1license: &#x27;本文采用 [署名-非商业性使用-相同方式共享 4.0 国际](https://creativecommons.org/licenses/by-nc-sa/4.0/) 许可协议，转载请注明出处。&#x27; 显示分享 blog/source/_data/wiki/hexo-stellar.yml1share: true 修改 wiki 路径 修改如下配置： blog/_config.stellar.yml123site_tree: wiki: base_dir: wiki # books / products ..."},{"title":"编写文章以及独立页面","path":"/wiki/Others/Stellar/编写文章以及独立页面.html","content":"文章类型 文章类型决定布局风格，有两种风格可选，tech: 默认技术类文章, story: 图文类文章，文字和段落间增距大。 blog/_config.stellar.yml12article: type: tech # tech/story 可以在 front-matter/topic/wiki 中覆盖此参数。 文章封面 在文章列表页面或者其他位置显示的文章摘要卡片上面的图片称之为「文章封面」 自动生成封面 根据 tags 作为关键词为每一篇文章在线搜索封面： blog/_config.stellar.yml12article: auto_cover: true 引用外部图片 在文章的 front-matter 中写上 cover: xxx 即可。例如： blog/source/_posts/xxx.md12345---# 本地图片路径为 blog/source/assets/xaoxuu/blog/2020-0927a@1x.svg# 也可以直接引用图片直链 https://xxx.jpgcover: /assets/xaoxuu/blog/2020-0927a@1x.svg--- 显示效果 上面这种方式会显示title与description或者摘要，若你想要图片全显示，可以加入如下参数： blog/source/_posts/xxx.md12345678---cover: /assets/xaoxuu/blog/2020-0927a@1x.svg # 必选poster: # 海报（可选，全图封面卡片） topic: 标题上方的小字 # 可选 headline: 大标题 # 必选 caption: 标题下方的小字 # 可选 color: 标题颜色 # 可选，默认为跟随主题的动态颜色 # white,red...--- Stellar v1.14.0 更换 cover-title cover-cat cover-subtitle cover-text-color 为 poster 为了显示美观，建议 topic 和 caption 选择其一与 headline 搭配使用。 显示效果填写 topic 与 headline 时大标题位于上方只填写 headline 或填写 headline 与 caption 时大标题位于下方 如果您想使用 Unsplash 搜索图片作为封面，可以在 cover 设置搜索关键词（用英文逗号隔开）： blog/source/_posts/xxx.md123---cover: workout,strava--- 内容摘要 自动生成摘要 建议您通过 description 或者 excerpt 方式生成摘要，但如果您希望自动从文章内容截取一定字数的文字作为摘要，可以这样设置： blog/_config.stellar.yml12article: auto_excerpt: 200 手动设置摘要 一篇文章开头一段文字描述就是摘要，摘要和正文用 &lt;!-- more --&gt; 隔开，前后一定要有空行。例如： blog/source/_posts/xxx.md123456789---cover: /assets/xaoxuu/blog/2020-0927a@1x.svg---在心率管家默默无闻地上线了一年多之后，现在终于打算来好好聊聊关于手机摄像头测量心率的那些事。本文参考了很多前辈的文章，将在文末列出。&lt;!-- more --&gt;后面是正文部分，在主页看不到。 AI摘要 基于 tianli_gpt 前端项目 Post-Summary-AI _config.stellar.yml123456789101112131415161718# AI 摘要# https://github.com/qxchuckle/Post-Summary-AItianli_gpt: enable: #true js: https://jsd.onmicrosoft.cn/gh/qxchuckle/Post-Summary-AI@6.0/chuckle-post-ai.min.js field: post # all, post, wiki key: 5Q5mpqRK5DkwT1X9Gi5e # tianli_gpt key total_length: 1000 # 设置提交的字数限制，默认为1000字，上限为5000，超过5000字符将被截断 typewriter: true # 打字机动画 summary_directly: true # 是否直接显示摘要，否则显示 AI 简介 rec_method: all # all, web # 文章推荐方式，all：匹配数据库内所有文章进行推荐，web：仅当前站内的文章，默认all hide_shuttle: true # 是否隐藏矩阵穿梭 summary_toggle: false # 是否开启切换简介功能，经过一些处理后，重新生成一份简介，这将消耗key字数 interface: # AI 信息展示 name: AI摘要 introduce: &#x27;我是文章辅助AI: QX-AI，点击下方的按钮，让我生成本文简介、推荐相关文章等。&#x27; version: TianliGPT button: [&quot;介绍自己&quot;, &quot;推荐文章&quot;, &quot;生成摘要&quot;, &quot;矩阵穿梭&quot;] # 底部按钮文字 如何获取 tianliGPT_key：到 爱发电 中购买，购买完成后，进入 网页后台管理 绑定key并添加自己的站点 key与博客地址为绑定状态，所以本地调试时是无法接收到数据的。 文章模板 使用 Hexo 自带模板实现命令行创建新文章时自动生成相关信息。 根目录下 scaffolds 文件夹中编辑 post.md 的 font-matter ，根据自己的需要增加你想要配置的内容 blog/scaffolds/post.md1234567891011121314151617181920212223242526272829303132---# 基本信息title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: []categories: []description: # excerpt 也可 # 封面cover: banner: poster: # 海报（可选，全图封面卡片） topic: 标题上方的小字 # 可选 headline: 大标题 # 必选 caption: 标题下方的小字 # 可选 color: 标题颜色 # 可选# 插件sticky: # 数字越大越靠前mermaid:katex: mathjax: # 可选topic: # 专栏 idauthor: references:comments: # 设置 false 禁止评论indexing: # 设置 false 避免被搜索breadcrumb: # 设置 false 隐藏面包屑导航leftbar: rightbar:h1: # 设置为 &#x27;&#x27; 隐藏标题type: # tech/story--- 文章页 横幅图片 文章页面顶部区域可以显示长长的横幅图片，设置方法如下： blog/source/_posts/xxx.md1banner: /assets/xaoxuu/blog/2020-0927a@1x.svg 如果您想使用 Unsplash 搜索图片作为横幅，可以在 banner 中设置搜索关键词（用英文逗号隔开）： blog/source/_posts/xxx.md123---banner: workout,strava--- 指定一级标题 默认的一级标题是文章的 title，如果希望使用别的文字作为一级标题，可以指定 h1，例如： blog/source/_posts/xxx.md123---h1: 快速开始--- 隐藏文章标题 同上述操作，但是 h1 设置为空字符串即可： blog/source/_posts/xxx.md123---h1: &#x27;&#x27;--- 文章索引与推荐 文章如果有分类和标签就会自动在主页出现「分类」、「标签」选项卡实现分类浏览，不需要手动添加页面。 文章分类 在文章列表页面会显示文章所属的第一级分类，例如： blog/source/_posts/xxx.md123---categories: [设计开发, iOS开发]--- 这样写就只会显示「设计开发」一级分类，而在文章页面顶部则会显示完整的面包屑导航。 文章标签 文章标签目前不可见，用于关键词、搜索、按标签检索、相关文章推荐等功能，例如： blog/source/_posts/xxx.md123---tags: [iOS, 心率]--- 相关文章推荐 要实现相关文章推荐功能，您需要安装插件： 然后在主题配置文件中开启： blog/_config.stellar.yml12345article: # npm i hexo-related-popular-posts related_posts: enable: true title: 您可能感兴趣的文章 开启后会在每篇文章的下方推荐相同类型的文章。 参考资料 用 markdown 格式填写引用的文章，注意要写在引号中： blog/source/_posts/xxx.md123456---references: - &#x27;[心跳之旅—💗—iOS用手机摄像头检测心率(PPG)](https://punmy.cn/2016/07/28/15231176397746.html)&#x27; - &#x27;[PPG光电容积脉搏波描记法技术概况](https://www.jianshu.com/p/695c131abfa5)&#x27; ...--- 效果见这篇文章： https://xaoxuu.com/blog/20200927/#referenceshttps://xaoxuu.com/blog/20200927/#references 许可协议 你可以更改协议内容或者自定义其他选项，支持 MarkDown 语法。 blog/_config.stellar.yml12article: license: &#x27;本文采用 [署名-非商业性使用-相同方式共享 4.0 国际](https://creativecommons.org/licenses/by-nc-sa/4.0/) 许可协议，转载请注明出处。&#x27; 若你配置了作者数据 _data/authors.yml 和文章作者，可以在 license 中使用 &#123;author.name&#125; 来自动替换为当前文章作者名字。 blog/_config.stellar.yml12article: license: &#x27;本文由&#123;author.name&#125;编写，采用...&#x27; 分享链接 分享至微信会生成对应的页面二维码，weibo 和 email 会自动跳转到对应软件或网页，link 会拷贝当前页面链接至剪切板。 blog/_config.stellar.yml12article: share: # [wechat, weibo, email, link] 覆盖 OpenGraph 如果分享到社交平台的缩略图不理想，可以通过这个特性覆盖为自己想要的： blog/source/_posts/xxx.md12open_graph: image: /assets/xaoxuu/blog/2022-1029a@2x.webp 更多的独立页面 Stellar 同时具有博客和 Wiki 两个大模块，为了能够正确进行导航栏高亮，引入了 menu_id 来进行区分，可以在 front-matter 中指定 menu_id 来使某个菜单按钮处于选中状态。 例如您有关于、友链两个页面，都希望高亮「更多」按钮： blog/source/about/index.md1234---menu_id: moretitle: 关于--- blog/source/friends/index.md1234---menu_id: moretitle: 友链--- 在主题配置文件中设置导航栏： blog/_config.stellar.yml123456789menubar: columns: 4 # 一行多少个 items: # 可按照自己需求增加，符合以下格式即可 ... - id: more theme: &#x27;&#x27; title: 更多 icon: &#x27;&#x27; url: /more/ 友链页面 友链被设计成标签，您可以在任何页面任何位置插入友链，详见： #友链标签https://xaoxuu.com/wiki/stellar/tag-plugins/#友链标签 关于页面 没有单独的关于页面布局，您可以自由组合丰富的标签来实现个性化的关于页面，例如：about、tabs、navbar、quot、timeline 标签。"},{"title":"Windows11 重装系统那些事","path":"/wiki/Others/Windows/Windows11 重装系统那些事.html","content":"每次电脑出问题后重装系统都是一件非常折磨的事情，故开此专题记录所踩过的坑，为以后的自己节约时间🙏🙏🙏 装系统过程中 有一页让选一些服务，只需要将位置勾上就行了 OneDrive 如果不使用的话，装好系统后先将账号和当前电脑解除关联，后续看情况要不要卸载。 英文系统中文字体显示问题 转载自：Windows 显示语言设置为英文时简体中文以日文或繁体中文字形显示的解决办法 - 知乎 (zhihu.com) 备份注册表 Computer\\HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Fonts 删除 Microsoft JhengHei （繁中）开头的3个键、Malgun（韩文）开头的3个键和 Yu Gothic（日文） 开头的4个键 重启 Windows Terminal 行距问题 第9行加一下cellHeight就可以 12345678910111213141516171819202122232425262728293031323334&quot;profiles&quot;: &#123; &quot;defaults&quot;: &#123; &quot;colorScheme&quot;: &quot;One Half Dark&quot;, &quot;font&quot;: &#123; &quot;face&quot;: &quot;DejaVuSansM Nerd Font Mono&quot;, &quot;cellHeight&quot;: &quot;1.5&quot; &#125;, &quot;opacity&quot;: 80 &#125;, &quot;list&quot;: [ &#123; &quot;commandline&quot;: &quot;%SystemRoot%\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\powershell.exe&quot;, &quot;guid&quot;: &quot;&#123;61c54bbd-c2c6-5271-96e7-009a87ff44bf&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Windows PowerShell&quot; &#125;, &#123; &quot;commandline&quot;: &quot;%SystemRoot%\\\\System32\\\\cmd.exe&quot;, &quot;guid&quot;: &quot;&#123;0caa0dad-35be-5f56-a8ff-afceeeaa6101&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Command Prompt&quot; &#125;, &#123; &quot;guid&quot;: &quot;&#123;b453ae62-4e3d-5e58-b989-0a998ec441b8&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Azure Cloud Shell&quot;, &quot;source&quot;: &quot;Windows.Terminal.Azure&quot; &#125; ] &#125;, git最小化安装及配置 最小化安装 如果直接安装git installer的话，他会装一些GUI，Bash什么的，其实后来发现都没啥用，所以可以采用最小化安装来避免安装这些软件。 在GitHub - git-for-windows/git: A fork of Git containing Windows-specific patches.项目中安装MinGit压缩包，解压后在系统环境变量Path中加入该文件夹中cmd文件夹路径即可。 在 Git 中配置邮箱、用户名以及代理需要以下几步： 配置邮箱和用户名 配置全局用户名和邮箱： 12git config --global user.name &quot;Your Name&quot;git config --global user.email &quot;your.email@example.com&quot; 配置特定仓库的用户名和邮箱： 进入你的仓库目录，然后执行以下命令： 12git config user.name &quot;Your Name&quot;git config user.email &quot;your.email@example.com&quot; 配置代理 HTTP 代理： 1git config --global http.proxy http://proxyuser:proxypassword@proxy.server.com:port HTTPS 代理： 1git config --global https.proxy https://proxyuser:proxypassword@proxy.server.com:port 取消代理配置： 12git config --global --unset http.proxygit config --global --unset https.proxy 查看当前配置 查看全局配置： 1git config --global --list 查看当前仓库配置： 1git config --list 通过上述步骤，你可以在 Git 中配置你的邮箱、用户名以及代理。 Node.js 配置 之前Node.js下在D盘，系统重置没有删除本体文件，因此只需要配置环境变量就可正确使用Node.js 本体环境变量配置 首先配置本体环境变量，这样才可以在系统全局中使用npm命令。 详情见博客Node.js 环境变量配置-腾讯云开发者社区-腾讯云 (tencent.com) 最后可以使用node,npm命令验证是否配置成功 全局node_module配置 配置完上一条其实node已经可以使用了。但是，若不进行环境变量配置，那么在使用命令安装 Node.js全局模块 （如：npm install -g vue）时，会默认安装到C盘的路径 (C:\\Users\\xxx\\AppData\\Roaming pm) 中，解决这个问题需要配置node_module全局的安装变量。 见博客Node.js 的安装及配置环境变量_Node.js配置环境变量-CSDN博客 最后可以安装一个全局模块如hexo或者vue通过安装位置来验证是否配置成功。 npm镜像配置 配置 npm 使用淘宝镜像可以显著提高在国内下载 npm 包的速度（推荐使用方案2）。以下是配置方法： 方法 1: 临时使用淘宝镜像 如果你只想临时使用淘宝镜像，可以在命令前加上 --registry 参数： 1npm install --registry=https://registry.npmmirror.com 方法 2: 永久配置淘宝镜像 通过命令配置 你可以通过命令行将淘宝镜像设置为默认的 npm 镜像： 1npm config set registry https://registry.npmmirror.com 通过配置文件配置 你也可以直接编辑 npm 的配置文件来设置淘宝镜像。在你的主目录下找到或创建一个 .npmrc 文件，并添加以下内容： 1registry=https://registry.npmmirror.com 方法 3: 使用 nrm 管理多个镜像源 nrm 是一个 npm 的镜像源管理工具，可以方便地在多个镜像源之间切换。 安装 nrm 1npm install -g nrm 查看可用镜像源 1nrm ls 使用淘宝镜像 1nrm use taobao 验证镜像配置 你可以通过以下命令验证镜像是否配置成功： 1npm config get registry 输出应为 https://registry.npmmirror.com： 1https://registry.npmmirror.com/ 这样，npm 就会使用淘宝镜像来下载包了。通过以上方法，你可以根据需要灵活配置 npm 的镜像源，提高下载速度。 PicGo 配置 这里主要介绍PicGO-Core的安装和配置 因为上面已经配置好npm的缘故，所以我们直接使用npm进行安装： 1npm install picgo -g 配置的话，我使用的是腾讯云，需要使用他两个云服务对象存储和访问管理，具体的教程见下面两个链接，这里不再赘述。 PicGo-Core 腾讯云COS对象存储+PicGo搭建图床教程-腾讯云开发者社区-腾讯云 (tencent.com) PowerShell 无法使用yarn，hexo命令问题 这些命令都是使用npm全局安装的命令，按照下面的步骤解决问题： 以管理员身份打开PowerShell 输入命令set-ExecutionPolicy RemoteSigned 全部选择Y即可。 MinGW 配置 将mingw中的bin文件夹添加到环境变量即可。 Typora 改主题标题大小 使用的是ladder主题，改配置文件中的font-size即可： 12345678h4 &#123; color: var(--tw-prose-headings); font-size: 1.55rem; font-weight: 600; margin-top: 1.8em; margin-bottom: 1.5rem; line-height: 1.375rem;&#125; anaconda 环境配置 Windows Defender 智能应用控制（Smart App Control）关闭（不然Zotero能给你启动20s） Device Security中的内核隔离（core isolation）我是关闭的，不知道会有什么影响，以后再说吧"},{"title":"mklink指令","path":"/wiki/Others/Windows/mklink指令.html","content":"基本介绍 mklink 是 Windows 中用于创建符号链接（symlink）、硬链接（hard link）和目录联接（directory junction）的命令。以下是一些常用的 mklink 命令以及如何使用它们创建文件夹链接的介绍。 基本语法 1mklink [options] &lt;Link&gt; &lt;Target&gt; &lt;Link&gt; 是要创建的链接的名称。 &lt;Target&gt; 是链接所指向的目标文件或文件夹。 常用选项 /D - 创建一个目录符号链接。默认情况下，mklink 创建的是文件符号链接，所以如果要创建一个指向目录的符号链接，需要使用这个选项。 /H - 创建一个硬链接，而不是符号链接。硬链接只能指向同一分区上的文件。 /J - 创建一个目录联接。这是一个特殊类型的符号链接，专门用于目录。 创建文件夹符号链接 假设有一个目标文件夹 C:\\OriginalFolder，如果想在 C:\\NewFolderLink 位置创建一个指向它的符号链接，使用以下命令： 1mklink /D C:\\NewFolderLink C:\\OriginalFolder 创建文件夹硬链接 硬链接不能用于目录，只能用于文件。所以如果需要创建目录的链接，建议使用符号链接或目录联接。 创建目录联接 如果希望创建一个目录联接，使用以下命令： 1mklink /J C:\\NewFolderLink C:\\OriginalFolder 示例 创建指向目录 C:\\Users\\YourName\\Documents\\MyFolder 的符号链接： 1mklink /D C:\\LinkToMyFolder C:\\Users\\YourName\\Documents\\MyFolder 创建指向目录 D:\\Data\\Projects 的目录联接： 1mklink /J C:\\ProjectsLink D:\\Data\\Projects 注意事项 确保在命令提示符（cmd）中以管理员身份运行 mklink 命令，因为创建符号链接需要管理员权限。 使用符号链接和目录联接时，请注意目标路径的有效性。如果目标路径不存在或不可访问，链接将无法正常工作。 通过 mklink 命令，可以方便地在不同目录之间创建快捷方式，简化文件和目录的管理。 \\D和\\J的区别 /D 和 /J 是 mklink 命令中用于创建不同类型目录链接的选项，它们的主要区别如下： /D - 创建目录符号链接 (Symbolic Link) 符号链接（symlink）是一个引用指向另一个文件或目录的路径。 符号链接可以跨不同的文件系统分区使用，也可以指向网络路径。 符号链接可以指向不存在的目标（当目标路径存在时链接才会有效）。 删除符号链接不会影响目标文件或目录。 示例： 1mklink /D C:\\LinkToFolder C:\\OriginalFolder 这个命令创建一个符号链接 C:\\LinkToFolder，它指向 C:\\OriginalFolder。 /J - 创建目录联接 (Directory Junction) 目录联接（junction）是 Windows 的 NTFS 文件系统中的一种特殊类型的符号链接。 目录联接只能用于本地目录，不能用于网络路径。 目录联接仅适用于目录，不适用于单个文件。 目录联接通常比符号链接更兼容一些旧的 Windows 应用程序。 删除目录联接不会影响目标目录中的文件。 示例： 1mklink /J C:\\JunctionToFolder C:\\OriginalFolder 这个命令创建一个目录联接 C:\\JunctionToFolder，它指向 C:\\OriginalFolder。 总结 符号链接（使用 /D）更灵活，可以指向任意文件或目录，并且可以跨分区和网络使用。 目录联接（使用 /J）更局限，只能用于本地目录，但可能在一些场景下有更好的兼容性。 选择使用 /D 还是 /J 取决于具体需求。如果需要更高的灵活性和跨分区支持，选择 /D；如果主要在本地文件系统中操作目录并且需要兼容一些旧应用程序，选择 /J。 查看文件/文件夹链接类型 要查看一个链接的类型是 /D（符号链接）还是 /J（目录联接），可以通过以下几种方法来进行检查： 使用 dir 命令 打开命令提示符（cmd）。 导航到包含链接的目录。 输入 dir 命令。 在 dir 命令的输出中： 符号链接会显示为 &lt;SYMLINKD&gt;。 目录联接会显示为 &lt;JUNCTION&gt;。 示例： 1C:\\&gt; dir 输出可能类似于： 12342024-05-30 10:30 AM &lt;DIR&gt; .2024-05-30 10:30 AM &lt;DIR&gt; ..2024-05-30 10:31 AM &lt;SYMLINKD&gt; LinkToFolder [C:\\OriginalFolder]2024-05-30 10:32 AM &lt;JUNCTION&gt; JunctionToFolder [C:\\OriginalFolder] 在这里，LinkToFolder 是符号链接，JunctionToFolder 是目录联接。 使用 fsutil 命令 打开命令提示符（cmd）以管理员身份运行。 使用 fsutil reparsepoint query 命令。 示例： 1fsutil reparsepoint query C:\\LinkToFolder 如果这是一个符号链接，输出会包含类似 Reparse Tag Value: 0xA000000C 的信息。 如果这是一个目录联接，输出会包含类似 Reparse Tag Value: 0xA0000003 的信息。 使用 PowerShell 打开 PowerShell。 导航到包含链接的目录。 使用 Get-Item cmdlet。 示例： 12(Get-Item C:\\LinkToFolder).LinkType(Get-Item C:\\JunctionToFolder).LinkType 输出会显示 SymbolicLink 或 Junction，分别表示符号链接和目录联接。 示例综合 假设有两个链接：C:\\LinkToFolder 和 C:\\JunctionToFolder。使用上述方法，可以轻松地确定它们的类型。 在命令提示符中运行 dir 命令，可以看到 LinkToFolder 是符号链接 (&lt;SYMLINKD&gt;)，而 JunctionToFolder 是目录联接 (&lt;JUNCTION&gt;）。 使用 fsutil reparsepoint query 命令可以进一步确认它们的类型。 在 PowerShell 中使用 Get-Item cmdlet 也可以查看链接的类型。 通过这些方法，可以准确地确定一个链接是符号链接还是目录联接。"},{"title":"最短路——Floyd算法","path":"/wiki/OI-Knowledge/图论/最短路/Floyd算法.html","content":"在图论中，寻找所有顶点对之间的最短路径是一个常见问题。Floyd-Warshall 算法是一种解决这个问题的有效方法。本文将介绍 Floyd-Warshall 算法的基本概念、工作原理，并提供相应的 C++ 实现代码。 什么是 Floyd-Warshall 算法？ Floyd-Warshall 算法是一种用于寻找加权图中所有顶点对之间最短路径的动态规划算法。该算法的主要思想是通过中间顶点迭代更新最短路径长度，从而逐步优化路径。 算法步骤 初始化：创建一个距离矩阵 dist，其中 dist[i][j] 表示顶点 i 到顶点 j 的初始距离。如果两顶点之间没有边，则距离设为无穷大（通常用一个大数表示）。 迭代更新：对于每个中间顶点 k，更新所有顶点对 (i, j) 的距离。如果通过顶点 k 的路径比当前已知路径更短，则更新 dist[i][j] 的值。 结果输出：最终的距离矩阵 dist 即为所有顶点对之间的最短路径长度。 算法复杂度 Floyd-Warshall 算法的时间复杂度为 O(V^3)，其中 V 是图中的顶点数。尽管时间复杂度较高，但该算法在稠密图（边数接近于顶点数平方）中表现良好。 代码实现 以下是 Floyd-Warshall 算法的 C++ 实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;limits&gt;const int INF = std::numeric_limits&lt;int&gt;::max();// Floyd-Warshall算法实现void floydWarshall(std::vector&lt;std::vector&lt;int&gt;&gt;&amp; graph) &#123; int V = graph.size(); std::vector&lt;std::vector&lt;int&gt;&gt; dist = graph; // 三重循环，更新距离矩阵 for (int k = 0; k &lt; V; ++k) &#123; for (int i = 0; i &lt; V; ++i) &#123; for (int j = 0; j &lt; V; ++j) &#123; if (dist[i][k] != INF &amp;&amp; dist[k][j] != INF &amp;&amp; dist[i][k] + dist[k][j] &lt; dist[i][j]) &#123; dist[i][j] = dist[i][k] + dist[k][j]; &#125; &#125; &#125; &#125; // 输出最终的距离矩阵 for (int i = 0; i &lt; V; ++i) &#123; for (int j = 0; j &lt; V; ++j) &#123; if (dist[i][j] == INF) &#123; std::cout &lt;&lt; &quot;INF &quot;; &#125; else &#123; std::cout &lt;&lt; dist[i][j] &lt;&lt; &quot; &quot;; &#125; &#125; std::cout &lt;&lt; std::endl; &#125;&#125;int main() &#123; std::vector&lt;std::vector&lt;int&gt;&gt; graph = &#123; &#123;0, 3, INF, 7&#125;, &#123;8, 0, 2, INF&#125;, &#123;5, INF, 0, 1&#125;, &#123;2, INF, INF, 0&#125; &#125;; floydWarshall(graph); return 0;&#125; 代码解析 定义常量 INF：用于表示无穷大。 初始化图的邻接矩阵 graph：每个元素表示顶点之间的距离。如果没有直接边，则距离为 INF。 调用 floydWarshall 函数：传入图的邻接矩阵进行处理。 更新距离矩阵：通过三重循环迭代更新距离矩阵中的值。 输出结果：打印最终的最短路径矩阵。 总结 Floyd-Warshall 算法是一种简单但功能强大的算法，可以有效地解决所有顶点对之间的最短路径问题。通过本文的介绍和代码示例，希望读者能够更好地理解并应用这一算法。"},{"title":"替换 VS Code 的水印","path":"/wiki/Others/Softwares/VSCode/替换 VS Code 的水印.html","content":"水印下载传送门：https://github.com/Aikoyori/ProgrammingVTuberLogos 安装 安装以下扩展: Custom CSS and JS Loader Fix VSCode Checksums Next 在你的设置 JSON 文件中添加以下内容(通过命令 Open User Settings (JSON) 打开 settings.json):123&quot;vscode_custom_css.imports&quot;: [ &quot;https://raw.githubusercontent.com/Aikoyori/ProgrammingVTuberLogos/main/ReplaceGuide/VSCode/style.css&quot;] 运行以下命令: Enable Custom CSS and JS Fix Checksums: Apply 重启 Visual Studio Code 卸载 从你的设置 JSON 文件(settings.json)中移除 vscode_custom_css.imports 部分 运行以下命令: Disable Custom CSS and JS Fix Checksums: Restore 卸载 Custom CSS and JS Loader 和 Fix VSCode Checksums Next 扩展 重启 Visual Studio Code"},{"title":"运行路径设置","path":"/wiki/Others/Softwares/VSCode/运行路径设置.html","content":"问题描述 在运行或调试Python代码的时候，我们可能会遇见下面这种情况，项目结构如下所示： 123456/path/to/your/project├── main.py├── a│ └── some_script.py└── b └── some_module.py 此时我们想运行a文件夹中的some_script.py文件，但他其中包含了一行引入代码如下所示： 1from b.some_module import * 直接使用传统python ./a/some_script.py运行代码会报错。 解决方案 设置PYTHONPATH参数，其作用是告诉脚本从哪个位置查找模块. 当你运行一个 Python 程序并使用 import 语句导入模块时，Python 会按顺序在以下几个位置查找模块： 当前脚本所在目录。 标准库目录。 PYTHONPATH 环境变量中指定的目录。 安装的第三方包所在的目录（例如，通过 pip 安装的包）。 运行 在命令行中设置PYTHONPATH,然后运行a文件夹中的代码。例如，如果你要运行a文件夹中的some_script.py，可以这样做： 1PYTHONPATH=$(pwd) python a/some_script.py 如果命令行正好处在当前项目文件夹下，直接写成如下形式,即可确保相关包能够被正确导入。 1PYTHONPATH=./ python a/some_script.py 调试 在vscode左侧初始化一个调试launch.json文件后，添加env参数，按照如下设置即可： 12345678910111213141516171819&#123; // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;Python Debugger: Current File&quot;, &quot;type&quot;: &quot;debugpy&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;file&#125;&quot;, &quot;console&quot;: &quot;integratedTerminal&quot;, // &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, &quot;env&quot;: &#123; &quot;PYTHONPATH&quot;: &quot;$&#123;workspaceFolder&#125;&quot; &#125; &#125; ]&#125; 注意其中的cwd参数作用是设置当前工作目录为工作区根目录，确保代码中所有相对路径都是相对于项目根目录。不要混淆。"}]